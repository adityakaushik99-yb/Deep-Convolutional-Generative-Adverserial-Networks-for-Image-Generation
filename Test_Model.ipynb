{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityakaushik99/Deep-Convolutional-Generative-Adverserial-Networks-for-Image-Generation/blob/master/Test_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGn4oiHLFpOT",
        "colab_type": "code",
        "outputId": "649f1192-9032-4b88-9bc9-ee2a2cf82e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2tJxFw-7BiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = 'gdrive/My Drive/DCGAN/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bUCUpfdINkr",
        "colab_type": "code",
        "outputId": "abe04753-dfb5-47d7-8d55-02fca7423690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!python3 gdrive/My\\ Drive/Colab\\ Notebooks/DCGAN/getdata.py --datasets CIFAR10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./datasets/cifar10/cifar-10-python.tar.gz\n",
            "Downloading CIFAR10\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  162M  100  162M    0     0  16.0M      0  0:00:10  0:00:10 --:--:-- 20.2M\n",
            "Preprocessing...\n",
            "[=========================================================================] 100%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMgdZfeF5s44",
        "colab_type": "code",
        "outputId": "72915005-ee8f-400d-eedd-11c3b9d9c3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip3 install colorlog"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Installing collected packages: colorlog\n",
            "Successfully installed colorlog-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fspBPqSGMBrc",
        "colab_type": "code",
        "outputId": "a87fe230-1543-40df-fe13-d529ca895bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 gdrive/My\\ Drive/Colab\\ Notebooks/DCGAN/trainer.py --dataset CIFAR10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[37m\u001b[01m[2019-06-28 04:09:01,935] Reading ./datasets/cifar10/data.hy ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:01,936] Reading Done: ./datasets/cifar10/data.hy\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:01,936] Reading ./datasets/cifar10/data.hy ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:01,936] Reading Done: ./datasets/cifar10/data.hy\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:09:01,937] Train Dir: ./train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190628-040901\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:01,937] input_ops [input]: Using 50000 IDs from dataset\u001b[0m\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0628 04:09:02.181508 140327063500672 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:31: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0628 04:09:02.186414 140327063500672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0628 04:09:02.187257 140327063500672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "W0628 04:09:02.188894 140327063500672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0628 04:09:02.189908 140327063500672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0628 04:09:02.194911 140327063500672 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:40: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0628 04:09:02.195897 140327063500672 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:49: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:02,205] input_ops [input]: Using 10000 IDs from dataset\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:09:02,275] Using Model Class : <class 'model.Model'>\u001b[0m\n",
            "W0628 04:09:02.275872 140327063500672 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0628 04:09:02.276669 140327063500672 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:33: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0628 04:09:02.277903 140327063500672 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:91: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0628 04:09:02.281803 140327063500672 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:54: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "\u001b[93mGenerator\u001b[0m\n",
            "Generator Tensor(\"Generator/g_1_deconv/Relu:0\", shape=(64, 2, 2, 384), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_2_deconv/Relu:0\", shape=(64, 6, 6, 128), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_3_deconv/Relu:0\", shape=(64, 14, 14, 64), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_4_deconv/Tanh:0\", shape=(64, 32, 32, 3), dtype=float32)\n",
            "\u001b[93mDiscriminator\u001b[0m\n",
            "W0628 04:09:03.096115 140327063500672 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/ops.py:19: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "Discriminator Tensor(\"d_1_conv/dropout/mul_1:0\", shape=(64, 16, 16, 64), dtype=float32)\n",
            "Discriminator Tensor(\"d_2_conv/dropout/mul_1:0\", shape=(64, 8, 8, 128), dtype=float32)\n",
            "Discriminator Tensor(\"d_3_conv/dropout/mul_1:0\", shape=(64, 4, 4, 256), dtype=float32)\n",
            "Discriminator Tensor(\"Discriminator/d_4_fc/BiasAdd:0\", shape=(64, 1), dtype=float32)\n",
            "W0628 04:09:03.636513 140327063500672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0628 04:09:03.659795 140327063500672 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0628 04:09:03.672794 140327063500672 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:127: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "\u001b[93mSuccessfully loaded the model.\u001b[0m\n",
            "W0628 04:09:03.675226 140327063500672 deprecation.py:323] From gdrive/My Drive/Colab Notebooks/DCGAN/trainer.py:38: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "\u001b[33m[2019-06-28 04:09:03,678] ********* d_var *********\u001b[0m\n",
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "Discriminator/d_1_conv/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
            "Discriminator/d_1_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
            "Discriminator/d_1_conv/BatchNorm/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "Discriminator/d_1_conv/BatchNorm/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "Discriminator/d_2_conv/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
            "Discriminator/d_2_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
            "Discriminator/d_2_conv/BatchNorm/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "Discriminator/d_2_conv/BatchNorm/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "Discriminator/d_3_conv/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
            "Discriminator/d_3_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
            "Discriminator/d_3_conv/BatchNorm/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
            "Discriminator/d_3_conv/BatchNorm/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
            "Discriminator/d_4_fc/weights:0 (float32_ref 4096x1) [4096, bytes: 16384]\n",
            "Discriminator/d_4_fc/biases:0 (float32_ref 1) [1, bytes: 4]\n",
            "Total size of variables: 1034241\n",
            "Total bytes of variables: 4136964\n",
            "\u001b[33m[2019-06-28 04:09:03,679] ********* g_var *********\u001b[0m\n",
            "---------\n",
            "Variables: name (type shape) [size]\n",
            "---------\n",
            "Generator/g_1_deconv/Conv2d_transpose/weights:0 (float32_ref 2x2x384x100) [153600, bytes: 614400]\n",
            "Generator/g_1_deconv/Conv2d_transpose/biases:0 (float32_ref 384) [384, bytes: 1536]\n",
            "Generator/g_1_deconv/BatchNorm/beta:0 (float32_ref 384) [384, bytes: 1536]\n",
            "Generator/g_1_deconv/BatchNorm/gamma:0 (float32_ref 384) [384, bytes: 1536]\n",
            "Generator/g_2_deconv/Conv2d_transpose/weights:0 (float32_ref 4x4x128x384) [786432, bytes: 3145728]\n",
            "Generator/g_2_deconv/Conv2d_transpose/biases:0 (float32_ref 128) [128, bytes: 512]\n",
            "Generator/g_2_deconv/BatchNorm/beta:0 (float32_ref 128) [128, bytes: 512]\n",
            "Generator/g_2_deconv/BatchNorm/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
            "Generator/g_3_deconv/Conv2d_transpose/weights:0 (float32_ref 4x4x64x128) [131072, bytes: 524288]\n",
            "Generator/g_3_deconv/Conv2d_transpose/biases:0 (float32_ref 64) [64, bytes: 256]\n",
            "Generator/g_3_deconv/BatchNorm/beta:0 (float32_ref 64) [64, bytes: 256]\n",
            "Generator/g_3_deconv/BatchNorm/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
            "Generator/g_4_deconv/Conv2d_transpose/weights:0 (float32_ref 6x6x3x64) [6912, bytes: 27648]\n",
            "Generator/g_4_deconv/Conv2d_transpose/biases:0 (float32_ref 3) [3, bytes: 12]\n",
            "Total size of variables: 1079747\n",
            "Total bytes of variables: 4318988\n",
            "[]\n",
            "W0628 04:09:03.680234 140327063500672 deprecation_wrapper.py:119] From gdrive/My Drive/Colab Notebooks/DCGAN/trainer.py:63: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0628 04:09:04.692306 140327063500672 deprecation_wrapper.py:119] From gdrive/My Drive/Colab Notebooks/DCGAN/trainer.py:74: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0628 04:09:04.693654 140327063500672 deprecation_wrapper.py:119] From gdrive/My Drive/Colab Notebooks/DCGAN/trainer.py:76: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0628 04:09:04.798698 140327063500672 deprecation_wrapper.py:119] From gdrive/My Drive/Colab Notebooks/DCGAN/trainer.py:78: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0628 04:09:04.799259 140327063500672 deprecation.py:323] From gdrive/My Drive/Colab Notebooks/DCGAN/trainer.py:82: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2019-06-28 04:09:05.001517: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-06-28 04:09:05.001721: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b1dc00 executing computations on platform Host. Devices:\n",
            "2019-06-28 04:09:05.001749: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-28 04:09:05.003671: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-28 04:09:05.166748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 04:09:05.167367: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b1f800 executing computations on platform CUDA. Devices:\n",
            "2019-06-28 04:09:05.167398: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-28 04:09:05.167615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 04:09:05.168036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-28 04:09:05.168367: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 04:09:05.169534: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-28 04:09:05.170631: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-28 04:09:05.170963: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-28 04:09:05.172378: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-28 04:09:05.173391: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-28 04:09:05.176480: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-28 04:09:05.176604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 04:09:05.177000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 04:09:05.177378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-28 04:09:05.177443: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 04:09:05.178514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-28 04:09:05.178540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-28 04:09:05.178550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-28 04:09:05.178819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 04:09:05.179216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 04:09:05.179567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-06-28 04:09:06.064087: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "\u001b[33m[2019-06-28 04:09:07,365] dataset: CIFAR10, learning_rate: 0.000100\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:09:07,365] training starts\u001b[0m\n",
            "{'id': <tf.Tensor 'shuffle_batch:0' shape=(64,) dtype=string>,\n",
            " 'image': <tf.Tensor 'shuffle_batch:1' shape=(64, 32, 32, 3) dtype=float32>}\n",
            "2019-06-28 04:09:09.242712: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-28 04:09:09.667776: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:11,609] [train step   0] D loss: 1.90851 G loss: 0.90579 (4.243 sec/batch, 15.083 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:09:11,609] Saved checkpoint at 0\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:12,610] [train step  11] D loss: 9.14331 G loss: 0.00064 (0.037 sec/batch, 1737.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:13,015] [train step  21] D loss: 1.98382 G loss: 3.94495 (0.036 sec/batch, 1759.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:13,400] [train step  30] D loss: 10.11407 G loss: 0.00033 (0.037 sec/batch, 1736.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:13,788] [train step  41] D loss: 9.57442 G loss: 0.00062 (0.041 sec/batch, 1564.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:14,188] [train step  50] D loss: 0.94487 G loss: 7.09131 (0.029 sec/batch, 2219.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:14,582] [train step  60] D loss: 6.55125 G loss: 0.01257 (0.039 sec/batch, 1633.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:14,978] [train step  71] D loss: 7.55065 G loss: 0.05849 (0.035 sec/batch, 1812.614 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:15,361] [train step  80] D loss: 1.71267 G loss: 8.91883 (0.033 sec/batch, 1952.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:15,749] [train step  90] D loss: 1.56628 G loss: 1.00927 (0.042 sec/batch, 1531.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:16,148] [train step 101] D loss: 4.57029 G loss: 0.09603 (0.037 sec/batch, 1718.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:16,530] [train step 111] D loss: 1.10350 G loss: 6.65434 (0.039 sec/batch, 1623.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:16,927] [train step 120] D loss: 2.86035 G loss: 0.24925 (0.050 sec/batch, 1270.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:17,305] [train step 130] D loss: 0.64975 G loss: 1.64595 (0.036 sec/batch, 1792.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:17,705] [train step 141] D loss: 1.43172 G loss: 3.96467 (0.033 sec/batch, 1941.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:18,096] [train step 150] D loss: 0.80352 G loss: 2.38306 (0.038 sec/batch, 1699.970 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:18,473] [train step 161] D loss: 0.44368 G loss: 1.89920 (0.036 sec/batch, 1763.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:18,865] [train step 171] D loss: 0.64692 G loss: 2.94704 (0.040 sec/batch, 1592.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:19,259] [train step 180] D loss: 3.67347 G loss: 0.23310 (0.040 sec/batch, 1595.228 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:19,646] [train step 191] D loss: 1.74694 G loss: 0.65477 (0.040 sec/batch, 1582.822 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:20,031] [train step 200] D loss: 2.14742 G loss: 8.08736 (0.038 sec/batch, 1691.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:20,419] [train step 210] D loss: 0.85018 G loss: 1.50661 (0.037 sec/batch, 1735.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:20,802] [train step 220] D loss: 1.65467 G loss: 0.60394 (0.045 sec/batch, 1406.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:21,206] [train step 230] D loss: 2.06364 G loss: 10.81026 (0.038 sec/batch, 1682.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:21,598] [train step 240] D loss: 0.95215 G loss: 7.49053 (0.037 sec/batch, 1723.580 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:21,980] [train step 251] D loss: 0.52866 G loss: 4.95758 (0.041 sec/batch, 1554.778 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:22,376] [train step 261] D loss: 0.55999 G loss: 5.41073 (0.037 sec/batch, 1709.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:22,763] [train step 270] D loss: 0.85768 G loss: 7.17690 (0.043 sec/batch, 1497.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:23,161] [train step 281] D loss: 0.94159 G loss: 0.86422 (0.037 sec/batch, 1745.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:23,548] [train step 291] D loss: 0.79522 G loss: 5.14544 (0.039 sec/batch, 1644.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:23,933] [train step 300] D loss: 1.91055 G loss: 1.00251 (0.038 sec/batch, 1704.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:24,330] [train step 311] D loss: 2.65865 G loss: 0.22514 (0.041 sec/batch, 1563.790 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:24,712] [train step 321] D loss: 1.04475 G loss: 9.97503 (0.033 sec/batch, 1917.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:25,093] [train step 330] D loss: 2.04138 G loss: 0.44490 (0.038 sec/batch, 1704.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:25,485] [train step 341] D loss: 0.58173 G loss: 5.62651 (0.043 sec/batch, 1491.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:25,865] [train step 351] D loss: 0.55548 G loss: 5.45756 (0.038 sec/batch, 1682.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:26,254] [train step 360] D loss: 0.43536 G loss: 3.92027 (0.037 sec/batch, 1715.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:26,651] [train step 371] D loss: 0.53728 G loss: 5.25228 (0.043 sec/batch, 1500.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:27,045] [train step 380] D loss: 0.39702 G loss: 2.37169 (0.038 sec/batch, 1705.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:27,443] [train step 390] D loss: 0.64450 G loss: 6.40899 (0.037 sec/batch, 1741.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:27,822] [train step 400] D loss: 0.43233 G loss: 1.79681 (0.036 sec/batch, 1792.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:28,236] [train step 411] D loss: 0.62678 G loss: 6.17406 (0.048 sec/batch, 1324.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:28,623] [train step 420] D loss: 0.40211 G loss: 3.38407 (0.038 sec/batch, 1700.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:29,006] [train step 431] D loss: 0.40125 G loss: 3.52151 (0.035 sec/batch, 1820.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:29,403] [train step 441] D loss: 0.67028 G loss: 6.67548 (0.035 sec/batch, 1850.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:29,788] [train step 450] D loss: 0.93972 G loss: 0.85061 (0.037 sec/batch, 1719.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:30,183] [train step 460] D loss: 0.63650 G loss: 6.31615 (0.041 sec/batch, 1553.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:30,591] [train step 470] D loss: 0.52483 G loss: 1.47233 (0.039 sec/batch, 1622.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:30,983] [train step 480] D loss: 0.69256 G loss: 6.84886 (0.038 sec/batch, 1679.096 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:31,375] [train step 490] D loss: 0.55300 G loss: 5.44106 (0.038 sec/batch, 1703.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:31,780] [train step 500] D loss: 0.40063 G loss: 2.04578 (0.046 sec/batch, 1401.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:32,169] [train step 510] D loss: 0.39181 G loss: 3.29689 (0.041 sec/batch, 1568.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:32,558] [train step 520] D loss: 0.39341 G loss: 2.16550 (0.040 sec/batch, 1617.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:32,942] [train step 530] D loss: 0.40528 G loss: 3.54946 (0.041 sec/batch, 1549.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:33,333] [train step 540] D loss: 0.59758 G loss: 5.91025 (0.039 sec/batch, 1624.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:33,722] [train step 550] D loss: 0.52191 G loss: 1.46282 (0.034 sec/batch, 1908.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:34,108] [train step 561] D loss: 0.69001 G loss: 6.84331 (0.038 sec/batch, 1676.977 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:34,504] [train step 570] D loss: 0.41159 G loss: 3.73646 (0.041 sec/batch, 1548.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:34,901] [train step 580] D loss: 0.45300 G loss: 4.29125 (0.040 sec/batch, 1600.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:35,281] [train step 590] D loss: 0.98006 G loss: 9.76731 (0.040 sec/batch, 1587.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:35,671] [train step 600] D loss: 0.47151 G loss: 4.48849 (0.035 sec/batch, 1853.913 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:36,065] [train step 611] D loss: 0.42223 G loss: 3.84264 (0.035 sec/batch, 1810.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:36,461] [train step 620] D loss: 0.37472 G loss: 2.28403 (0.048 sec/batch, 1344.894 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:36,853] [train step 630] D loss: 0.38957 G loss: 3.27408 (0.040 sec/batch, 1581.609 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:37,243] [train step 640] D loss: 0.39191 G loss: 2.42097 (0.048 sec/batch, 1333.483 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:37,640] [train step 650] D loss: 0.36743 G loss: 2.95169 (0.035 sec/batch, 1827.185 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:38,034] [train step 660] D loss: 0.37515 G loss: 2.66320 (0.038 sec/batch, 1670.351 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:38,417] [train step 670] D loss: 0.36102 G loss: 2.62103 (0.031 sec/batch, 2051.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:38,837] [train step 680] D loss: 0.37172 G loss: 2.79625 (0.039 sec/batch, 1646.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:39,229] [train step 690] D loss: 0.46890 G loss: 1.65162 (0.045 sec/batch, 1408.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:39,631] [train step 701] D loss: 0.47112 G loss: 4.55869 (0.039 sec/batch, 1635.954 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:40,020] [train step 710] D loss: 0.38921 G loss: 3.22553 (0.039 sec/batch, 1633.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:40,400] [train step 720] D loss: 0.36739 G loss: 2.51230 (0.037 sec/batch, 1744.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:40,804] [train step 731] D loss: 0.38547 G loss: 2.52386 (0.037 sec/batch, 1714.027 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:41,187] [train step 741] D loss: 0.38244 G loss: 2.37339 (0.036 sec/batch, 1788.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:41,571] [train step 750] D loss: 0.36501 G loss: 2.78455 (0.037 sec/batch, 1728.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:41,960] [train step 761] D loss: 0.36764 G loss: 3.01668 (0.036 sec/batch, 1783.944 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:42,331] [train step 771] D loss: 0.37307 G loss: 2.50072 (0.040 sec/batch, 1616.799 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:42,721] [train step 780] D loss: 0.45360 G loss: 4.31674 (0.038 sec/batch, 1671.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:43,104] [train step 791] D loss: 0.59450 G loss: 5.81744 (0.034 sec/batch, 1876.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:43,480] [train step 800] D loss: 0.50596 G loss: 1.41545 (0.043 sec/batch, 1479.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:43,870] [train step 810] D loss: 0.51083 G loss: 4.94418 (0.039 sec/batch, 1630.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:44,243] [train step 821] D loss: 0.38546 G loss: 1.99152 (0.035 sec/batch, 1836.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:44,623] [train step 831] D loss: 0.38350 G loss: 3.13772 (0.034 sec/batch, 1908.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:45,009] [train step 840] D loss: 0.39772 G loss: 1.89149 (0.036 sec/batch, 1768.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:45,384] [train step 850] D loss: 0.38985 G loss: 2.85730 (0.036 sec/batch, 1756.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:45,773] [train step 861] D loss: 0.36739 G loss: 2.79112 (0.039 sec/batch, 1623.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:46,153] [train step 870] D loss: 0.38986 G loss: 3.42710 (0.037 sec/batch, 1732.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:46,538] [train step 881] D loss: 0.36916 G loss: 2.68126 (0.038 sec/batch, 1699.174 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:46,929] [train step 890] D loss: 0.35827 G loss: 2.43444 (0.038 sec/batch, 1678.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:47,309] [train step 900] D loss: 0.37211 G loss: 2.87944 (0.033 sec/batch, 1910.940 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:47,700] [train step 910] D loss: 0.39140 G loss: 2.59766 (0.037 sec/batch, 1725.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:48,095] [train step 920] D loss: 0.35050 G loss: 2.62540 (0.040 sec/batch, 1615.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:48,477] [train step 930] D loss: 0.36038 G loss: 2.38723 (0.037 sec/batch, 1722.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:48,876] [train step 940] D loss: 0.35360 G loss: 2.39266 (0.037 sec/batch, 1752.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:49,288] [train step 951] D loss: 0.43465 G loss: 4.11653 (0.047 sec/batch, 1351.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:49,676] [train step 960] D loss: 0.37589 G loss: 3.18082 (0.035 sec/batch, 1837.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:50,078] [train step 971] D loss: 0.41412 G loss: 3.76638 (0.040 sec/batch, 1596.481 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:50,467] [train step 981] D loss: 0.34916 G loss: 2.54360 (0.036 sec/batch, 1761.492 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:50,877] [train step 990] D loss: 0.37753 G loss: 3.19428 (0.041 sec/batch, 1569.394 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:51,277] [train step1001] D loss: 0.35777 G loss: 2.63858 (0.040 sec/batch, 1583.064 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:09:51,277] Saved checkpoint at 1000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:51,866] [train step1011] D loss: 0.36329 G loss: 2.91586 (0.039 sec/batch, 1627.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:52,268] [train step1020] D loss: 0.35558 G loss: 2.67315 (0.036 sec/batch, 1767.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:52,660] [train step1031] D loss: 0.39724 G loss: 3.57264 (0.040 sec/batch, 1599.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:53,061] [train step1040] D loss: 0.37146 G loss: 2.18497 (0.035 sec/batch, 1809.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:53,447] [train step1050] D loss: 0.35973 G loss: 2.81833 (0.037 sec/batch, 1716.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:53,847] [train step1060] D loss: 0.36901 G loss: 2.23632 (0.037 sec/batch, 1729.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:54,244] [train step1071] D loss: 0.37306 G loss: 2.74856 (0.036 sec/batch, 1767.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:54,651] [train step1080] D loss: 0.38911 G loss: 1.97925 (0.042 sec/batch, 1538.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:55,056] [train step1090] D loss: 0.35678 G loss: 2.86808 (0.042 sec/batch, 1520.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:55,451] [train step1100] D loss: 0.48406 G loss: 4.71203 (0.040 sec/batch, 1582.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:55,840] [train step1110] D loss: 0.38129 G loss: 3.37345 (0.039 sec/batch, 1622.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:56,240] [train step1120] D loss: 0.44357 G loss: 4.24463 (0.042 sec/batch, 1531.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:56,628] [train step1130] D loss: 0.42528 G loss: 3.98410 (0.036 sec/batch, 1779.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:57,017] [train step1140] D loss: 0.37707 G loss: 3.25208 (0.039 sec/batch, 1650.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:57,405] [train step1151] D loss: 0.39754 G loss: 3.65177 (0.040 sec/batch, 1593.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:57,795] [train step1160] D loss: 0.37258 G loss: 2.28735 (0.039 sec/batch, 1645.027 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:58,188] [train step1170] D loss: 0.35825 G loss: 2.50582 (0.032 sec/batch, 2009.338 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:58,580] [train step1180] D loss: 0.34926 G loss: 2.38881 (0.038 sec/batch, 1696.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:58,965] [train step1190] D loss: 0.35803 G loss: 2.79543 (0.048 sec/batch, 1326.832 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:59,361] [train step1200] D loss: 0.35440 G loss: 2.71160 (0.038 sec/batch, 1701.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:09:59,769] [train step1211] D loss: 0.36430 G loss: 3.00623 (0.038 sec/batch, 1687.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:00,167] [train step1221] D loss: 0.35188 G loss: 2.51347 (0.048 sec/batch, 1327.298 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:00,543] [train step1230] D loss: 0.36002 G loss: 2.65115 (0.028 sec/batch, 2275.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:00,936] [train step1241] D loss: 0.35472 G loss: 2.40329 (0.037 sec/batch, 1733.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:01,329] [train step1250] D loss: 0.36310 G loss: 2.48803 (0.042 sec/batch, 1518.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:01,714] [train step1260] D loss: 0.36620 G loss: 2.84639 (0.040 sec/batch, 1593.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:02,104] [train step1271] D loss: 0.35871 G loss: 2.28625 (0.041 sec/batch, 1544.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:02,507] [train step1280] D loss: 0.71490 G loss: 7.13485 (0.043 sec/batch, 1493.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:02,916] [train step1290] D loss: 0.68419 G loss: 1.08365 (0.039 sec/batch, 1655.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:03,310] [train step1301] D loss: 1.29443 G loss: 12.94000 (0.038 sec/batch, 1664.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:03,689] [train step1310] D loss: 0.40635 G loss: 3.63619 (0.039 sec/batch, 1656.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:04,077] [train step1320] D loss: 1.18601 G loss: 0.51421 (0.038 sec/batch, 1686.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:04,476] [train step1331] D loss: 0.75040 G loss: 7.49217 (0.037 sec/batch, 1738.403 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:04,874] [train step1340] D loss: 0.42788 G loss: 1.69495 (0.039 sec/batch, 1643.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:05,267] [train step1350] D loss: 0.39037 G loss: 3.51828 (0.047 sec/batch, 1367.824 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:05,654] [train step1361] D loss: 0.38062 G loss: 2.06841 (0.044 sec/batch, 1450.759 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:06,043] [train step1371] D loss: 0.36297 G loss: 2.76337 (0.048 sec/batch, 1333.344 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:06,445] [train step1380] D loss: 0.37613 G loss: 2.32452 (0.040 sec/batch, 1613.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:06,827] [train step1390] D loss: 0.36714 G loss: 2.05454 (0.031 sec/batch, 2062.366 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:07,237] [train step1400] D loss: 0.41048 G loss: 3.80447 (0.039 sec/batch, 1652.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:07,647] [train step1410] D loss: 0.44156 G loss: 1.65241 (0.035 sec/batch, 1838.813 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:08,038] [train step1420] D loss: 0.40131 G loss: 3.66693 (0.038 sec/batch, 1674.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:08,431] [train step1431] D loss: 0.52389 G loss: 5.15231 (0.038 sec/batch, 1684.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:08,822] [train step1440] D loss: 0.46269 G loss: 4.46651 (0.043 sec/batch, 1497.732 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:09,202] [train step1450] D loss: 0.36362 G loss: 2.55517 (0.038 sec/batch, 1703.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:09,599] [train step1461] D loss: 0.36969 G loss: 3.03318 (0.037 sec/batch, 1718.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:09,981] [train step1470] D loss: 0.36224 G loss: 2.14640 (0.038 sec/batch, 1690.857 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:10,389] [train step1481] D loss: 0.36637 G loss: 2.84413 (0.049 sec/batch, 1304.776 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:10,774] [train step1491] D loss: 0.36446 G loss: 2.38807 (0.038 sec/batch, 1681.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:11,154] [train step1500] D loss: 0.35701 G loss: 2.87008 (0.040 sec/batch, 1605.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:11,548] [train step1510] D loss: 0.35487 G loss: 2.68778 (0.038 sec/batch, 1680.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:11,927] [train step1520] D loss: 0.35425 G loss: 2.17346 (0.039 sec/batch, 1635.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:12,321] [train step1530] D loss: 0.36378 G loss: 2.61470 (0.040 sec/batch, 1584.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:12,711] [train step1541] D loss: 0.38577 G loss: 2.08068 (0.037 sec/batch, 1733.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:13,094] [train step1551] D loss: 0.36533 G loss: 2.93379 (0.035 sec/batch, 1844.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:13,478] [train step1560] D loss: 0.34843 G loss: 2.33876 (0.039 sec/batch, 1641.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:13,864] [train step1570] D loss: 0.36563 G loss: 2.88584 (0.038 sec/batch, 1673.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:14,235] [train step1581] D loss: 0.37709 G loss: 3.26115 (0.037 sec/batch, 1719.230 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:14,618] [train step1590] D loss: 0.36278 G loss: 2.65984 (0.036 sec/batch, 1761.700 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:14,994] [train step1600] D loss: 0.35057 G loss: 2.64399 (0.035 sec/batch, 1849.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:15,373] [train step1611] D loss: 0.35270 G loss: 2.41584 (0.033 sec/batch, 1913.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:15,761] [train step1620] D loss: 0.35516 G loss: 2.64009 (0.034 sec/batch, 1884.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:16,141] [train step1630] D loss: 0.35675 G loss: 2.50348 (0.037 sec/batch, 1732.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:16,527] [train step1641] D loss: 0.36334 G loss: 2.45459 (0.036 sec/batch, 1761.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:16,916] [train step1650] D loss: 0.36884 G loss: 2.29390 (0.035 sec/batch, 1814.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:17,293] [train step1661] D loss: 0.35968 G loss: 2.43467 (0.036 sec/batch, 1770.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:17,677] [train step1671] D loss: 0.37347 G loss: 3.26069 (0.038 sec/batch, 1706.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:18,056] [train step1680] D loss: 0.35632 G loss: 2.45925 (0.038 sec/batch, 1679.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:18,432] [train step1690] D loss: 0.37242 G loss: 3.11831 (0.037 sec/batch, 1726.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:18,821] [train step1701] D loss: 0.36047 G loss: 2.01148 (0.036 sec/batch, 1766.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:19,201] [train step1710] D loss: 0.39077 G loss: 3.50220 (0.035 sec/batch, 1835.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:19,583] [train step1721] D loss: 0.36460 G loss: 1.89607 (0.039 sec/batch, 1656.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:19,983] [train step1730] D loss: 0.35330 G loss: 2.71630 (0.038 sec/batch, 1688.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:20,357] [train step1740] D loss: 0.36886 G loss: 2.01423 (0.032 sec/batch, 2031.217 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:20,744] [train step1750] D loss: 0.34838 G loss: 2.56288 (0.041 sec/batch, 1566.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:21,135] [train step1761] D loss: 0.38974 G loss: 3.55237 (0.040 sec/batch, 1596.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:21,529] [train step1770] D loss: 0.34653 G loss: 2.67757 (0.037 sec/batch, 1733.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:21,910] [train step1781] D loss: 0.38371 G loss: 3.37271 (0.038 sec/batch, 1694.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:22,301] [train step1790] D loss: 0.36348 G loss: 1.98941 (0.041 sec/batch, 1572.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:22,681] [train step1800] D loss: 0.35668 G loss: 2.97051 (0.041 sec/batch, 1564.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:23,064] [train step1810] D loss: 0.35043 G loss: 2.19451 (0.040 sec/batch, 1608.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:23,447] [train step1821] D loss: 0.36053 G loss: 2.23670 (0.037 sec/batch, 1744.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:23,847] [train step1830] D loss: 0.35028 G loss: 2.45964 (0.037 sec/batch, 1708.735 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:24,234] [train step1840] D loss: 0.35114 G loss: 2.35612 (0.037 sec/batch, 1737.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:24,627] [train step1850] D loss: 0.35798 G loss: 2.87050 (0.041 sec/batch, 1544.908 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:25,015] [train step1860] D loss: 0.34665 G loss: 2.50734 (0.037 sec/batch, 1716.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:25,408] [train step1870] D loss: 0.35009 G loss: 2.39202 (0.041 sec/batch, 1542.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:25,795] [train step1881] D loss: 0.35041 G loss: 2.71614 (0.039 sec/batch, 1662.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:26,193] [train step1890] D loss: 0.35556 G loss: 2.96123 (0.040 sec/batch, 1587.211 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:26,571] [train step1900] D loss: 0.36733 G loss: 3.16848 (0.037 sec/batch, 1710.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:26,962] [train step1911] D loss: 0.36140 G loss: 2.02029 (0.033 sec/batch, 1927.391 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:27,349] [train step1920] D loss: 0.37549 G loss: 3.34313 (0.036 sec/batch, 1771.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:27,734] [train step1931] D loss: 0.37162 G loss: 2.04236 (0.039 sec/batch, 1626.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:28,127] [train step1941] D loss: 0.34825 G loss: 2.53346 (0.036 sec/batch, 1789.391 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:28,508] [train step1950] D loss: 0.34819 G loss: 2.10536 (0.043 sec/batch, 1472.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:28,894] [train step1961] D loss: 0.34629 G loss: 2.42577 (0.039 sec/batch, 1644.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:29,289] [train step1971] D loss: 0.34609 G loss: 2.71748 (0.039 sec/batch, 1625.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:29,674] [train step1980] D loss: 0.34853 G loss: 2.62690 (0.038 sec/batch, 1698.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:30,067] [train step1991] D loss: 0.36191 G loss: 3.01861 (0.036 sec/batch, 1761.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:30,456] [train step2001] D loss: 0.34684 G loss: 2.51236 (0.038 sec/batch, 1686.619 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:10:30,456] Saved checkpoint at 2000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:31,035] [train step2010] D loss: 0.34447 G loss: 2.55147 (0.041 sec/batch, 1560.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:31,414] [train step2021] D loss: 0.36953 G loss: 1.91527 (0.036 sec/batch, 1769.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:31,825] [train step2030] D loss: 0.34487 G loss: 2.56186 (0.039 sec/batch, 1634.211 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:32,223] [train step2040] D loss: 0.34366 G loss: 2.25530 (0.035 sec/batch, 1807.403 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:32,614] [train step2051] D loss: 0.34050 G loss: 2.60599 (0.040 sec/batch, 1597.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:32,996] [train step2061] D loss: 0.36940 G loss: 1.88456 (0.039 sec/batch, 1633.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:33,394] [train step2070] D loss: 0.35429 G loss: 2.99265 (0.038 sec/batch, 1698.357 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:33,789] [train step2080] D loss: 0.34206 G loss: 2.29658 (0.042 sec/batch, 1531.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:34,184] [train step2091] D loss: 0.34351 G loss: 2.40590 (0.040 sec/batch, 1601.338 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:34,563] [train step2100] D loss: 0.34811 G loss: 2.11017 (0.030 sec/batch, 2133.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:34,945] [train step2111] D loss: 0.34738 G loss: 2.37607 (0.038 sec/batch, 1685.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:35,338] [train step2121] D loss: 0.35352 G loss: 2.77932 (0.038 sec/batch, 1700.228 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:35,718] [train step2130] D loss: 0.34454 G loss: 2.35191 (0.035 sec/batch, 1852.800 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:36,103] [train step2141] D loss: 0.34558 G loss: 2.69806 (0.040 sec/batch, 1619.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:36,493] [train step2151] D loss: 0.35612 G loss: 2.98851 (0.037 sec/batch, 1719.913 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:36,873] [train step2160] D loss: 0.34941 G loss: 2.25261 (0.037 sec/batch, 1712.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:37,260] [train step2171] D loss: 0.34567 G loss: 2.15588 (0.036 sec/batch, 1775.590 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:37,640] [train step2181] D loss: 0.34501 G loss: 2.23801 (0.036 sec/batch, 1759.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:38,030] [train step2190] D loss: 0.34761 G loss: 2.16442 (0.042 sec/batch, 1526.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:38,422] [train step2201] D loss: 0.35366 G loss: 2.21609 (0.036 sec/batch, 1765.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:38,799] [train step2210] D loss: 0.35298 G loss: 2.22322 (0.035 sec/batch, 1845.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:39,184] [train step2220] D loss: 0.34301 G loss: 2.53462 (0.038 sec/batch, 1679.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:39,565] [train step2231] D loss: 0.33984 G loss: 2.33575 (0.041 sec/batch, 1579.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:39,949] [train step2241] D loss: 0.34833 G loss: 2.41159 (0.038 sec/batch, 1684.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:40,342] [train step2250] D loss: 0.34351 G loss: 2.18579 (0.041 sec/batch, 1551.874 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:40,726] [train step2260] D loss: 0.34451 G loss: 2.58201 (0.040 sec/batch, 1608.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:41,099] [train step2271] D loss: 0.34662 G loss: 2.80647 (0.033 sec/batch, 1945.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:41,489] [train step2280] D loss: 0.34933 G loss: 2.43126 (0.047 sec/batch, 1354.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:41,866] [train step2290] D loss: 0.34732 G loss: 2.28373 (0.037 sec/batch, 1719.065 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:42,269] [train step2301] D loss: 0.34554 G loss: 2.34104 (0.034 sec/batch, 1874.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:42,664] [train step2310] D loss: 0.34315 G loss: 2.34041 (0.040 sec/batch, 1615.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:43,035] [train step2321] D loss: 0.34036 G loss: 2.17705 (0.036 sec/batch, 1773.971 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:43,414] [train step2331] D loss: 0.33992 G loss: 2.19090 (0.034 sec/batch, 1866.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:43,787] [train step2340] D loss: 0.34045 G loss: 2.34636 (0.035 sec/batch, 1847.876 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:44,159] [train step2351] D loss: 0.34361 G loss: 2.56436 (0.038 sec/batch, 1674.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:44,547] [train step2361] D loss: 0.34674 G loss: 2.19332 (0.036 sec/batch, 1784.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:44,918] [train step2370] D loss: 0.37053 G loss: 3.31061 (0.037 sec/batch, 1719.726 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:45,288] [train step2380] D loss: 0.33982 G loss: 2.46636 (0.034 sec/batch, 1865.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:45,667] [train step2391] D loss: 0.34321 G loss: 2.74299 (0.038 sec/batch, 1696.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:46,041] [train step2400] D loss: 0.34192 G loss: 2.45357 (0.035 sec/batch, 1850.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:46,426] [train step2411] D loss: 0.33863 G loss: 2.24928 (0.045 sec/batch, 1424.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:46,802] [train step2421] D loss: 0.33843 G loss: 2.46177 (0.041 sec/batch, 1563.972 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:47,171] [train step2430] D loss: 0.34226 G loss: 2.28433 (0.035 sec/batch, 1844.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:47,548] [train step2441] D loss: 0.33826 G loss: 2.34249 (0.032 sec/batch, 1977.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:47,919] [train step2451] D loss: 0.35240 G loss: 2.23277 (0.032 sec/batch, 2017.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:48,299] [train step2460] D loss: 0.35280 G loss: 1.99024 (0.037 sec/batch, 1708.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:48,687] [train step2471] D loss: 0.33813 G loss: 2.59753 (0.038 sec/batch, 1703.217 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:49,062] [train step2481] D loss: 0.35546 G loss: 3.06808 (0.037 sec/batch, 1731.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:49,434] [train step2490] D loss: 0.33931 G loss: 2.44374 (0.040 sec/batch, 1616.429 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:49,818] [train step2501] D loss: 0.37169 G loss: 3.30250 (0.036 sec/batch, 1773.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:50,193] [train step2511] D loss: 0.33956 G loss: 2.28527 (0.036 sec/batch, 1758.054 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:50,577] [train step2520] D loss: 0.34324 G loss: 2.76005 (0.039 sec/batch, 1653.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:50,951] [train step2530] D loss: 0.34188 G loss: 2.70264 (0.035 sec/batch, 1834.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:51,324] [train step2541] D loss: 0.34138 G loss: 2.58093 (0.039 sec/batch, 1632.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:51,710] [train step2550] D loss: 0.33946 G loss: 2.22453 (0.034 sec/batch, 1895.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:52,094] [train step2560] D loss: 0.34146 G loss: 2.40637 (0.042 sec/batch, 1521.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:52,472] [train step2571] D loss: 0.34802 G loss: 2.77343 (0.038 sec/batch, 1675.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:52,880] [train step2580] D loss: 0.34124 G loss: 2.47816 (0.039 sec/batch, 1653.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:53,250] [train step2590] D loss: 0.34371 G loss: 2.80903 (0.028 sec/batch, 2276.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:53,644] [train step2600] D loss: 0.34359 G loss: 2.10746 (0.037 sec/batch, 1713.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:54,029] [train step2610] D loss: 0.34785 G loss: 2.86567 (0.029 sec/batch, 2239.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:54,418] [train step2621] D loss: 0.33987 G loss: 2.28711 (0.039 sec/batch, 1654.191 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:54,812] [train step2630] D loss: 0.34480 G loss: 2.10214 (0.041 sec/batch, 1547.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:55,192] [train step2640] D loss: 0.35603 G loss: 3.11744 (0.038 sec/batch, 1670.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:55,573] [train step2650] D loss: 0.33740 G loss: 2.32571 (0.039 sec/batch, 1638.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:55,960] [train step2660] D loss: 0.33923 G loss: 2.14277 (0.030 sec/batch, 2105.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:56,354] [train step2670] D loss: 0.34140 G loss: 2.71168 (0.044 sec/batch, 1454.296 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:56,736] [train step2680] D loss: 0.34850 G loss: 2.05876 (0.039 sec/batch, 1621.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:57,120] [train step2690] D loss: 0.33959 G loss: 2.18447 (0.036 sec/batch, 1774.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:57,514] [train step2700] D loss: 0.33928 G loss: 2.19348 (0.038 sec/batch, 1664.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:57,899] [train step2710] D loss: 0.33530 G loss: 2.31767 (0.034 sec/batch, 1862.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:58,284] [train step2720] D loss: 0.33781 G loss: 2.59929 (0.043 sec/batch, 1475.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:58,668] [train step2730] D loss: 0.34748 G loss: 2.13625 (0.041 sec/batch, 1562.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:59,058] [train step2740] D loss: 0.33731 G loss: 2.41644 (0.039 sec/batch, 1655.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:59,430] [train step2751] D loss: 0.34264 G loss: 2.55235 (0.035 sec/batch, 1850.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:10:59,821] [train step2760] D loss: 0.33722 G loss: 2.50508 (0.037 sec/batch, 1727.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:00,195] [train step2771] D loss: 0.35227 G loss: 2.99817 (0.032 sec/batch, 1969.908 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:00,573] [train step2781] D loss: 0.33659 G loss: 2.41881 (0.036 sec/batch, 1753.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:00,965] [train step2790] D loss: 0.33912 G loss: 2.27413 (0.038 sec/batch, 1677.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:01,358] [train step2800] D loss: 0.33814 G loss: 2.29179 (0.052 sec/batch, 1241.515 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:01,746] [train step2810] D loss: 0.33627 G loss: 2.35811 (0.041 sec/batch, 1565.915 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:02,133] [train step2820] D loss: 0.34926 G loss: 2.11359 (0.034 sec/batch, 1876.738 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:02,525] [train step2830] D loss: 0.33579 G loss: 2.37133 (0.036 sec/batch, 1757.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:02,917] [train step2841] D loss: 0.35539 G loss: 3.07399 (0.037 sec/batch, 1728.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:03,314] [train step2850] D loss: 0.33555 G loss: 2.28492 (0.045 sec/batch, 1411.370 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:03,696] [train step2861] D loss: 0.33753 G loss: 2.61224 (0.038 sec/batch, 1678.078 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:04,086] [train step2870] D loss: 0.34127 G loss: 2.01927 (0.038 sec/batch, 1694.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:04,519] [train step2880] D loss: 0.35842 G loss: 3.15366 (0.049 sec/batch, 1312.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:04,930] [train step2890] D loss: 0.33829 G loss: 2.34493 (0.045 sec/batch, 1423.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:05,323] [train step2900] D loss: 0.33877 G loss: 2.16853 (0.038 sec/batch, 1701.716 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:05,719] [train step2910] D loss: 0.33617 G loss: 2.47548 (0.048 sec/batch, 1323.137 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:06,123] [train step2921] D loss: 0.33589 G loss: 2.26613 (0.043 sec/batch, 1497.999 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:06,532] [train step2930] D loss: 0.33908 G loss: 2.19360 (0.038 sec/batch, 1690.283 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:06,928] [train step2940] D loss: 0.33099 G loss: 2.38642 (0.038 sec/batch, 1671.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:07,340] [train step2951] D loss: 0.33835 G loss: 2.28718 (0.040 sec/batch, 1597.754 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:07,747] [train step2961] D loss: 0.34750 G loss: 2.94490 (0.055 sec/batch, 1156.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:08,162] [train step2970] D loss: 0.33692 G loss: 2.28487 (0.039 sec/batch, 1661.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:08,561] [train step2981] D loss: 0.33333 G loss: 2.45869 (0.043 sec/batch, 1498.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:08,955] [train step2990] D loss: 0.33720 G loss: 2.34373 (0.041 sec/batch, 1570.735 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:09,359] [train step3000] D loss: 0.33826 G loss: 2.64158 (0.037 sec/batch, 1739.586 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:11:09,360] Saved checkpoint at 3000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:09,949] [train step3011] D loss: 0.33774 G loss: 2.23847 (0.038 sec/batch, 1681.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:10,347] [train step3020] D loss: 0.33549 G loss: 2.20690 (0.034 sec/batch, 1900.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:10,742] [train step3030] D loss: 0.33404 G loss: 2.55919 (0.040 sec/batch, 1607.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:11,152] [train step3040] D loss: 0.33710 G loss: 2.59862 (0.038 sec/batch, 1705.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:11,546] [train step3051] D loss: 0.34146 G loss: 2.07725 (0.040 sec/batch, 1606.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:11,941] [train step3060] D loss: 0.33769 G loss: 2.69222 (0.040 sec/batch, 1595.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:12,335] [train step3071] D loss: 0.33661 G loss: 2.18719 (0.039 sec/batch, 1656.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:12,728] [train step3080] D loss: 0.33243 G loss: 2.30647 (0.036 sec/batch, 1764.920 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:13,143] [train step3090] D loss: 0.33655 G loss: 2.40399 (0.045 sec/batch, 1429.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:13,543] [train step3101] D loss: 0.33643 G loss: 2.26796 (0.040 sec/batch, 1593.117 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:13,936] [train step3111] D loss: 0.33697 G loss: 2.22513 (0.039 sec/batch, 1661.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:14,333] [train step3120] D loss: 0.33832 G loss: 2.66190 (0.037 sec/batch, 1738.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:14,731] [train step3131] D loss: 0.33659 G loss: 2.61190 (0.034 sec/batch, 1874.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:15,117] [train step3140] D loss: 0.34343 G loss: 2.01250 (0.037 sec/batch, 1709.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:15,499] [train step3150] D loss: 0.34789 G loss: 2.95855 (0.038 sec/batch, 1670.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:15,874] [train step3161] D loss: 0.33587 G loss: 2.34005 (0.036 sec/batch, 1772.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:16,256] [train step3171] D loss: 0.33815 G loss: 2.32175 (0.041 sec/batch, 1547.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:16,637] [train step3180] D loss: 0.33708 G loss: 2.24110 (0.041 sec/batch, 1551.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:17,019] [train step3191] D loss: 0.33514 G loss: 2.28854 (0.036 sec/batch, 1774.440 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:17,406] [train step3201] D loss: 0.34002 G loss: 2.05047 (0.036 sec/batch, 1768.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:17,782] [train step3210] D loss: 0.34367 G loss: 2.80271 (0.032 sec/batch, 2000.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:18,152] [train step3221] D loss: 0.33897 G loss: 2.08082 (0.039 sec/batch, 1643.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:18,539] [train step3230] D loss: 0.33327 G loss: 2.37755 (0.037 sec/batch, 1745.729 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:18,903] [train step3240] D loss: 0.34025 G loss: 2.08242 (0.030 sec/batch, 2119.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:19,284] [train step3251] D loss: 0.33130 G loss: 2.26617 (0.037 sec/batch, 1719.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:19,663] [train step3260] D loss: 0.34000 G loss: 2.71223 (0.036 sec/batch, 1768.384 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:20,038] [train step3270] D loss: 0.33481 G loss: 2.20071 (0.036 sec/batch, 1754.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:20,426] [train step3280] D loss: 0.33237 G loss: 2.48106 (0.040 sec/batch, 1608.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:20,807] [train step3290] D loss: 0.33460 G loss: 2.54470 (0.037 sec/batch, 1728.730 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:21,178] [train step3300] D loss: 0.33216 G loss: 2.27723 (0.035 sec/batch, 1812.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:21,568] [train step3311] D loss: 0.33301 G loss: 2.33452 (0.036 sec/batch, 1783.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:21,949] [train step3321] D loss: 0.33789 G loss: 2.68702 (0.038 sec/batch, 1696.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:22,349] [train step3330] D loss: 0.33607 G loss: 2.17046 (0.043 sec/batch, 1501.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:22,741] [train step3341] D loss: 0.33340 G loss: 2.38789 (0.039 sec/batch, 1655.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:23,121] [train step3350] D loss: 0.33356 G loss: 2.37924 (0.040 sec/batch, 1587.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:23,515] [train step3360] D loss: 0.33430 G loss: 2.40176 (0.042 sec/batch, 1512.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:23,900] [train step3371] D loss: 0.33414 G loss: 2.59071 (0.038 sec/batch, 1695.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:24,281] [train step3380] D loss: 0.33291 G loss: 2.48923 (0.039 sec/batch, 1647.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:24,687] [train step3390] D loss: 0.33465 G loss: 2.45981 (0.041 sec/batch, 1576.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:25,069] [train step3401] D loss: 0.33526 G loss: 2.25641 (0.039 sec/batch, 1651.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:25,453] [train step3410] D loss: 0.33638 G loss: 2.19052 (0.038 sec/batch, 1697.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:25,849] [train step3420] D loss: 0.33674 G loss: 2.27731 (0.043 sec/batch, 1505.392 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:26,226] [train step3430] D loss: 0.33554 G loss: 2.19839 (0.038 sec/batch, 1699.776 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:26,623] [train step3440] D loss: 0.33819 G loss: 2.67633 (0.038 sec/batch, 1665.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:27,004] [train step3450] D loss: 0.33509 G loss: 2.20500 (0.036 sec/batch, 1765.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:27,382] [train step3461] D loss: 0.33175 G loss: 2.43445 (0.036 sec/batch, 1779.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:27,770] [train step3471] D loss: 0.33506 G loss: 2.45766 (0.036 sec/batch, 1766.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:28,157] [train step3480] D loss: 0.33150 G loss: 2.45536 (0.042 sec/batch, 1525.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:28,531] [train step3490] D loss: 0.33190 G loss: 2.46020 (0.030 sec/batch, 2113.066 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:28,934] [train step3500] D loss: 0.33236 G loss: 2.25526 (0.039 sec/batch, 1629.568 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:29,315] [train step3510] D loss: 0.33953 G loss: 2.76366 (0.042 sec/batch, 1539.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:29,700] [train step3521] D loss: 0.33898 G loss: 2.67898 (0.037 sec/batch, 1735.862 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:30,079] [train step3531] D loss: 0.33532 G loss: 2.59342 (0.038 sec/batch, 1703.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:30,469] [train step3540] D loss: 0.33514 G loss: 2.15248 (0.045 sec/batch, 1438.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:30,858] [train step3550] D loss: 0.33202 G loss: 2.39362 (0.038 sec/batch, 1699.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:31,244] [train step3561] D loss: 0.33467 G loss: 2.27124 (0.036 sec/batch, 1793.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:31,627] [train step3570] D loss: 0.33222 G loss: 2.32868 (0.036 sec/batch, 1759.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:32,023] [train step3580] D loss: 0.33329 G loss: 2.29210 (0.037 sec/batch, 1741.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:32,404] [train step3590] D loss: 0.34613 G loss: 2.01460 (0.038 sec/batch, 1681.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:32,794] [train step3600] D loss: 0.35568 G loss: 3.10297 (0.038 sec/batch, 1700.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:33,172] [train step3611] D loss: 0.33511 G loss: 2.19046 (0.036 sec/batch, 1762.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:33,553] [train step3620] D loss: 0.33201 G loss: 2.29190 (0.036 sec/batch, 1779.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:33,949] [train step3630] D loss: 0.34279 G loss: 1.97177 (0.038 sec/batch, 1705.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:34,328] [train step3640] D loss: 0.33321 G loss: 2.21128 (0.036 sec/batch, 1768.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:34,719] [train step3651] D loss: 0.34227 G loss: 2.85035 (0.036 sec/batch, 1755.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:35,128] [train step3660] D loss: 0.33412 G loss: 2.16962 (0.039 sec/batch, 1627.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:35,506] [train step3671] D loss: 0.33532 G loss: 2.60236 (0.047 sec/batch, 1370.324 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:35,891] [train step3681] D loss: 0.33988 G loss: 2.76848 (0.032 sec/batch, 1984.735 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:36,266] [train step3690] D loss: 0.33100 G loss: 2.38466 (0.037 sec/batch, 1715.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:36,650] [train step3700] D loss: 0.35247 G loss: 3.06374 (0.034 sec/batch, 1901.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:37,046] [train step3711] D loss: 0.34303 G loss: 1.97086 (0.035 sec/batch, 1810.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:37,429] [train step3720] D loss: 0.36157 G loss: 3.21872 (0.039 sec/batch, 1654.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:37,817] [train step3730] D loss: 0.33387 G loss: 2.17226 (0.038 sec/batch, 1667.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:38,208] [train step3740] D loss: 0.33134 G loss: 2.41298 (0.040 sec/batch, 1598.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:38,594] [train step3750] D loss: 0.33199 G loss: 2.34000 (0.037 sec/batch, 1710.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:38,985] [train step3761] D loss: 0.33799 G loss: 2.70276 (0.037 sec/batch, 1738.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:39,365] [train step3770] D loss: 0.33766 G loss: 2.64513 (0.026 sec/batch, 2450.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:39,766] [train step3780] D loss: 0.33420 G loss: 2.32539 (0.037 sec/batch, 1729.732 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:40,173] [train step3790] D loss: 0.33493 G loss: 2.24522 (0.041 sec/batch, 1565.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:40,548] [train step3801] D loss: 0.33512 G loss: 2.12484 (0.036 sec/batch, 1776.577 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:40,945] [train step3810] D loss: 0.33438 G loss: 2.50092 (0.048 sec/batch, 1345.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:41,326] [train step3820] D loss: 0.33694 G loss: 2.11297 (0.036 sec/batch, 1792.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:41,717] [train step3830] D loss: 0.33063 G loss: 2.30697 (0.038 sec/batch, 1692.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:42,107] [train step3840] D loss: 0.33130 G loss: 2.41891 (0.042 sec/batch, 1536.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:42,489] [train step3850] D loss: 0.33262 G loss: 2.29520 (0.037 sec/batch, 1736.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:42,870] [train step3861] D loss: 0.33223 G loss: 2.30825 (0.041 sec/batch, 1560.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:43,270] [train step3870] D loss: 0.33224 G loss: 2.42914 (0.040 sec/batch, 1586.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:43,637] [train step3880] D loss: 0.33201 G loss: 2.27415 (0.032 sec/batch, 1974.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:44,025] [train step3891] D loss: 0.33502 G loss: 2.15330 (0.050 sec/batch, 1272.948 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:44,395] [train step3900] D loss: 0.34209 G loss: 2.82559 (0.035 sec/batch, 1816.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:44,767] [train step3910] D loss: 0.33169 G loss: 2.39354 (0.037 sec/batch, 1738.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:45,145] [train step3920] D loss: 0.33198 G loss: 2.52827 (0.034 sec/batch, 1866.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:45,536] [train step3930] D loss: 0.33181 G loss: 2.24408 (0.038 sec/batch, 1663.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:45,912] [train step3940] D loss: 0.33165 G loss: 2.43579 (0.036 sec/batch, 1784.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:46,300] [train step3950] D loss: 0.33112 G loss: 2.29870 (0.038 sec/batch, 1668.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:46,677] [train step3960] D loss: 0.33363 G loss: 2.61088 (0.036 sec/batch, 1778.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:47,056] [train step3971] D loss: 0.33253 G loss: 2.22166 (0.039 sec/batch, 1648.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:47,440] [train step3981] D loss: 0.33183 G loss: 2.53791 (0.036 sec/batch, 1785.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:47,814] [train step3990] D loss: 0.33518 G loss: 2.03418 (0.036 sec/batch, 1775.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:48,201] [train step4001] D loss: 0.33096 G loss: 2.29770 (0.043 sec/batch, 1501.250 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:11:48,201] Saved checkpoint at 4000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:48,766] [train step4010] D loss: 0.33639 G loss: 2.67171 (0.034 sec/batch, 1875.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:49,155] [train step4020] D loss: 0.33469 G loss: 2.04818 (0.037 sec/batch, 1719.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:49,539] [train step4031] D loss: 0.33381 G loss: 2.19031 (0.036 sec/batch, 1771.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:49,914] [train step4041] D loss: 0.32881 G loss: 2.30272 (0.035 sec/batch, 1849.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:50,301] [train step4050] D loss: 0.33165 G loss: 2.51593 (0.036 sec/batch, 1802.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:50,676] [train step4061] D loss: 0.33296 G loss: 2.42059 (0.038 sec/batch, 1667.394 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:51,041] [train step4070] D loss: 0.33055 G loss: 2.29680 (0.031 sec/batch, 2033.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:51,428] [train step4080] D loss: 0.33196 G loss: 2.39540 (0.036 sec/batch, 1762.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:51,819] [train step4090] D loss: 0.34470 G loss: 2.90087 (0.041 sec/batch, 1561.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:52,204] [train step4100] D loss: 0.33677 G loss: 2.58369 (0.039 sec/batch, 1650.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:52,585] [train step4110] D loss: 0.34036 G loss: 2.64050 (0.041 sec/batch, 1550.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:52,963] [train step4120] D loss: 0.33937 G loss: 2.03825 (0.037 sec/batch, 1716.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:53,350] [train step4131] D loss: 0.33531 G loss: 2.07477 (0.038 sec/batch, 1704.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:53,730] [train step4140] D loss: 0.33464 G loss: 2.52100 (0.038 sec/batch, 1703.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:54,121] [train step4151] D loss: 0.33645 G loss: 2.53494 (0.036 sec/batch, 1784.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:54,516] [train step4160] D loss: 0.34272 G loss: 2.70346 (0.041 sec/batch, 1556.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:54,901] [train step4170] D loss: 0.33387 G loss: 2.46485 (0.039 sec/batch, 1650.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:55,281] [train step4180] D loss: 0.38534 G loss: 3.45140 (0.038 sec/batch, 1663.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:55,669] [train step4190] D loss: 0.36000 G loss: 1.86192 (0.035 sec/batch, 1821.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:56,060] [train step4200] D loss: 0.42075 G loss: 3.92779 (0.039 sec/batch, 1643.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:56,448] [train step4210] D loss: 0.40595 G loss: 1.50411 (0.031 sec/batch, 2061.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:56,845] [train step4221] D loss: 0.43077 G loss: 3.83870 (0.041 sec/batch, 1554.201 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:57,224] [train step4230] D loss: 1.39206 G loss: 0.41973 (0.041 sec/batch, 1570.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:57,611] [train step4241] D loss: 0.38926 G loss: 2.89948 (0.034 sec/batch, 1887.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:57,985] [train step4250] D loss: 0.40093 G loss: 3.72601 (0.030 sec/batch, 2104.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:58,375] [train step4260] D loss: 0.38571 G loss: 1.77588 (0.040 sec/batch, 1582.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:58,772] [train step4270] D loss: 0.34774 G loss: 2.53512 (0.040 sec/batch, 1608.457 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:59,158] [train step4280] D loss: 0.51155 G loss: 5.03534 (0.036 sec/batch, 1763.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:59,549] [train step4290] D loss: 0.35708 G loss: 3.02629 (0.040 sec/batch, 1591.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:11:59,940] [train step4301] D loss: 0.90857 G loss: 9.07884 (0.038 sec/batch, 1676.558 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:00,322] [train step4310] D loss: 0.37671 G loss: 3.05932 (0.038 sec/batch, 1704.904 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:00,715] [train step4320] D loss: 0.45954 G loss: 1.37286 (0.040 sec/batch, 1595.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:01,096] [train step4330] D loss: 2.45474 G loss: 0.10315 (0.040 sec/batch, 1602.983 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:01,478] [train step4341] D loss: 1.09132 G loss: 0.55009 (0.039 sec/batch, 1634.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:01,867] [train step4350] D loss: 0.73186 G loss: 7.29442 (0.035 sec/batch, 1815.323 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:02,248] [train step4361] D loss: 0.73024 G loss: 0.98112 (0.041 sec/batch, 1543.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:02,640] [train step4371] D loss: 0.57145 G loss: 5.67426 (0.037 sec/batch, 1730.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:03,028] [train step4380] D loss: 0.82456 G loss: 0.73141 (0.040 sec/batch, 1615.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:03,406] [train step4390] D loss: 1.13372 G loss: 11.30904 (0.036 sec/batch, 1782.286 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:03,802] [train step4401] D loss: 0.35016 G loss: 2.59418 (0.037 sec/batch, 1715.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:04,188] [train step4410] D loss: 0.47719 G loss: 1.31523 (0.036 sec/batch, 1798.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:04,576] [train step4420] D loss: 0.61732 G loss: 6.11556 (0.039 sec/batch, 1636.732 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:04,966] [train step4430] D loss: 0.64119 G loss: 0.92597 (0.037 sec/batch, 1728.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:05,354] [train step4440] D loss: 0.76823 G loss: 7.66738 (0.040 sec/batch, 1607.003 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:05,748] [train step4450] D loss: 0.66167 G loss: 6.59990 (0.043 sec/batch, 1486.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:06,127] [train step4460] D loss: 0.57930 G loss: 1.03499 (0.033 sec/batch, 1946.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:06,519] [train step4470] D loss: 0.69980 G loss: 6.98623 (0.042 sec/batch, 1517.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:06,919] [train step4481] D loss: 0.58216 G loss: 5.78556 (0.036 sec/batch, 1779.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:07,306] [train step4491] D loss: 0.55004 G loss: 1.07286 (0.045 sec/batch, 1437.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:07,699] [train step4500] D loss: 0.64831 G loss: 6.46231 (0.041 sec/batch, 1549.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:08,083] [train step4510] D loss: 0.50494 G loss: 1.31684 (0.036 sec/batch, 1802.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:08,464] [train step4521] D loss: 0.36834 G loss: 2.22538 (0.037 sec/batch, 1722.960 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:08,851] [train step4530] D loss: 0.56213 G loss: 5.57485 (0.036 sec/batch, 1793.228 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:09,237] [train step4540] D loss: 0.37983 G loss: 3.43430 (0.037 sec/batch, 1736.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:09,624] [train step4551] D loss: 0.43474 G loss: 4.16502 (0.034 sec/batch, 1859.860 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:10,020] [train step4560] D loss: 0.42137 G loss: 4.01695 (0.035 sec/batch, 1818.816 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:10,405] [train step4571] D loss: 0.36246 G loss: 1.94076 (0.038 sec/batch, 1690.719 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:10,801] [train step4580] D loss: 0.35763 G loss: 3.08673 (0.038 sec/batch, 1692.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:11,181] [train step4590] D loss: 0.34952 G loss: 2.03950 (0.037 sec/batch, 1745.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:11,561] [train step4601] D loss: 0.34305 G loss: 2.51759 (0.036 sec/batch, 1789.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:11,945] [train step4610] D loss: 0.33827 G loss: 2.40368 (0.037 sec/batch, 1738.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:12,328] [train step4620] D loss: 0.34421 G loss: 2.05264 (0.042 sec/batch, 1541.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:12,707] [train step4630] D loss: 0.34147 G loss: 2.60302 (0.040 sec/batch, 1594.215 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:13,114] [train step4640] D loss: 0.34230 G loss: 2.64610 (0.051 sec/batch, 1258.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:13,494] [train step4650] D loss: 0.34140 G loss: 2.32829 (0.037 sec/batch, 1740.319 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:13,883] [train step4661] D loss: 0.33699 G loss: 2.46496 (0.036 sec/batch, 1756.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:14,254] [train step4670] D loss: 0.34065 G loss: 2.70311 (0.035 sec/batch, 1811.489 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:14,622] [train step4680] D loss: 0.34152 G loss: 2.19614 (0.036 sec/batch, 1787.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:14,996] [train step4691] D loss: 0.33910 G loss: 2.48497 (0.037 sec/batch, 1739.225 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:15,370] [train step4701] D loss: 0.34128 G loss: 2.29021 (0.038 sec/batch, 1674.738 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:15,743] [train step4710] D loss: 0.33569 G loss: 2.38758 (0.037 sec/batch, 1722.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:16,124] [train step4720] D loss: 0.33913 G loss: 2.47161 (0.038 sec/batch, 1697.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:16,505] [train step4731] D loss: 0.33669 G loss: 2.35324 (0.041 sec/batch, 1565.112 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:16,880] [train step4740] D loss: 0.34115 G loss: 2.20983 (0.037 sec/batch, 1753.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:17,274] [train step4750] D loss: 0.34427 G loss: 2.24124 (0.048 sec/batch, 1345.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:17,670] [train step4761] D loss: 0.33806 G loss: 2.49587 (0.038 sec/batch, 1672.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:18,056] [train step4770] D loss: 0.34207 G loss: 2.32333 (0.035 sec/batch, 1811.049 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:18,430] [train step4781] D loss: 0.33874 G loss: 2.34295 (0.035 sec/batch, 1844.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:18,809] [train step4791] D loss: 0.34328 G loss: 2.62433 (0.043 sec/batch, 1483.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:19,193] [train step4800] D loss: 0.35278 G loss: 1.87927 (0.038 sec/batch, 1688.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:19,561] [train step4811] D loss: 0.33900 G loss: 2.43485 (0.038 sec/batch, 1696.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:19,932] [train step4821] D loss: 0.41849 G loss: 3.97196 (0.036 sec/batch, 1753.643 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:20,331] [train step4830] D loss: 0.34351 G loss: 2.61546 (0.045 sec/batch, 1414.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:20,710] [train step4841] D loss: 0.68131 G loss: 6.78870 (0.038 sec/batch, 1682.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:21,120] [train step4851] D loss: 0.36684 G loss: 3.18796 (0.037 sec/batch, 1738.786 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:21,498] [train step4860] D loss: 1.84816 G loss: 5.78817 (0.034 sec/batch, 1900.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:21,877] [train step4871] D loss: 0.46447 G loss: 1.29764 (0.036 sec/batch, 1793.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:22,274] [train step4881] D loss: 0.42490 G loss: 4.03199 (0.035 sec/batch, 1824.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:22,651] [train step4890] D loss: 0.40244 G loss: 1.61658 (0.035 sec/batch, 1836.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:23,028] [train step4901] D loss: 0.34865 G loss: 2.71170 (0.040 sec/batch, 1590.107 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:23,420] [train step4910] D loss: 0.38946 G loss: 3.56261 (0.039 sec/batch, 1633.147 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:23,798] [train step4920] D loss: 0.33729 G loss: 2.32116 (0.036 sec/batch, 1777.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:24,190] [train step4930] D loss: 0.37142 G loss: 3.29589 (0.040 sec/batch, 1603.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:24,574] [train step4941] D loss: 0.37545 G loss: 1.94419 (0.040 sec/batch, 1591.096 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:24,950] [train step4950] D loss: 0.37977 G loss: 3.43287 (0.034 sec/batch, 1900.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:25,340] [train step4960] D loss: 0.35592 G loss: 1.94935 (0.038 sec/batch, 1672.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:25,719] [train step4971] D loss: 0.34013 G loss: 2.28893 (0.035 sec/batch, 1829.788 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:26,101] [train step4980] D loss: 0.34923 G loss: 1.99236 (0.038 sec/batch, 1682.264 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:26,488] [train step4991] D loss: 0.34194 G loss: 2.22009 (0.041 sec/batch, 1558.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:26,878] [train step5001] D loss: 0.34055 G loss: 2.64228 (0.037 sec/batch, 1733.844 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:12:26,878] Saved checkpoint at 5000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:27,473] [train step5010] D loss: 0.33949 G loss: 2.27006 (0.038 sec/batch, 1664.128 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:27,871] [train step5021] D loss: 0.34764 G loss: 2.50037 (0.050 sec/batch, 1287.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:28,272] [train step5030] D loss: 0.34787 G loss: 2.20994 (0.043 sec/batch, 1489.884 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:28,659] [train step5040] D loss: 0.34845 G loss: 2.85819 (0.043 sec/batch, 1490.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:29,039] [train step5051] D loss: 0.34220 G loss: 2.27690 (0.039 sec/batch, 1634.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:29,431] [train step5061] D loss: 0.34392 G loss: 2.11866 (0.038 sec/batch, 1693.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:29,814] [train step5070] D loss: 0.34231 G loss: 2.67819 (0.037 sec/batch, 1750.784 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:30,195] [train step5081] D loss: 0.34041 G loss: 2.65452 (0.038 sec/batch, 1691.433 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:30,589] [train step5090] D loss: 0.34155 G loss: 2.40982 (0.038 sec/batch, 1687.011 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:30,970] [train step5100] D loss: 0.34159 G loss: 2.34750 (0.037 sec/batch, 1720.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:31,363] [train step5110] D loss: 0.33958 G loss: 2.22549 (0.048 sec/batch, 1335.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:31,744] [train step5121] D loss: 0.34921 G loss: 2.89578 (0.037 sec/batch, 1745.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:32,126] [train step5130] D loss: 0.33662 G loss: 2.50752 (0.039 sec/batch, 1621.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:32,527] [train step5140] D loss: 0.36914 G loss: 3.30169 (0.034 sec/batch, 1881.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:32,918] [train step5151] D loss: 0.33483 G loss: 2.17223 (0.037 sec/batch, 1749.597 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:33,303] [train step5160] D loss: 0.35519 G loss: 3.07241 (0.039 sec/batch, 1660.977 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:33,705] [train step5171] D loss: 0.35441 G loss: 3.02658 (0.043 sec/batch, 1479.016 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:34,078] [train step5181] D loss: 0.33662 G loss: 2.25161 (0.035 sec/batch, 1812.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:34,466] [train step5190] D loss: 0.33472 G loss: 2.45463 (0.031 sec/batch, 2050.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:34,855] [train step5201] D loss: 0.33908 G loss: 2.41733 (0.038 sec/batch, 1695.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:35,255] [train step5210] D loss: 0.33968 G loss: 2.69734 (0.038 sec/batch, 1674.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:35,641] [train step5220] D loss: 0.34651 G loss: 2.24163 (0.037 sec/batch, 1724.133 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:36,025] [train step5231] D loss: 0.33879 G loss: 2.62457 (0.035 sec/batch, 1842.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:36,412] [train step5240] D loss: 0.33457 G loss: 2.43532 (0.052 sec/batch, 1233.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:36,800] [train step5250] D loss: 0.34214 G loss: 2.17659 (0.035 sec/batch, 1805.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:37,184] [train step5261] D loss: 0.34172 G loss: 2.04039 (0.036 sec/batch, 1756.547 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:37,569] [train step5271] D loss: 0.34875 G loss: 2.07502 (0.038 sec/batch, 1666.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:37,946] [train step5280] D loss: 0.33687 G loss: 2.38059 (0.035 sec/batch, 1844.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:38,331] [train step5290] D loss: 0.33560 G loss: 2.38452 (0.038 sec/batch, 1667.208 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:38,741] [train step5301] D loss: 0.33256 G loss: 2.24382 (0.038 sec/batch, 1700.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:39,128] [train step5310] D loss: 0.34159 G loss: 2.18646 (0.035 sec/batch, 1837.403 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:39,505] [train step5320] D loss: 0.33719 G loss: 2.24842 (0.040 sec/batch, 1620.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:39,895] [train step5331] D loss: 0.33864 G loss: 2.66429 (0.034 sec/batch, 1870.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:40,280] [train step5340] D loss: 0.33408 G loss: 2.53959 (0.033 sec/batch, 1925.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:40,676] [train step5351] D loss: 0.33731 G loss: 2.61868 (0.039 sec/batch, 1628.471 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:41,051] [train step5361] D loss: 0.33275 G loss: 2.31279 (0.045 sec/batch, 1429.460 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:41,428] [train step5370] D loss: 0.33791 G loss: 2.66085 (0.040 sec/batch, 1614.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:41,823] [train step5381] D loss: 0.34686 G loss: 2.94375 (0.038 sec/batch, 1672.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:42,200] [train step5391] D loss: 0.33442 G loss: 2.37083 (0.038 sec/batch, 1691.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:42,569] [train step5400] D loss: 0.33321 G loss: 2.39506 (0.029 sec/batch, 2189.201 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:42,978] [train step5411] D loss: 0.33413 G loss: 2.43159 (0.041 sec/batch, 1549.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:43,360] [train step5421] D loss: 0.33720 G loss: 2.30323 (0.035 sec/batch, 1849.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:43,747] [train step5430] D loss: 0.33689 G loss: 2.56311 (0.039 sec/batch, 1631.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:44,133] [train step5441] D loss: 0.35059 G loss: 1.86598 (0.036 sec/batch, 1793.336 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:44,502] [train step5450] D loss: 0.33941 G loss: 2.53043 (0.036 sec/batch, 1754.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:44,888] [train step5460] D loss: 0.34144 G loss: 2.05740 (0.037 sec/batch, 1712.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:45,269] [train step5470] D loss: 0.33423 G loss: 2.28456 (0.037 sec/batch, 1732.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:45,646] [train step5480] D loss: 0.33368 G loss: 2.48648 (0.041 sec/batch, 1567.689 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:46,028] [train step5490] D loss: 0.33481 G loss: 2.26541 (0.042 sec/batch, 1511.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:46,402] [train step5500] D loss: 0.33359 G loss: 2.31378 (0.034 sec/batch, 1856.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:46,776] [train step5511] D loss: 0.33467 G loss: 2.52944 (0.040 sec/batch, 1587.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:47,160] [train step5520] D loss: 0.33447 G loss: 2.44624 (0.044 sec/batch, 1465.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:47,529] [train step5530] D loss: 0.33327 G loss: 2.28590 (0.034 sec/batch, 1893.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:47,916] [train step5540] D loss: 0.34013 G loss: 2.09640 (0.038 sec/batch, 1688.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:48,298] [train step5550] D loss: 0.33471 G loss: 2.20809 (0.040 sec/batch, 1583.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:48,677] [train step5560] D loss: 0.33294 G loss: 2.27108 (0.038 sec/batch, 1666.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:49,079] [train step5570] D loss: 0.33269 G loss: 2.40880 (0.036 sec/batch, 1762.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:49,453] [train step5580] D loss: 0.33313 G loss: 2.30311 (0.037 sec/batch, 1733.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:49,838] [train step5590] D loss: 0.33461 G loss: 2.55124 (0.051 sec/batch, 1266.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:50,211] [train step5601] D loss: 0.33296 G loss: 2.47996 (0.033 sec/batch, 1962.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:50,577] [train step5610] D loss: 0.33209 G loss: 2.23333 (0.029 sec/batch, 2240.678 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:50,976] [train step5620] D loss: 0.33315 G loss: 2.31800 (0.041 sec/batch, 1575.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:51,361] [train step5631] D loss: 0.33557 G loss: 2.26625 (0.036 sec/batch, 1800.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:51,744] [train step5640] D loss: 0.33382 G loss: 2.46132 (0.039 sec/batch, 1654.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:52,133] [train step5651] D loss: 0.33738 G loss: 2.67043 (0.035 sec/batch, 1829.539 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:52,524] [train step5661] D loss: 0.33388 G loss: 2.16404 (0.039 sec/batch, 1626.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:52,910] [train step5670] D loss: 0.33605 G loss: 2.59452 (0.040 sec/batch, 1596.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:53,286] [train step5680] D loss: 0.33478 G loss: 2.62377 (0.038 sec/batch, 1669.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:53,663] [train step5690] D loss: 0.33459 G loss: 2.46157 (0.037 sec/batch, 1716.526 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:54,053] [train step5700] D loss: 0.33975 G loss: 2.75706 (0.037 sec/batch, 1748.799 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:54,437] [train step5710] D loss: 0.33497 G loss: 2.61399 (0.039 sec/batch, 1647.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:54,820] [train step5721] D loss: 0.33689 G loss: 2.22259 (0.039 sec/batch, 1642.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:55,205] [train step5730] D loss: 0.34270 G loss: 2.85457 (0.037 sec/batch, 1718.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:55,593] [train step5741] D loss: 0.34339 G loss: 2.84947 (0.037 sec/batch, 1717.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:55,964] [train step5751] D loss: 0.33279 G loss: 2.40413 (0.038 sec/batch, 1699.862 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:56,356] [train step5760] D loss: 0.33204 G loss: 2.43051 (0.039 sec/batch, 1658.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:56,737] [train step5771] D loss: 0.33317 G loss: 2.41680 (0.037 sec/batch, 1752.224 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:57,120] [train step5780] D loss: 0.33455 G loss: 2.58222 (0.035 sec/batch, 1847.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:57,501] [train step5790] D loss: 0.33343 G loss: 2.26305 (0.037 sec/batch, 1737.210 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:57,879] [train step5800] D loss: 0.33450 G loss: 2.25959 (0.035 sec/batch, 1806.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:58,270] [train step5811] D loss: 0.33099 G loss: 2.34529 (0.035 sec/batch, 1821.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:58,653] [train step5820] D loss: 0.33211 G loss: 2.31726 (0.039 sec/batch, 1642.420 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:59,039] [train step5831] D loss: 0.33410 G loss: 2.57866 (0.036 sec/batch, 1766.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:59,422] [train step5841] D loss: 0.33282 G loss: 2.39820 (0.033 sec/batch, 1940.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:12:59,819] [train step5850] D loss: 0.33123 G loss: 2.37114 (0.034 sec/batch, 1901.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:00,222] [train step5860] D loss: 0.33473 G loss: 2.53099 (0.036 sec/batch, 1754.549 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:00,609] [train step5871] D loss: 0.33225 G loss: 2.45310 (0.038 sec/batch, 1695.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:00,983] [train step5880] D loss: 0.33138 G loss: 2.26716 (0.038 sec/batch, 1686.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:01,375] [train step5891] D loss: 0.33298 G loss: 2.50421 (0.039 sec/batch, 1640.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:01,761] [train step5900] D loss: 0.33237 G loss: 2.35745 (0.040 sec/batch, 1617.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:02,167] [train step5910] D loss: 0.33239 G loss: 2.23590 (0.049 sec/batch, 1305.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:02,554] [train step5920] D loss: 0.33307 G loss: 2.13237 (0.039 sec/batch, 1658.237 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:02,935] [train step5930] D loss: 0.33219 G loss: 2.36885 (0.040 sec/batch, 1612.913 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:03,333] [train step5940] D loss: 0.33137 G loss: 2.24471 (0.035 sec/batch, 1813.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:03,726] [train step5951] D loss: 0.33111 G loss: 2.35072 (0.040 sec/batch, 1595.797 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:04,104] [train step5961] D loss: 0.33041 G loss: 2.29607 (0.038 sec/batch, 1675.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:04,512] [train step5970] D loss: 0.33401 G loss: 2.22368 (0.040 sec/batch, 1582.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:04,904] [train step5980] D loss: 0.33280 G loss: 2.26487 (0.046 sec/batch, 1397.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:05,291] [train step5990] D loss: 0.33072 G loss: 2.39782 (0.047 sec/batch, 1370.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:05,676] [train step6000] D loss: 0.33029 G loss: 2.36168 (0.040 sec/batch, 1605.206 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:13:05,676] Saved checkpoint at 6000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:06,237] [train step6010] D loss: 0.33789 G loss: 2.69750 (0.027 sec/batch, 2376.460 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:06,650] [train step6021] D loss: 0.33181 G loss: 2.48318 (0.036 sec/batch, 1777.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:07,041] [train step6030] D loss: 0.33163 G loss: 2.43848 (0.042 sec/batch, 1532.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:07,432] [train step6041] D loss: 0.33026 G loss: 2.36249 (0.042 sec/batch, 1524.102 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:07,809] [train step6050] D loss: 0.33398 G loss: 2.58763 (0.037 sec/batch, 1721.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:08,191] [train step6060] D loss: 0.33225 G loss: 2.24296 (0.038 sec/batch, 1689.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:08,578] [train step6070] D loss: 0.33129 G loss: 2.27784 (0.031 sec/batch, 2041.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:08,969] [train step6081] D loss: 0.33408 G loss: 2.58388 (0.037 sec/batch, 1724.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:09,350] [train step6090] D loss: 0.33200 G loss: 2.22235 (0.042 sec/batch, 1508.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:09,741] [train step6101] D loss: 0.34931 G loss: 2.92225 (0.040 sec/batch, 1615.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:10,126] [train step6110] D loss: 0.40909 G loss: 1.55552 (0.036 sec/batch, 1781.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:10,572] [train step6120] D loss: 0.50381 G loss: 4.57078 (0.042 sec/batch, 1537.457 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:10,968] [train step6131] D loss: 0.67731 G loss: 6.73675 (0.040 sec/batch, 1599.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:11,352] [train step6140] D loss: 0.74443 G loss: 7.41543 (0.037 sec/batch, 1743.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:11,757] [train step6150] D loss: 0.71636 G loss: 7.12429 (0.037 sec/batch, 1727.050 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:12,145] [train step6160] D loss: 0.57159 G loss: 1.02778 (0.040 sec/batch, 1613.882 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:12,546] [train step6171] D loss: 0.95298 G loss: 9.51289 (0.037 sec/batch, 1707.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:12,928] [train step6180] D loss: 0.44234 G loss: 1.53634 (0.035 sec/batch, 1804.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:13,318] [train step6191] D loss: 0.54387 G loss: 5.37573 (0.037 sec/batch, 1719.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:13,718] [train step6201] D loss: 0.47698 G loss: 4.65447 (0.039 sec/batch, 1632.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:14,110] [train step6210] D loss: 0.37859 G loss: 3.43148 (0.041 sec/batch, 1572.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:14,500] [train step6221] D loss: 0.51879 G loss: 5.11336 (0.035 sec/batch, 1846.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:14,891] [train step6230] D loss: 0.33929 G loss: 2.26776 (0.037 sec/batch, 1719.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:15,280] [train step6240] D loss: 0.36748 G loss: 1.75962 (0.039 sec/batch, 1643.003 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:15,672] [train step6250] D loss: 0.34099 G loss: 2.75482 (0.048 sec/batch, 1337.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:16,049] [train step6261] D loss: 0.36492 G loss: 3.24841 (0.040 sec/batch, 1597.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:16,420] [train step6270] D loss: 0.34391 G loss: 2.72710 (0.036 sec/batch, 1782.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:16,799] [train step6281] D loss: 0.35404 G loss: 3.06286 (0.031 sec/batch, 2034.881 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:17,175] [train step6290] D loss: 0.35923 G loss: 1.80437 (0.036 sec/batch, 1777.977 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:17,547] [train step6300] D loss: 0.36691 G loss: 3.24161 (0.033 sec/batch, 1964.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:17,935] [train step6311] D loss: 0.35694 G loss: 1.80178 (0.035 sec/batch, 1838.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:18,309] [train step6321] D loss: 0.33697 G loss: 2.58088 (0.035 sec/batch, 1831.886 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:18,689] [train step6330] D loss: 0.34855 G loss: 1.92539 (0.043 sec/batch, 1492.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:19,058] [train step6341] D loss: 0.33796 G loss: 2.49733 (0.040 sec/batch, 1615.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:19,431] [train step6350] D loss: 0.34321 G loss: 2.70693 (0.036 sec/batch, 1788.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:19,819] [train step6360] D loss: 0.33709 G loss: 2.21820 (0.037 sec/batch, 1736.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:20,197] [train step6371] D loss: 0.33507 G loss: 2.41776 (0.036 sec/batch, 1791.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:20,566] [train step6381] D loss: 0.33713 G loss: 2.61061 (0.038 sec/batch, 1677.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:20,968] [train step6390] D loss: 0.34509 G loss: 2.04225 (0.039 sec/batch, 1633.534 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:21,335] [train step6401] D loss: 0.33450 G loss: 2.42817 (0.036 sec/batch, 1768.117 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:21,709] [train step6411] D loss: 0.33544 G loss: 2.48778 (0.044 sec/batch, 1468.722 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:22,091] [train step6420] D loss: 0.33986 G loss: 2.30061 (0.038 sec/batch, 1704.536 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:22,473] [train step6431] D loss: 0.33227 G loss: 2.29414 (0.039 sec/batch, 1650.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:22,855] [train step6441] D loss: 0.33839 G loss: 2.22783 (0.037 sec/batch, 1744.231 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:23,227] [train step6450] D loss: 0.33375 G loss: 2.53683 (0.032 sec/batch, 1992.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:23,617] [train step6461] D loss: 0.33425 G loss: 2.40491 (0.037 sec/batch, 1708.431 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:23,996] [train step6471] D loss: 0.33565 G loss: 2.26253 (0.033 sec/batch, 1917.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:24,386] [train step6480] D loss: 0.33232 G loss: 2.33334 (0.038 sec/batch, 1687.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:24,764] [train step6490] D loss: 0.33515 G loss: 2.27366 (0.039 sec/batch, 1652.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:25,154] [train step6501] D loss: 0.33739 G loss: 2.56490 (0.038 sec/batch, 1680.621 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:25,533] [train step6510] D loss: 0.33351 G loss: 2.28117 (0.037 sec/batch, 1743.631 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:25,927] [train step6521] D loss: 0.33559 G loss: 2.39865 (0.042 sec/batch, 1505.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:26,315] [train step6530] D loss: 0.33981 G loss: 2.11785 (0.039 sec/batch, 1642.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:26,689] [train step6540] D loss: 0.33984 G loss: 2.71534 (0.039 sec/batch, 1644.100 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:27,087] [train step6550] D loss: 0.34500 G loss: 1.94622 (0.040 sec/batch, 1580.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:27,464] [train step6560] D loss: 0.33644 G loss: 2.56066 (0.036 sec/batch, 1766.302 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:27,847] [train step6570] D loss: 0.33949 G loss: 2.06897 (0.038 sec/batch, 1677.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:28,241] [train step6581] D loss: 0.33807 G loss: 2.62035 (0.041 sec/batch, 1553.356 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:28,624] [train step6590] D loss: 0.33553 G loss: 2.13731 (0.037 sec/batch, 1725.807 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:29,024] [train step6600] D loss: 0.33722 G loss: 2.50664 (0.041 sec/batch, 1561.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:29,407] [train step6611] D loss: 0.33820 G loss: 2.28332 (0.038 sec/batch, 1678.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:29,797] [train step6621] D loss: 0.33446 G loss: 2.31780 (0.036 sec/batch, 1769.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:30,190] [train step6630] D loss: 0.33545 G loss: 2.18778 (0.043 sec/batch, 1495.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:30,576] [train step6640] D loss: 0.33733 G loss: 2.64098 (0.040 sec/batch, 1581.246 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:30,969] [train step6650] D loss: 0.33662 G loss: 2.64866 (0.039 sec/batch, 1650.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:31,354] [train step6660] D loss: 0.33467 G loss: 2.50112 (0.035 sec/batch, 1841.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:31,754] [train step6671] D loss: 0.33690 G loss: 2.65931 (0.037 sec/batch, 1719.902 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:32,144] [train step6681] D loss: 0.33849 G loss: 2.43050 (0.038 sec/batch, 1665.997 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:32,521] [train step6690] D loss: 0.33761 G loss: 2.47731 (0.040 sec/batch, 1617.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:32,907] [train step6700] D loss: 0.33536 G loss: 2.44690 (0.039 sec/batch, 1639.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:33,304] [train step6711] D loss: 0.33345 G loss: 2.42586 (0.037 sec/batch, 1748.799 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:33,683] [train step6720] D loss: 0.33811 G loss: 2.09231 (0.040 sec/batch, 1612.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:34,084] [train step6730] D loss: 0.33430 G loss: 2.51072 (0.050 sec/batch, 1268.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:34,463] [train step6740] D loss: 0.33342 G loss: 2.48394 (0.036 sec/batch, 1780.395 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:34,850] [train step6750] D loss: 0.33411 G loss: 2.49618 (0.040 sec/batch, 1584.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:35,239] [train step6760] D loss: 0.33483 G loss: 2.15993 (0.037 sec/batch, 1710.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:35,619] [train step6771] D loss: 0.33466 G loss: 2.49381 (0.034 sec/batch, 1900.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:36,003] [train step6780] D loss: 0.33055 G loss: 2.32691 (0.037 sec/batch, 1751.298 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:36,392] [train step6791] D loss: 0.33493 G loss: 2.43866 (0.035 sec/batch, 1809.169 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:36,771] [train step6801] D loss: 0.33596 G loss: 2.20184 (0.037 sec/batch, 1737.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:37,160] [train step6810] D loss: 0.33551 G loss: 2.42855 (0.045 sec/batch, 1435.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:37,554] [train step6821] D loss: 0.33255 G loss: 2.54224 (0.038 sec/batch, 1703.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:37,964] [train step6831] D loss: 0.33858 G loss: 2.11129 (0.040 sec/batch, 1583.027 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:38,361] [train step6840] D loss: 0.34950 G loss: 2.93014 (0.037 sec/batch, 1732.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:38,747] [train step6850] D loss: 0.34505 G loss: 1.91766 (0.041 sec/batch, 1553.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:39,130] [train step6861] D loss: 0.33485 G loss: 2.57873 (0.038 sec/batch, 1699.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:39,527] [train step6870] D loss: 0.33495 G loss: 2.11587 (0.037 sec/batch, 1750.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:39,909] [train step6881] D loss: 0.33432 G loss: 2.33099 (0.037 sec/batch, 1752.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:40,308] [train step6891] D loss: 0.33462 G loss: 2.45979 (0.039 sec/batch, 1623.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:40,685] [train step6900] D loss: 0.33064 G loss: 2.40916 (0.034 sec/batch, 1860.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:41,076] [train step6911] D loss: 0.33922 G loss: 2.72298 (0.039 sec/batch, 1660.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:41,466] [train step6921] D loss: 0.33655 G loss: 2.15407 (0.036 sec/batch, 1757.018 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:41,852] [train step6930] D loss: 0.33972 G loss: 2.72080 (0.037 sec/batch, 1727.662 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:42,251] [train step6941] D loss: 0.33532 G loss: 2.10641 (0.036 sec/batch, 1780.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:42,633] [train step6951] D loss: 0.33298 G loss: 2.39936 (0.036 sec/batch, 1776.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:43,011] [train step6960] D loss: 0.33206 G loss: 2.29060 (0.038 sec/batch, 1701.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:43,402] [train step6970] D loss: 0.33106 G loss: 2.41521 (0.038 sec/batch, 1697.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:43,783] [train step6981] D loss: 0.33168 G loss: 2.39766 (0.036 sec/batch, 1793.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:44,166] [train step6990] D loss: 0.33568 G loss: 2.47951 (0.031 sec/batch, 2076.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:44,563] [train step7001] D loss: 0.33189 G loss: 2.38361 (0.037 sec/batch, 1722.562 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:13:44,564] Saved checkpoint at 7000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:45,124] [train step7011] D loss: 0.33041 G loss: 2.23974 (0.039 sec/batch, 1641.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:45,516] [train step7020] D loss: 0.33520 G loss: 2.60149 (0.042 sec/batch, 1519.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:45,887] [train step7031] D loss: 0.33226 G loss: 2.43041 (0.038 sec/batch, 1700.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:46,262] [train step7040] D loss: 0.33334 G loss: 2.31634 (0.035 sec/batch, 1825.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:46,653] [train step7050] D loss: 0.33375 G loss: 2.47779 (0.043 sec/batch, 1491.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:47,027] [train step7061] D loss: 0.33464 G loss: 2.15149 (0.035 sec/batch, 1826.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:47,404] [train step7070] D loss: 0.33127 G loss: 2.43678 (0.040 sec/batch, 1611.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:47,795] [train step7080] D loss: 0.33003 G loss: 2.27032 (0.038 sec/batch, 1704.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:48,171] [train step7090] D loss: 0.33354 G loss: 2.55164 (0.035 sec/batch, 1850.794 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:48,560] [train step7100] D loss: 0.33166 G loss: 2.35694 (0.035 sec/batch, 1816.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:48,953] [train step7110] D loss: 0.32974 G loss: 2.28901 (0.040 sec/batch, 1619.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:49,328] [train step7120] D loss: 0.32951 G loss: 2.32532 (0.036 sec/batch, 1798.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:49,726] [train step7130] D loss: 0.33157 G loss: 2.33588 (0.039 sec/batch, 1626.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:50,101] [train step7140] D loss: 0.33081 G loss: 2.44118 (0.038 sec/batch, 1680.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:50,490] [train step7150] D loss: 0.32972 G loss: 2.36580 (0.041 sec/batch, 1557.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:50,875] [train step7161] D loss: 0.33024 G loss: 2.40560 (0.038 sec/batch, 1664.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:51,246] [train step7170] D loss: 0.33079 G loss: 2.31373 (0.032 sec/batch, 2021.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:51,642] [train step7180] D loss: 0.32964 G loss: 2.38227 (0.038 sec/batch, 1686.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:52,021] [train step7190] D loss: 0.33490 G loss: 2.12948 (0.036 sec/batch, 1764.920 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:52,404] [train step7200] D loss: 0.33089 G loss: 2.26576 (0.037 sec/batch, 1742.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:52,815] [train step7210] D loss: 0.32999 G loss: 2.39216 (0.039 sec/batch, 1650.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:53,197] [train step7220] D loss: 0.33142 G loss: 2.53076 (0.037 sec/batch, 1741.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:53,583] [train step7230] D loss: 0.33300 G loss: 2.39213 (0.038 sec/batch, 1696.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:53,982] [train step7240] D loss: 0.33135 G loss: 2.28348 (0.037 sec/batch, 1722.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:54,366] [train step7251] D loss: 0.33081 G loss: 2.28010 (0.039 sec/batch, 1640.824 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:54,758] [train step7260] D loss: 0.33186 G loss: 2.43707 (0.039 sec/batch, 1642.018 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:55,136] [train step7271] D loss: 0.33134 G loss: 2.17042 (0.033 sec/batch, 1920.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:55,514] [train step7280] D loss: 0.33082 G loss: 2.53378 (0.036 sec/batch, 1771.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:55,909] [train step7290] D loss: 0.33288 G loss: 2.21851 (0.037 sec/batch, 1744.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:56,300] [train step7301] D loss: 0.32946 G loss: 2.25987 (0.040 sec/batch, 1596.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:56,686] [train step7311] D loss: 0.32936 G loss: 2.36822 (0.039 sec/batch, 1633.117 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:57,077] [train step7320] D loss: 0.32940 G loss: 2.25888 (0.038 sec/batch, 1701.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:57,452] [train step7330] D loss: 0.33074 G loss: 2.35464 (0.044 sec/batch, 1456.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:57,836] [train step7341] D loss: 0.33365 G loss: 2.62374 (0.036 sec/batch, 1774.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:58,217] [train step7350] D loss: 0.33072 G loss: 2.43471 (0.037 sec/batch, 1743.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:58,600] [train step7361] D loss: 0.33275 G loss: 2.53227 (0.039 sec/batch, 1645.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:58,993] [train step7370] D loss: 0.33120 G loss: 2.24296 (0.037 sec/batch, 1743.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:59,384] [train step7380] D loss: 0.32918 G loss: 2.25209 (0.040 sec/batch, 1598.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:13:59,768] [train step7391] D loss: 0.32872 G loss: 2.35970 (0.041 sec/batch, 1556.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:00,155] [train step7400] D loss: 0.33457 G loss: 2.60909 (0.040 sec/batch, 1592.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:00,534] [train step7410] D loss: 0.32966 G loss: 2.26428 (0.038 sec/batch, 1663.231 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:00,918] [train step7421] D loss: 0.33138 G loss: 2.31333 (0.037 sec/batch, 1726.817 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:01,291] [train step7430] D loss: 0.32951 G loss: 2.30485 (0.031 sec/batch, 2094.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:01,665] [train step7440] D loss: 0.33095 G loss: 2.44337 (0.027 sec/batch, 2392.025 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:02,069] [train step7451] D loss: 0.33192 G loss: 2.17519 (0.037 sec/batch, 1745.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:02,453] [train step7460] D loss: 0.32956 G loss: 2.44393 (0.037 sec/batch, 1706.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:02,829] [train step7470] D loss: 0.33222 G loss: 2.26809 (0.037 sec/batch, 1736.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:03,221] [train step7481] D loss: 0.32853 G loss: 2.30199 (0.037 sec/batch, 1728.329 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:03,627] [train step7491] D loss: 0.33169 G loss: 2.53982 (0.036 sec/batch, 1778.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:04,014] [train step7500] D loss: 0.33002 G loss: 2.40717 (0.037 sec/batch, 1743.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:04,398] [train step7510] D loss: 0.33035 G loss: 2.20205 (0.036 sec/batch, 1773.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:04,787] [train step7521] D loss: 0.33009 G loss: 2.39695 (0.040 sec/batch, 1609.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:05,174] [train step7530] D loss: 0.33078 G loss: 2.22594 (0.043 sec/batch, 1479.277 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:05,559] [train step7541] D loss: 0.33036 G loss: 2.46193 (0.036 sec/batch, 1793.288 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:05,935] [train step7551] D loss: 0.32927 G loss: 2.43681 (0.033 sec/batch, 1953.920 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:06,332] [train step7560] D loss: 0.32884 G loss: 2.31164 (0.036 sec/batch, 1783.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:06,723] [train step7570] D loss: 0.33000 G loss: 2.42761 (0.038 sec/batch, 1703.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:07,122] [train step7581] D loss: 0.32894 G loss: 2.27042 (0.042 sec/batch, 1526.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:07,511] [train step7590] D loss: 0.33238 G loss: 2.18292 (0.038 sec/batch, 1699.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:07,884] [train step7601] D loss: 0.33091 G loss: 2.26794 (0.035 sec/batch, 1844.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:08,271] [train step7611] D loss: 0.33614 G loss: 2.64718 (0.037 sec/batch, 1722.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:08,659] [train step7620] D loss: 0.32966 G loss: 2.38332 (0.036 sec/batch, 1791.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:09,036] [train step7631] D loss: 0.33416 G loss: 2.62030 (0.042 sec/batch, 1523.038 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:09,424] [train step7640] D loss: 0.32973 G loss: 2.26248 (0.037 sec/batch, 1737.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:09,803] [train step7650] D loss: 0.33222 G loss: 2.52019 (0.036 sec/batch, 1770.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:10,190] [train step7661] D loss: 0.33096 G loss: 2.20245 (0.037 sec/batch, 1722.308 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:10,580] [train step7671] D loss: 0.32952 G loss: 2.28086 (0.038 sec/batch, 1666.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:10,960] [train step7680] D loss: 0.32818 G loss: 2.31132 (0.039 sec/batch, 1622.967 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:11,350] [train step7690] D loss: 0.33034 G loss: 2.33894 (0.037 sec/batch, 1749.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:11,725] [train step7701] D loss: 0.33210 G loss: 2.60285 (0.037 sec/batch, 1746.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:12,109] [train step7710] D loss: 0.33360 G loss: 2.19223 (0.039 sec/batch, 1652.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:12,497] [train step7721] D loss: 0.32867 G loss: 2.35042 (0.038 sec/batch, 1690.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:12,880] [train step7731] D loss: 0.33117 G loss: 2.50895 (0.036 sec/batch, 1796.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:13,273] [train step7740] D loss: 0.32903 G loss: 2.39951 (0.037 sec/batch, 1724.155 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:13,661] [train step7751] D loss: 0.32795 G loss: 2.37850 (0.038 sec/batch, 1666.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:14,062] [train step7760] D loss: 0.33012 G loss: 2.34565 (0.045 sec/batch, 1437.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:14,451] [train step7770] D loss: 0.33040 G loss: 2.41883 (0.036 sec/batch, 1780.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:14,834] [train step7781] D loss: 0.32939 G loss: 2.40823 (0.038 sec/batch, 1689.751 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:15,207] [train step7791] D loss: 0.32976 G loss: 2.49979 (0.041 sec/batch, 1554.606 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:15,585] [train step7800] D loss: 0.33126 G loss: 2.24540 (0.034 sec/batch, 1883.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:15,959] [train step7810] D loss: 0.33022 G loss: 2.28240 (0.039 sec/batch, 1662.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:16,346] [train step7820] D loss: 0.33167 G loss: 2.53911 (0.038 sec/batch, 1702.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:16,725] [train step7830] D loss: 0.33177 G loss: 2.15886 (0.043 sec/batch, 1479.489 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:17,094] [train step7840] D loss: 0.32915 G loss: 2.37819 (0.034 sec/batch, 1907.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:17,481] [train step7850] D loss: 0.32858 G loss: 2.40836 (0.036 sec/batch, 1763.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:17,855] [train step7860] D loss: 0.33127 G loss: 2.45402 (0.039 sec/batch, 1659.108 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:18,227] [train step7871] D loss: 0.32927 G loss: 2.29982 (0.037 sec/batch, 1739.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:18,621] [train step7881] D loss: 0.32869 G loss: 2.31458 (0.039 sec/batch, 1627.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:19,016] [train step7890] D loss: 0.32801 G loss: 2.38835 (0.036 sec/batch, 1780.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:19,408] [train step7901] D loss: 0.33169 G loss: 2.56010 (0.038 sec/batch, 1688.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:19,778] [train step7911] D loss: 0.33267 G loss: 2.13758 (0.039 sec/batch, 1630.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:20,155] [train step7920] D loss: 0.32942 G loss: 2.36422 (0.034 sec/batch, 1881.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:20,543] [train step7931] D loss: 0.32947 G loss: 2.17552 (0.037 sec/batch, 1718.614 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:20,911] [train step7941] D loss: 0.32993 G loss: 2.27306 (0.037 sec/batch, 1750.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:21,290] [train step7950] D loss: 0.33271 G loss: 2.54114 (0.038 sec/batch, 1675.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:21,678] [train step7960] D loss: 0.33253 G loss: 2.11807 (0.043 sec/batch, 1500.511 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:22,055] [train step7970] D loss: 0.32896 G loss: 2.39056 (0.033 sec/batch, 1937.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:22,446] [train step7980] D loss: 0.32914 G loss: 2.26044 (0.044 sec/batch, 1467.887 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:22,830] [train step7990] D loss: 0.32968 G loss: 2.33091 (0.037 sec/batch, 1750.327 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:23,207] [train step8001] D loss: 0.32918 G loss: 2.32136 (0.032 sec/batch, 1984.442 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:14:23,208] Saved checkpoint at 8000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:23,803] [train step8010] D loss: 0.32957 G loss: 2.45041 (0.039 sec/batch, 1645.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:24,177] [train step8021] D loss: 0.32956 G loss: 2.24905 (0.036 sec/batch, 1761.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:24,568] [train step8030] D loss: 0.32775 G loss: 2.25369 (0.027 sec/batch, 2412.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:24,962] [train step8040] D loss: 0.32857 G loss: 2.33943 (0.033 sec/batch, 1955.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:25,346] [train step8051] D loss: 0.32951 G loss: 2.36797 (0.036 sec/batch, 1788.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:25,740] [train step8060] D loss: 0.32938 G loss: 2.45037 (0.043 sec/batch, 1498.066 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:26,118] [train step8070] D loss: 0.32843 G loss: 2.34063 (0.036 sec/batch, 1792.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:26,499] [train step8081] D loss: 0.32812 G loss: 2.27615 (0.042 sec/batch, 1514.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:26,888] [train step8091] D loss: 0.32926 G loss: 2.34933 (0.037 sec/batch, 1707.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:27,266] [train step8100] D loss: 0.32848 G loss: 2.29025 (0.036 sec/batch, 1757.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:27,660] [train step8111] D loss: 0.32773 G loss: 2.32057 (0.041 sec/batch, 1555.733 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:28,038] [train step8120] D loss: 0.32792 G loss: 2.29207 (0.037 sec/batch, 1706.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:28,420] [train step8130] D loss: 0.32910 G loss: 2.36234 (0.036 sec/batch, 1802.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:28,810] [train step8141] D loss: 0.33035 G loss: 2.46261 (0.039 sec/batch, 1645.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:29,192] [train step8151] D loss: 0.32950 G loss: 2.37479 (0.038 sec/batch, 1690.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:29,570] [train step8160] D loss: 0.33045 G loss: 2.48313 (0.040 sec/batch, 1593.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:29,957] [train step8171] D loss: 0.33207 G loss: 2.22549 (0.037 sec/batch, 1750.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:30,346] [train step8181] D loss: 0.32802 G loss: 2.37966 (0.041 sec/batch, 1547.856 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:30,739] [train step8190] D loss: 0.32951 G loss: 2.47473 (0.041 sec/batch, 1563.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:31,115] [train step8201] D loss: 0.33066 G loss: 2.48170 (0.035 sec/batch, 1843.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:31,496] [train step8211] D loss: 0.32773 G loss: 2.28324 (0.035 sec/batch, 1854.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:31,892] [train step8220] D loss: 0.33020 G loss: 2.22099 (0.047 sec/batch, 1373.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:32,270] [train step8231] D loss: 0.32933 G loss: 2.39625 (0.043 sec/batch, 1496.246 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:32,651] [train step8240] D loss: 0.32815 G loss: 2.41056 (0.040 sec/batch, 1587.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:33,044] [train step8250] D loss: 0.32744 G loss: 2.32749 (0.039 sec/batch, 1656.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:33,424] [train step8260] D loss: 0.32780 G loss: 2.35074 (0.037 sec/batch, 1742.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:33,822] [train step8270] D loss: 0.32965 G loss: 2.35521 (0.040 sec/batch, 1594.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:34,204] [train step8280] D loss: 0.32701 G loss: 2.35236 (0.034 sec/batch, 1863.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:34,590] [train step8291] D loss: 0.32861 G loss: 2.25490 (0.044 sec/batch, 1456.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:34,999] [train step8301] D loss: 0.33531 G loss: 2.63744 (0.063 sec/batch, 1020.128 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:35,389] [train step8310] D loss: 0.32744 G loss: 2.30414 (0.041 sec/batch, 1575.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:35,764] [train step8320] D loss: 0.32896 G loss: 2.45642 (0.036 sec/batch, 1774.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:36,154] [train step8330] D loss: 0.32842 G loss: 2.24241 (0.040 sec/batch, 1611.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:36,533] [train step8340] D loss: 0.32819 G loss: 2.34198 (0.038 sec/batch, 1674.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:36,920] [train step8351] D loss: 0.32892 G loss: 2.34700 (0.038 sec/batch, 1670.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:37,300] [train step8361] D loss: 0.32786 G loss: 2.33642 (0.039 sec/batch, 1656.344 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:37,688] [train step8370] D loss: 0.32814 G loss: 2.37523 (0.039 sec/batch, 1652.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:38,076] [train step8381] D loss: 0.32807 G loss: 2.34470 (0.033 sec/batch, 1949.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:38,467] [train step8390] D loss: 0.32825 G loss: 2.19263 (0.037 sec/batch, 1715.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:38,848] [train step8400] D loss: 0.32954 G loss: 2.26305 (0.036 sec/batch, 1753.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:39,240] [train step8411] D loss: 0.32719 G loss: 2.28804 (0.035 sec/batch, 1814.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:39,619] [train step8421] D loss: 0.32950 G loss: 2.28895 (0.036 sec/batch, 1802.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:40,015] [train step8430] D loss: 0.33158 G loss: 2.56300 (0.046 sec/batch, 1404.025 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:40,402] [train step8441] D loss: 0.33196 G loss: 2.14580 (0.039 sec/batch, 1657.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:40,784] [train step8451] D loss: 0.33081 G loss: 2.41877 (0.040 sec/batch, 1581.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:41,181] [train step8460] D loss: 0.33644 G loss: 2.02530 (0.040 sec/batch, 1588.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:41,572] [train step8470] D loss: 0.33264 G loss: 2.46409 (0.038 sec/batch, 1677.543 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:41,953] [train step8480] D loss: 0.32805 G loss: 2.37665 (0.038 sec/batch, 1665.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:42,342] [train step8490] D loss: 0.32754 G loss: 2.35351 (0.029 sec/batch, 2175.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:42,734] [train step8501] D loss: 0.33198 G loss: 2.54568 (0.036 sec/batch, 1765.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:43,125] [train step8511] D loss: 0.32840 G loss: 2.26464 (0.038 sec/batch, 1667.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:43,507] [train step8520] D loss: 0.32825 G loss: 2.32019 (0.034 sec/batch, 1883.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:43,890] [train step8531] D loss: 0.32963 G loss: 2.49251 (0.038 sec/batch, 1701.328 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:44,293] [train step8540] D loss: 0.33136 G loss: 2.16944 (0.037 sec/batch, 1717.668 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:44,667] [train step8550] D loss: 0.32998 G loss: 2.44877 (0.039 sec/batch, 1659.970 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:45,054] [train step8560] D loss: 0.33136 G loss: 2.10773 (0.036 sec/batch, 1784.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:45,445] [train step8570] D loss: 0.32818 G loss: 2.33852 (0.035 sec/batch, 1827.347 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:45,827] [train step8580] D loss: 0.33069 G loss: 2.23706 (0.030 sec/batch, 2129.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:46,213] [train step8590] D loss: 0.32849 G loss: 2.34610 (0.030 sec/batch, 2103.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:46,597] [train step8600] D loss: 0.32798 G loss: 2.26169 (0.040 sec/batch, 1592.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:46,970] [train step8610] D loss: 0.32767 G loss: 2.22748 (0.038 sec/batch, 1690.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:47,348] [train step8621] D loss: 0.32732 G loss: 2.39160 (0.036 sec/batch, 1767.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:47,723] [train step8631] D loss: 0.32704 G loss: 2.32234 (0.038 sec/batch, 1669.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:48,107] [train step8640] D loss: 0.32761 G loss: 2.33995 (0.040 sec/batch, 1607.244 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:48,491] [train step8650] D loss: 0.32899 G loss: 2.18402 (0.034 sec/batch, 1910.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:48,873] [train step8661] D loss: 0.32713 G loss: 2.33487 (0.038 sec/batch, 1681.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:49,258] [train step8670] D loss: 0.32699 G loss: 2.28431 (0.037 sec/batch, 1717.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:49,630] [train step8680] D loss: 0.32721 G loss: 2.34868 (0.037 sec/batch, 1717.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:50,012] [train step8691] D loss: 0.32730 G loss: 2.35308 (0.036 sec/batch, 1756.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:50,396] [train step8700] D loss: 0.32920 G loss: 2.30861 (0.036 sec/batch, 1772.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:50,777] [train step8710] D loss: 0.32756 G loss: 2.31100 (0.046 sec/batch, 1400.107 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:51,151] [train step8721] D loss: 0.33144 G loss: 2.12297 (0.033 sec/batch, 1914.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:51,539] [train step8730] D loss: 0.33385 G loss: 2.56665 (0.039 sec/batch, 1629.162 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:51,916] [train step8741] D loss: 0.33183 G loss: 2.13463 (0.038 sec/batch, 1705.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:52,305] [train step8751] D loss: 0.33001 G loss: 2.46216 (0.040 sec/batch, 1586.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:52,669] [train step8760] D loss: 0.32911 G loss: 2.21521 (0.035 sec/batch, 1834.816 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:53,052] [train step8771] D loss: 0.32908 G loss: 2.34988 (0.032 sec/batch, 2015.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:53,435] [train step8780] D loss: 0.32770 G loss: 2.33729 (0.035 sec/batch, 1846.237 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:53,817] [train step8790] D loss: 0.33256 G loss: 2.26185 (0.037 sec/batch, 1715.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:54,191] [train step8800] D loss: 0.32837 G loss: 2.33378 (0.035 sec/batch, 1803.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:54,599] [train step8810] D loss: 0.32753 G loss: 2.31547 (0.044 sec/batch, 1440.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:54,979] [train step8820] D loss: 0.32857 G loss: 2.43043 (0.038 sec/batch, 1706.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:55,365] [train step8830] D loss: 0.32736 G loss: 2.25423 (0.042 sec/batch, 1518.128 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:55,755] [train step8840] D loss: 0.33010 G loss: 2.42424 (0.038 sec/batch, 1678.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:56,134] [train step8850] D loss: 0.32795 G loss: 2.18835 (0.039 sec/batch, 1639.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:56,541] [train step8860] D loss: 0.32861 G loss: 2.32864 (0.038 sec/batch, 1677.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:56,913] [train step8870] D loss: 0.32876 G loss: 2.43226 (0.029 sec/batch, 2208.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:57,297] [train step8880] D loss: 0.32848 G loss: 2.21181 (0.038 sec/batch, 1689.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:57,691] [train step8891] D loss: 0.32813 G loss: 2.39126 (0.036 sec/batch, 1790.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:58,070] [train step8901] D loss: 0.33021 G loss: 2.25470 (0.041 sec/batch, 1575.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:58,449] [train step8910] D loss: 0.32709 G loss: 2.26927 (0.040 sec/batch, 1613.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:58,836] [train step8921] D loss: 0.32931 G loss: 2.39139 (0.037 sec/batch, 1722.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:59,218] [train step8930] D loss: 0.32752 G loss: 2.22627 (0.037 sec/batch, 1722.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:59,607] [train step8940] D loss: 0.32797 G loss: 2.38306 (0.038 sec/batch, 1665.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:14:59,987] [train step8951] D loss: 0.32738 G loss: 2.36028 (0.035 sec/batch, 1803.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:00,360] [train step8960] D loss: 0.32811 G loss: 2.26816 (0.031 sec/batch, 2053.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:00,769] [train step8970] D loss: 0.32760 G loss: 2.34291 (0.041 sec/batch, 1550.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:01,148] [train step8980] D loss: 0.32838 G loss: 2.28192 (0.040 sec/batch, 1611.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:01,546] [train step8990] D loss: 0.32764 G loss: 2.32102 (0.049 sec/batch, 1312.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:01,929] [train step9000] D loss: 0.32921 G loss: 2.43228 (0.033 sec/batch, 1956.798 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:15:01,930] Saved checkpoint at 9000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:02,513] [train step9011] D loss: 0.32792 G loss: 2.37511 (0.037 sec/batch, 1739.495 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:02,908] [train step9020] D loss: 0.32780 G loss: 2.28699 (0.040 sec/batch, 1616.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:03,283] [train step9030] D loss: 0.32938 G loss: 2.23339 (0.043 sec/batch, 1500.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:03,679] [train step9040] D loss: 0.32784 G loss: 2.21142 (0.038 sec/batch, 1682.875 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:04,060] [train step9050] D loss: 0.32837 G loss: 2.39144 (0.037 sec/batch, 1718.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:04,439] [train step9060] D loss: 0.32976 G loss: 2.31821 (0.032 sec/batch, 2031.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:04,821] [train step9071] D loss: 0.33303 G loss: 2.54472 (0.034 sec/batch, 1869.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:05,204] [train step9081] D loss: 0.33286 G loss: 2.05036 (0.038 sec/batch, 1702.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:05,594] [train step9090] D loss: 0.32697 G loss: 2.37251 (0.040 sec/batch, 1601.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:05,982] [train step9101] D loss: 0.32869 G loss: 2.25210 (0.037 sec/batch, 1743.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:06,367] [train step9110] D loss: 0.32797 G loss: 2.38253 (0.033 sec/batch, 1967.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:06,762] [train step9120] D loss: 0.32815 G loss: 2.20019 (0.033 sec/batch, 1955.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:07,177] [train step9131] D loss: 0.32807 G loss: 2.36652 (0.041 sec/batch, 1549.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:07,552] [train step9140] D loss: 0.32843 G loss: 2.25703 (0.036 sec/batch, 1767.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:07,938] [train step9150] D loss: 0.32763 G loss: 2.35608 (0.028 sec/batch, 2322.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:08,334] [train step9161] D loss: 0.32756 G loss: 2.31761 (0.038 sec/batch, 1693.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:08,721] [train step9171] D loss: 0.32728 G loss: 2.38203 (0.044 sec/batch, 1446.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:09,107] [train step9180] D loss: 0.32803 G loss: 2.38262 (0.038 sec/batch, 1684.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:09,495] [train step9191] D loss: 0.32730 G loss: 2.32656 (0.037 sec/batch, 1750.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:09,897] [train step9200] D loss: 0.32788 G loss: 2.28470 (0.039 sec/batch, 1622.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:10,289] [train step9210] D loss: 0.32666 G loss: 2.31678 (0.038 sec/batch, 1688.454 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:10,663] [train step9220] D loss: 0.32865 G loss: 2.46284 (0.044 sec/batch, 1438.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:11,061] [train step9230] D loss: 0.32884 G loss: 2.23807 (0.038 sec/batch, 1682.897 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:11,453] [train step9240] D loss: 0.32930 G loss: 2.45148 (0.041 sec/batch, 1551.282 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:11,846] [train step9251] D loss: 0.32760 G loss: 2.26354 (0.041 sec/batch, 1559.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:12,236] [train step9260] D loss: 0.32658 G loss: 2.32457 (0.039 sec/batch, 1658.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:12,623] [train step9270] D loss: 0.32721 G loss: 2.28982 (0.043 sec/batch, 1486.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:13,024] [train step9280] D loss: 0.32725 G loss: 2.23674 (0.046 sec/batch, 1385.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:13,419] [train step9290] D loss: 0.32759 G loss: 2.21046 (0.042 sec/batch, 1519.323 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:13,807] [train step9300] D loss: 0.32782 G loss: 2.27924 (0.037 sec/batch, 1724.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:14,198] [train step9310] D loss: 0.32725 G loss: 2.36621 (0.036 sec/batch, 1778.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:14,586] [train step9321] D loss: 0.32690 G loss: 2.34472 (0.041 sec/batch, 1578.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:14,976] [train step9330] D loss: 0.32892 G loss: 2.52372 (0.036 sec/batch, 1785.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:15,365] [train step9341] D loss: 0.32822 G loss: 2.30723 (0.038 sec/batch, 1685.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:15,742] [train step9351] D loss: 0.32900 G loss: 2.20520 (0.035 sec/batch, 1821.828 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:16,125] [train step9360] D loss: 0.32969 G loss: 2.45133 (0.041 sec/batch, 1554.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:16,503] [train step9370] D loss: 0.32801 G loss: 2.26390 (0.040 sec/batch, 1619.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:16,877] [train step9381] D loss: 0.32779 G loss: 2.43780 (0.035 sec/batch, 1822.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:17,257] [train step9390] D loss: 0.32696 G loss: 2.31035 (0.039 sec/batch, 1636.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:17,644] [train step9401] D loss: 0.32790 G loss: 2.35727 (0.034 sec/batch, 1862.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:18,025] [train step9411] D loss: 0.32743 G loss: 2.40653 (0.044 sec/batch, 1464.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:18,402] [train step9420] D loss: 0.33023 G loss: 2.46318 (0.039 sec/batch, 1647.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:18,779] [train step9430] D loss: 0.32753 G loss: 2.32216 (0.040 sec/batch, 1584.578 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:19,175] [train step9441] D loss: 0.32712 G loss: 2.32607 (0.035 sec/batch, 1815.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:19,554] [train step9450] D loss: 0.32724 G loss: 2.36638 (0.038 sec/batch, 1686.089 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:19,917] [train step9461] D loss: 0.32746 G loss: 2.30370 (0.035 sec/batch, 1809.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:20,299] [train step9471] D loss: 0.32746 G loss: 2.31776 (0.036 sec/batch, 1789.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:20,674] [train step9480] D loss: 0.32702 G loss: 2.31682 (0.046 sec/batch, 1393.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:21,044] [train step9490] D loss: 0.32968 G loss: 2.21629 (0.044 sec/batch, 1450.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:21,430] [train step9500] D loss: 0.32705 G loss: 2.27370 (0.045 sec/batch, 1411.667 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:21,809] [train step9510] D loss: 0.32723 G loss: 2.24139 (0.039 sec/batch, 1660.114 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:22,194] [train step9521] D loss: 0.32840 G loss: 2.26557 (0.043 sec/batch, 1483.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:22,573] [train step9531] D loss: 0.32828 G loss: 2.45282 (0.037 sec/batch, 1751.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:22,957] [train step9540] D loss: 0.32817 G loss: 2.19550 (0.037 sec/batch, 1723.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:23,343] [train step9551] D loss: 0.33044 G loss: 2.54893 (0.035 sec/batch, 1819.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:23,722] [train step9561] D loss: 0.32974 G loss: 2.19029 (0.041 sec/batch, 1572.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:24,110] [train step9570] D loss: 0.32715 G loss: 2.25753 (0.038 sec/batch, 1677.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:24,491] [train step9581] D loss: 0.32754 G loss: 2.23631 (0.038 sec/batch, 1679.884 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:24,863] [train step9590] D loss: 0.32902 G loss: 2.44550 (0.035 sec/batch, 1806.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:25,253] [train step9600] D loss: 0.32707 G loss: 2.36107 (0.046 sec/batch, 1404.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:25,630] [train step9610] D loss: 0.32716 G loss: 2.38433 (0.043 sec/batch, 1498.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:26,014] [train step9621] D loss: 0.32768 G loss: 2.42333 (0.037 sec/batch, 1752.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:26,399] [train step9630] D loss: 0.32782 G loss: 2.26153 (0.037 sec/batch, 1752.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:26,782] [train step9640] D loss: 0.32794 G loss: 2.23167 (0.041 sec/batch, 1560.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:27,165] [train step9651] D loss: 0.32706 G loss: 2.27096 (0.035 sec/batch, 1805.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:27,551] [train step9660] D loss: 0.32812 G loss: 2.22448 (0.042 sec/batch, 1537.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:27,927] [train step9671] D loss: 0.32995 G loss: 2.53461 (0.037 sec/batch, 1752.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:28,335] [train step9681] D loss: 0.32989 G loss: 2.14974 (0.038 sec/batch, 1664.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:28,715] [train step9690] D loss: 0.33305 G loss: 2.59673 (0.038 sec/batch, 1672.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:29,086] [train step9701] D loss: 0.33569 G loss: 2.03640 (0.037 sec/batch, 1739.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:29,479] [train step9710] D loss: 0.32758 G loss: 2.38106 (0.038 sec/batch, 1670.871 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:29,860] [train step9720] D loss: 0.32800 G loss: 2.32550 (0.039 sec/batch, 1660.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:30,240] [train step9731] D loss: 0.32821 G loss: 2.30841 (0.038 sec/batch, 1686.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:30,632] [train step9740] D loss: 0.32978 G loss: 2.47657 (0.037 sec/batch, 1722.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:31,023] [train step9750] D loss: 0.32716 G loss: 2.31076 (0.038 sec/batch, 1666.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:31,425] [train step9761] D loss: 0.32911 G loss: 2.42807 (0.040 sec/batch, 1615.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:31,810] [train step9771] D loss: 0.32836 G loss: 2.20805 (0.045 sec/batch, 1433.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:32,185] [train step9780] D loss: 0.32947 G loss: 2.31775 (0.038 sec/batch, 1690.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:32,585] [train step9791] D loss: 0.32726 G loss: 2.28017 (0.040 sec/batch, 1588.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:32,974] [train step9800] D loss: 0.32732 G loss: 2.40927 (0.041 sec/batch, 1547.464 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:33,358] [train step9810] D loss: 0.32947 G loss: 2.45190 (0.037 sec/batch, 1723.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:33,753] [train step9821] D loss: 0.33685 G loss: 2.64009 (0.036 sec/batch, 1797.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:34,140] [train step9831] D loss: 0.33091 G loss: 2.05972 (0.039 sec/batch, 1649.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:34,543] [train step9840] D loss: 0.32905 G loss: 2.46131 (0.041 sec/batch, 1579.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:34,929] [train step9851] D loss: 0.32888 G loss: 2.20468 (0.037 sec/batch, 1716.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:35,311] [train step9861] D loss: 0.32762 G loss: 2.31592 (0.036 sec/batch, 1755.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:35,717] [train step9870] D loss: 0.32827 G loss: 2.35953 (0.030 sec/batch, 2149.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:36,111] [train step9880] D loss: 0.32823 G loss: 2.42888 (0.038 sec/batch, 1695.353 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:36,493] [train step9890] D loss: 0.32791 G loss: 2.25926 (0.038 sec/batch, 1685.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:36,878] [train step9900] D loss: 0.32684 G loss: 2.33749 (0.036 sec/batch, 1782.902 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:37,257] [train step9910] D loss: 0.32735 G loss: 2.40301 (0.045 sec/batch, 1435.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:37,653] [train step9920] D loss: 0.32834 G loss: 2.23586 (0.036 sec/batch, 1774.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:38,029] [train step9930] D loss: 0.33060 G loss: 2.52982 (0.032 sec/batch, 2012.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:38,415] [train step9941] D loss: 0.32690 G loss: 2.35473 (0.031 sec/batch, 2055.433 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:38,826] [train step9951] D loss: 0.32771 G loss: 2.34393 (0.038 sec/batch, 1679.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:39,206] [train step9960] D loss: 0.32694 G loss: 2.29643 (0.037 sec/batch, 1750.122 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:39,587] [train step9971] D loss: 0.32676 G loss: 2.32677 (0.038 sec/batch, 1701.781 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:39,980] [train step9981] D loss: 0.32774 G loss: 2.31433 (0.034 sec/batch, 1872.745 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:40,376] [train step9990] D loss: 0.32805 G loss: 2.21423 (0.041 sec/batch, 1542.210 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:40,761] [train step10001] D loss: 0.32803 G loss: 2.25921 (0.034 sec/batch, 1862.428 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:15:40,761] Saved checkpoint at 10000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:41,334] [train step10011] D loss: 0.32718 G loss: 2.38030 (0.032 sec/batch, 2004.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:41,718] [train step10020] D loss: 0.32748 G loss: 2.37347 (0.033 sec/batch, 1961.058 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:42,106] [train step10030] D loss: 0.32755 G loss: 2.28686 (0.040 sec/batch, 1618.944 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:42,494] [train step10041] D loss: 0.32816 G loss: 2.18989 (0.044 sec/batch, 1460.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:42,874] [train step10050] D loss: 0.32648 G loss: 2.35665 (0.034 sec/batch, 1877.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:43,255] [train step10061] D loss: 0.32661 G loss: 2.31544 (0.034 sec/batch, 1884.353 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:43,642] [train step10070] D loss: 0.32802 G loss: 2.24141 (0.037 sec/batch, 1726.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:44,034] [train step10080] D loss: 0.32764 G loss: 2.41130 (0.036 sec/batch, 1756.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:44,418] [train step10091] D loss: 0.32766 G loss: 2.26830 (0.030 sec/batch, 2104.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:44,813] [train step10100] D loss: 0.32905 G loss: 2.25768 (0.033 sec/batch, 1945.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:45,201] [train step10110] D loss: 0.32744 G loss: 2.31203 (0.041 sec/batch, 1558.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:45,586] [train step10121] D loss: 0.32710 G loss: 2.26567 (0.041 sec/batch, 1578.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:45,978] [train step10130] D loss: 0.32702 G loss: 2.38185 (0.041 sec/batch, 1558.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:46,349] [train step10140] D loss: 0.32776 G loss: 2.26873 (0.038 sec/batch, 1670.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:46,721] [train step10150] D loss: 0.32705 G loss: 2.26385 (0.037 sec/batch, 1745.184 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:47,105] [train step10161] D loss: 0.32695 G loss: 2.29040 (0.038 sec/batch, 1671.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:47,481] [train step10170] D loss: 0.32789 G loss: 2.38650 (0.033 sec/batch, 1916.999 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:47,868] [train step10180] D loss: 0.32707 G loss: 2.21397 (0.052 sec/batch, 1228.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:48,241] [train step10190] D loss: 0.32725 G loss: 2.26865 (0.036 sec/batch, 1770.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:48,622] [train step10200] D loss: 0.32743 G loss: 2.36626 (0.038 sec/batch, 1694.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:49,006] [train step10210] D loss: 0.32819 G loss: 2.15276 (0.030 sec/batch, 2114.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:49,395] [train step10220] D loss: 0.32852 G loss: 2.27571 (0.028 sec/batch, 2308.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:49,786] [train step10230] D loss: 0.32784 G loss: 2.43022 (0.040 sec/batch, 1610.030 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:50,170] [train step10241] D loss: 0.32697 G loss: 2.26940 (0.040 sec/batch, 1593.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:50,547] [train step10251] D loss: 0.32686 G loss: 2.37326 (0.038 sec/batch, 1670.788 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:50,914] [train step10260] D loss: 0.32740 G loss: 2.29131 (0.045 sec/batch, 1407.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:51,302] [train step10270] D loss: 0.32791 G loss: 2.33134 (0.037 sec/batch, 1712.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:51,679] [train step10280] D loss: 0.33018 G loss: 2.53379 (0.037 sec/batch, 1745.014 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:52,068] [train step10290] D loss: 0.32704 G loss: 2.26651 (0.037 sec/batch, 1745.615 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:52,438] [train step10301] D loss: 0.32746 G loss: 2.35208 (0.035 sec/batch, 1829.252 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:52,812] [train step10311] D loss: 0.32748 G loss: 2.26173 (0.036 sec/batch, 1764.606 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:53,202] [train step10320] D loss: 0.32712 G loss: 2.37548 (0.035 sec/batch, 1810.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:53,581] [train step10331] D loss: 0.32966 G loss: 2.43160 (0.039 sec/batch, 1629.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:53,965] [train step10340] D loss: 0.32787 G loss: 2.20921 (0.042 sec/batch, 1521.122 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:54,345] [train step10350] D loss: 0.32720 G loss: 2.34686 (0.034 sec/batch, 1859.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:54,729] [train step10361] D loss: 0.32979 G loss: 2.48267 (0.037 sec/batch, 1741.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:55,112] [train step10370] D loss: 0.32645 G loss: 2.32286 (0.039 sec/batch, 1648.431 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:55,496] [train step10380] D loss: 0.32705 G loss: 2.34143 (0.037 sec/batch, 1741.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:55,876] [train step10391] D loss: 0.32836 G loss: 2.38399 (0.041 sec/batch, 1555.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:56,258] [train step10401] D loss: 0.32744 G loss: 2.30553 (0.037 sec/batch, 1720.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:56,643] [train step10410] D loss: 0.32943 G loss: 2.16929 (0.037 sec/batch, 1720.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:57,020] [train step10420] D loss: 0.33324 G loss: 2.08827 (0.046 sec/batch, 1379.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:57,409] [train step10431] D loss: 1.32745 G loss: 13.27040 (0.040 sec/batch, 1591.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:57,794] [train step10440] D loss: 0.98704 G loss: 9.86881 (0.040 sec/batch, 1613.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:58,173] [train step10450] D loss: 0.33639 G loss: 2.38629 (0.043 sec/batch, 1499.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:58,569] [train step10460] D loss: 0.37379 G loss: 1.71155 (0.038 sec/batch, 1696.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:58,952] [train step10470] D loss: 0.54919 G loss: 5.44611 (0.041 sec/batch, 1559.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:59,343] [train step10481] D loss: 0.52082 G loss: 1.12171 (0.038 sec/batch, 1667.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:15:59,744] [train step10491] D loss: 0.84172 G loss: 8.41386 (0.038 sec/batch, 1696.950 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:00,122] [train step10500] D loss: 0.46219 G loss: 4.51517 (0.034 sec/batch, 1866.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:00,517] [train step10511] D loss: 0.95271 G loss: 9.52495 (0.038 sec/batch, 1679.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:00,903] [train step10521] D loss: 0.85500 G loss: 8.54622 (0.036 sec/batch, 1773.314 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:01,297] [train step10530] D loss: 0.39122 G loss: 3.65044 (0.039 sec/batch, 1647.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:01,679] [train step10540] D loss: 0.79280 G loss: 7.91965 (0.037 sec/batch, 1726.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:02,058] [train step10551] D loss: 0.72964 G loss: 7.27872 (0.040 sec/batch, 1599.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:02,441] [train step10560] D loss: 0.39109 G loss: 3.60639 (0.036 sec/batch, 1795.327 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:02,834] [train step10570] D loss: 0.74840 G loss: 7.47029 (0.042 sec/batch, 1535.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:03,216] [train step10580] D loss: 0.69403 G loss: 6.91562 (0.039 sec/batch, 1654.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:03,607] [train step10590] D loss: 0.40676 G loss: 3.77080 (0.045 sec/batch, 1410.161 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:03,992] [train step10601] D loss: 0.53960 G loss: 5.32439 (0.039 sec/batch, 1637.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:04,393] [train step10610] D loss: 0.37769 G loss: 3.41388 (0.039 sec/batch, 1650.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:04,776] [train step10620] D loss: 0.46817 G loss: 1.17573 (0.034 sec/batch, 1902.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:05,168] [train step10631] D loss: 0.46994 G loss: 4.55277 (0.037 sec/batch, 1736.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:05,566] [train step10641] D loss: 0.41690 G loss: 1.34816 (0.038 sec/batch, 1682.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:05,944] [train step10650] D loss: 0.41205 G loss: 3.91365 (0.037 sec/batch, 1740.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:06,324] [train step10661] D loss: 0.33967 G loss: 1.95439 (0.037 sec/batch, 1717.240 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:06,719] [train step10671] D loss: 0.33301 G loss: 2.55679 (0.037 sec/batch, 1735.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:07,115] [train step10680] D loss: 0.33107 G loss: 2.32065 (0.039 sec/batch, 1653.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:07,507] [train step10691] D loss: 0.33247 G loss: 2.29631 (0.037 sec/batch, 1725.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:07,884] [train step10701] D loss: 0.33144 G loss: 2.23116 (0.040 sec/batch, 1605.034 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:08,263] [train step10710] D loss: 0.33083 G loss: 2.43610 (0.037 sec/batch, 1744.288 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:08,653] [train step10720] D loss: 0.33183 G loss: 2.35493 (0.037 sec/batch, 1719.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:09,036] [train step10730] D loss: 0.32964 G loss: 2.40147 (0.041 sec/batch, 1569.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:09,421] [train step10740] D loss: 0.33277 G loss: 2.19509 (0.043 sec/batch, 1489.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:09,807] [train step10750] D loss: 0.32998 G loss: 2.25169 (0.033 sec/batch, 1910.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:10,205] [train step10760] D loss: 0.33396 G loss: 2.61078 (0.037 sec/batch, 1734.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:10,593] [train step10770] D loss: 0.32912 G loss: 2.41495 (0.031 sec/batch, 2098.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:10,977] [train step10781] D loss: 0.33552 G loss: 2.66439 (0.038 sec/batch, 1662.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:11,374] [train step10791] D loss: 0.33119 G loss: 2.27775 (0.037 sec/batch, 1747.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:11,763] [train step10800] D loss: 0.32972 G loss: 2.44233 (0.039 sec/batch, 1620.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:12,141] [train step10810] D loss: 0.32990 G loss: 2.22358 (0.037 sec/batch, 1713.819 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:12,530] [train step10820] D loss: 0.33013 G loss: 2.38650 (0.037 sec/batch, 1724.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:12,923] [train step10830] D loss: 0.32930 G loss: 2.17936 (0.042 sec/batch, 1523.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:13,308] [train step10841] D loss: 0.33017 G loss: 2.37501 (0.035 sec/batch, 1832.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:13,697] [train step10851] D loss: 0.32877 G loss: 2.33872 (0.040 sec/batch, 1608.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:14,088] [train step10860] D loss: 0.33184 G loss: 2.46335 (0.039 sec/batch, 1625.246 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:14,476] [train step10870] D loss: 0.33003 G loss: 2.23933 (0.041 sec/batch, 1564.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:14,859] [train step10881] D loss: 0.32963 G loss: 2.36629 (0.034 sec/batch, 1866.040 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:15,272] [train step10890] D loss: 0.32990 G loss: 2.33231 (0.040 sec/batch, 1605.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:15,681] [train step10900] D loss: 0.32963 G loss: 2.33036 (0.041 sec/batch, 1575.224 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:16,069] [train step10910] D loss: 0.33022 G loss: 2.22906 (0.035 sec/batch, 1818.016 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:16,452] [train step10920] D loss: 0.32883 G loss: 2.40763 (0.040 sec/batch, 1603.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:16,853] [train step10930] D loss: 0.33016 G loss: 2.31262 (0.044 sec/batch, 1439.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:17,224] [train step10941] D loss: 0.32990 G loss: 2.34359 (0.034 sec/batch, 1883.969 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:17,606] [train step10950] D loss: 0.32834 G loss: 2.32929 (0.035 sec/batch, 1818.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:18,001] [train step10961] D loss: 0.32989 G loss: 2.20986 (0.037 sec/batch, 1727.662 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:18,381] [train step10971] D loss: 0.32930 G loss: 2.47897 (0.031 sec/batch, 2049.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:18,778] [train step10980] D loss: 0.32933 G loss: 2.25900 (0.038 sec/batch, 1702.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:19,168] [train step10991] D loss: 0.32974 G loss: 2.24025 (0.041 sec/batch, 1573.285 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:19,560] [train step11000] D loss: 0.33095 G loss: 2.20573 (0.038 sec/batch, 1692.339 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:16:19,560] Saved checkpoint at 11000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:20,155] [train step11010] D loss: 0.32862 G loss: 2.29809 (0.043 sec/batch, 1477.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:20,532] [train step11020] D loss: 0.32793 G loss: 2.34370 (0.038 sec/batch, 1677.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:20,933] [train step11031] D loss: 0.33009 G loss: 2.45315 (0.039 sec/batch, 1657.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:21,316] [train step11040] D loss: 0.32891 G loss: 2.37293 (0.032 sec/batch, 1974.095 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:21,711] [train step11050] D loss: 0.33012 G loss: 2.37789 (0.040 sec/batch, 1599.534 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:22,098] [train step11061] D loss: 0.33134 G loss: 2.25856 (0.037 sec/batch, 1731.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:22,484] [train step11070] D loss: 0.33051 G loss: 2.50917 (0.042 sec/batch, 1509.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:22,877] [train step11081] D loss: 0.32947 G loss: 2.17446 (0.042 sec/batch, 1514.129 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:23,269] [train step11091] D loss: 0.32960 G loss: 2.25625 (0.036 sec/batch, 1759.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:23,658] [train step11100] D loss: 0.32983 G loss: 2.26379 (0.039 sec/batch, 1655.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:24,051] [train step11111] D loss: 0.32816 G loss: 2.29718 (0.038 sec/batch, 1664.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:24,437] [train step11121] D loss: 0.32952 G loss: 2.29756 (0.041 sec/batch, 1559.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:24,820] [train step11130] D loss: 0.32923 G loss: 2.28776 (0.042 sec/batch, 1534.425 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:25,227] [train step11141] D loss: 0.32902 G loss: 2.36894 (0.036 sec/batch, 1786.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:25,599] [train step11151] D loss: 0.32914 G loss: 2.37059 (0.037 sec/batch, 1739.597 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:25,989] [train step11160] D loss: 0.32879 G loss: 2.40323 (0.037 sec/batch, 1727.640 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:26,364] [train step11171] D loss: 0.32915 G loss: 2.41289 (0.035 sec/batch, 1850.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:26,745] [train step11181] D loss: 0.33003 G loss: 2.24964 (0.035 sec/batch, 1824.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:27,132] [train step11190] D loss: 0.32912 G loss: 2.47476 (0.041 sec/batch, 1550.861 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:27,523] [train step11200] D loss: 0.32950 G loss: 2.42729 (0.038 sec/batch, 1692.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:27,903] [train step11211] D loss: 0.33107 G loss: 2.19944 (0.038 sec/batch, 1691.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:28,302] [train step11220] D loss: 0.33131 G loss: 2.55418 (0.035 sec/batch, 1825.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:28,696] [train step11230] D loss: 0.32976 G loss: 2.19611 (0.039 sec/batch, 1622.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:29,093] [train step11241] D loss: 0.32867 G loss: 2.39474 (0.035 sec/batch, 1845.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:29,482] [train step11250] D loss: 0.32926 G loss: 2.26105 (0.040 sec/batch, 1595.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:29,872] [train step11260] D loss: 0.32906 G loss: 2.22952 (0.039 sec/batch, 1624.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:30,272] [train step11271] D loss: 0.32780 G loss: 2.39757 (0.039 sec/batch, 1660.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:30,652] [train step11280] D loss: 0.32991 G loss: 2.38783 (0.038 sec/batch, 1704.828 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:31,052] [train step11291] D loss: 0.32783 G loss: 2.31727 (0.040 sec/batch, 1584.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:31,461] [train step11300] D loss: 0.32848 G loss: 2.30577 (0.041 sec/batch, 1546.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:31,846] [train step11310] D loss: 0.32883 G loss: 2.26629 (0.040 sec/batch, 1589.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:32,241] [train step11320] D loss: 0.32939 G loss: 2.45629 (0.038 sec/batch, 1680.800 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:32,627] [train step11331] D loss: 0.32818 G loss: 2.27216 (0.046 sec/batch, 1397.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:33,006] [train step11340] D loss: 0.32898 G loss: 2.41005 (0.039 sec/batch, 1645.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:33,401] [train step11351] D loss: 0.32792 G loss: 2.35812 (0.037 sec/batch, 1721.237 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:33,789] [train step11360] D loss: 0.32792 G loss: 2.20882 (0.038 sec/batch, 1668.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:34,178] [train step11370] D loss: 0.32776 G loss: 2.30478 (0.052 sec/batch, 1238.817 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:34,563] [train step11381] D loss: 0.32895 G loss: 2.32403 (0.039 sec/batch, 1655.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:34,942] [train step11391] D loss: 0.32825 G loss: 2.39248 (0.036 sec/batch, 1771.828 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:35,340] [train step11400] D loss: 0.32757 G loss: 2.37568 (0.037 sec/batch, 1725.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:35,720] [train step11410] D loss: 0.32892 G loss: 2.24091 (0.035 sec/batch, 1809.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:36,097] [train step11421] D loss: 0.32782 G loss: 2.37236 (0.041 sec/batch, 1576.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:36,484] [train step11430] D loss: 0.32811 G loss: 2.24036 (0.039 sec/batch, 1624.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:36,868] [train step11441] D loss: 0.32747 G loss: 2.37287 (0.037 sec/batch, 1747.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:37,262] [train step11451] D loss: 0.32788 G loss: 2.32540 (0.045 sec/batch, 1417.473 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:37,639] [train step11460] D loss: 0.32798 G loss: 2.36285 (0.036 sec/batch, 1775.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:38,019] [train step11471] D loss: 0.32802 G loss: 2.37195 (0.037 sec/batch, 1728.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:38,409] [train step11480] D loss: 0.32758 G loss: 2.35406 (0.036 sec/batch, 1787.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:38,805] [train step11490] D loss: 0.32881 G loss: 2.37016 (0.038 sec/batch, 1705.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:39,184] [train step11501] D loss: 0.32811 G loss: 2.30313 (0.037 sec/batch, 1726.184 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:39,567] [train step11511] D loss: 0.32852 G loss: 2.24428 (0.032 sec/batch, 1989.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:39,959] [train step11520] D loss: 0.32866 G loss: 2.25586 (0.039 sec/batch, 1622.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:40,348] [train step11531] D loss: 0.32876 G loss: 2.46546 (0.050 sec/batch, 1280.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:40,736] [train step11540] D loss: 0.32753 G loss: 2.29132 (0.038 sec/batch, 1667.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:41,115] [train step11550] D loss: 0.32846 G loss: 2.29752 (0.036 sec/batch, 1793.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:41,502] [train step11561] D loss: 0.32756 G loss: 2.22946 (0.036 sec/batch, 1778.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:41,874] [train step11570] D loss: 0.32713 G loss: 2.29188 (0.039 sec/batch, 1624.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:42,288] [train step11580] D loss: 0.32874 G loss: 2.42920 (0.036 sec/batch, 1766.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:42,684] [train step11591] D loss: 0.32691 G loss: 2.27800 (0.038 sec/batch, 1689.751 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:43,066] [train step11601] D loss: 0.32752 G loss: 2.28915 (0.039 sec/batch, 1635.286 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:43,453] [train step11610] D loss: 0.32738 G loss: 2.31167 (0.039 sec/batch, 1645.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:43,842] [train step11620] D loss: 0.32776 G loss: 2.45693 (0.036 sec/batch, 1762.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:44,229] [train step11631] D loss: 0.32829 G loss: 2.37262 (0.037 sec/batch, 1749.027 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:44,629] [train step11640] D loss: 0.32654 G loss: 2.30123 (0.037 sec/batch, 1715.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:45,015] [train step11651] D loss: 0.32725 G loss: 2.33093 (0.039 sec/batch, 1625.050 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:45,388] [train step11661] D loss: 0.32787 G loss: 2.30146 (0.035 sec/batch, 1852.237 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:45,790] [train step11670] D loss: 0.32699 G loss: 2.29603 (0.040 sec/batch, 1593.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:46,172] [train step11680] D loss: 0.32707 G loss: 2.33530 (0.038 sec/batch, 1690.112 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:46,566] [train step11690] D loss: 0.32802 G loss: 2.35142 (0.043 sec/batch, 1492.759 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:46,947] [train step11700] D loss: 0.32869 G loss: 2.17866 (0.035 sec/batch, 1815.765 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:47,315] [train step11711] D loss: 0.32789 G loss: 2.39736 (0.036 sec/batch, 1797.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:47,693] [train step11720] D loss: 0.32752 G loss: 2.29354 (0.039 sec/batch, 1660.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:48,068] [train step11730] D loss: 0.32818 G loss: 2.27337 (0.041 sec/batch, 1556.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:48,447] [train step11741] D loss: 0.32834 G loss: 2.25357 (0.038 sec/batch, 1686.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:48,826] [train step11751] D loss: 0.32810 G loss: 2.39102 (0.041 sec/batch, 1547.205 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:49,193] [train step11760] D loss: 0.32704 G loss: 2.32958 (0.033 sec/batch, 1922.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:49,570] [train step11771] D loss: 0.32689 G loss: 2.23129 (0.034 sec/batch, 1867.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:49,954] [train step11781] D loss: 0.32893 G loss: 2.24016 (0.037 sec/batch, 1710.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:50,337] [train step11790] D loss: 0.32729 G loss: 2.28661 (0.039 sec/batch, 1643.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:50,720] [train step11800] D loss: 0.32803 G loss: 2.29684 (0.036 sec/batch, 1754.962 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:51,091] [train step11810] D loss: 0.32798 G loss: 2.31411 (0.031 sec/batch, 2079.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:51,473] [train step11820] D loss: 0.32811 G loss: 2.30224 (0.035 sec/batch, 1832.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:51,863] [train step11831] D loss: 0.32735 G loss: 2.30599 (0.041 sec/batch, 1554.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:52,235] [train step11841] D loss: 0.32737 G loss: 2.24876 (0.034 sec/batch, 1866.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:52,632] [train step11850] D loss: 0.32696 G loss: 2.28836 (0.046 sec/batch, 1402.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:53,011] [train step11860] D loss: 0.32743 G loss: 2.25971 (0.035 sec/batch, 1834.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:53,388] [train step11871] D loss: 0.32687 G loss: 2.32532 (0.036 sec/batch, 1774.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:53,770] [train step11880] D loss: 0.32818 G loss: 2.43519 (0.037 sec/batch, 1718.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:54,143] [train step11890] D loss: 0.32681 G loss: 2.25125 (0.042 sec/batch, 1514.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:54,514] [train step11900] D loss: 0.32729 G loss: 2.25562 (0.038 sec/batch, 1677.585 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:54,897] [train step11910] D loss: 0.32835 G loss: 2.20093 (0.040 sec/batch, 1597.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:55,277] [train step11921] D loss: 0.32638 G loss: 2.27603 (0.037 sec/batch, 1738.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:55,650] [train step11931] D loss: 0.32751 G loss: 2.33422 (0.036 sec/batch, 1788.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:56,041] [train step11940] D loss: 0.32909 G loss: 2.14237 (0.037 sec/batch, 1745.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:56,422] [train step11950] D loss: 0.32998 G loss: 2.37929 (0.041 sec/batch, 1557.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:56,805] [train step11961] D loss: 0.64264 G loss: 6.40068 (0.050 sec/batch, 1275.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:57,181] [train step11970] D loss: 0.50091 G loss: 4.91475 (0.037 sec/batch, 1719.065 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:57,562] [train step11981] D loss: 0.90713 G loss: 9.06768 (0.038 sec/batch, 1689.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:57,951] [train step11990] D loss: 0.65691 G loss: 6.54510 (0.038 sec/batch, 1673.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:58,326] [train step12000] D loss: 0.37514 G loss: 1.78611 (0.038 sec/batch, 1691.934 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:16:58,326] Saved checkpoint at 12000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:58,911] [train step12010] D loss: 0.39836 G loss: 3.69772 (0.040 sec/batch, 1612.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:59,292] [train step12020] D loss: 0.36517 G loss: 3.18606 (0.033 sec/batch, 1912.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:16:59,677] [train step12030] D loss: 0.35738 G loss: 1.91576 (0.044 sec/batch, 1462.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:00,072] [train step12040] D loss: 0.35395 G loss: 2.09266 (0.040 sec/batch, 1591.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:00,458] [train step12050] D loss: 0.34503 G loss: 2.55195 (0.039 sec/batch, 1620.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:00,836] [train step12060] D loss: 0.33759 G loss: 2.38952 (0.040 sec/batch, 1586.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:01,226] [train step12070] D loss: 0.33949 G loss: 2.41359 (0.038 sec/batch, 1696.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:01,604] [train step12081] D loss: 0.66499 G loss: 6.63356 (0.036 sec/batch, 1794.583 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:01,998] [train step12090] D loss: 0.54241 G loss: 5.37136 (0.040 sec/batch, 1596.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:02,378] [train step12100] D loss: 0.47414 G loss: 4.63541 (0.037 sec/batch, 1707.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:02,774] [train step12111] D loss: 0.56553 G loss: 0.98559 (0.034 sec/batch, 1897.902 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:03,170] [train step12120] D loss: 0.93999 G loss: 9.39418 (0.039 sec/batch, 1623.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:03,566] [train step12131] D loss: 0.78248 G loss: 7.81560 (0.036 sec/batch, 1792.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:03,939] [train step12141] D loss: 0.93282 G loss: 0.54273 (0.036 sec/batch, 1779.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:04,326] [train step12150] D loss: 0.85042 G loss: 8.49639 (0.035 sec/batch, 1810.719 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:04,714] [train step12160] D loss: 0.84486 G loss: 8.43849 (0.037 sec/batch, 1730.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:05,106] [train step12171] D loss: 0.41555 G loss: 1.48047 (0.036 sec/batch, 1755.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:05,485] [train step12180] D loss: 0.33708 G loss: 2.52327 (0.034 sec/batch, 1873.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:05,853] [train step12190] D loss: 0.33862 G loss: 2.11036 (0.030 sec/batch, 2105.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:06,262] [train step12200] D loss: 0.33535 G loss: 2.56725 (0.039 sec/batch, 1633.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:06,646] [train step12210] D loss: 0.33303 G loss: 2.33344 (0.035 sec/batch, 1806.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:07,033] [train step12221] D loss: 0.33358 G loss: 2.31019 (0.035 sec/batch, 1836.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:07,421] [train step12231] D loss: 0.33420 G loss: 2.37491 (0.036 sec/batch, 1759.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:07,801] [train step12240] D loss: 0.33325 G loss: 2.27206 (0.035 sec/batch, 1824.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:08,194] [train step12251] D loss: 0.33270 G loss: 2.47420 (0.036 sec/batch, 1785.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:08,574] [train step12260] D loss: 0.33358 G loss: 2.26329 (0.034 sec/batch, 1861.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:08,953] [train step12270] D loss: 0.33848 G loss: 2.71419 (0.037 sec/batch, 1730.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:09,345] [train step12281] D loss: 0.33528 G loss: 2.25011 (0.037 sec/batch, 1731.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:09,722] [train step12291] D loss: 0.33269 G loss: 2.17876 (0.040 sec/batch, 1610.977 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:10,100] [train step12300] D loss: 0.33509 G loss: 2.10595 (0.035 sec/batch, 1816.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:10,489] [train step12311] D loss: 0.33040 G loss: 2.30593 (0.037 sec/batch, 1748.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:10,870] [train step12321] D loss: 0.33122 G loss: 2.28685 (0.040 sec/batch, 1602.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:11,262] [train step12330] D loss: 0.33227 G loss: 2.14229 (0.040 sec/batch, 1616.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:11,643] [train step12341] D loss: 0.33219 G loss: 2.44174 (0.036 sec/batch, 1766.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:12,026] [train step12351] D loss: 0.33210 G loss: 2.50328 (0.041 sec/batch, 1563.489 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:12,403] [train step12360] D loss: 0.33181 G loss: 2.16759 (0.035 sec/batch, 1804.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:12,782] [train step12370] D loss: 0.33105 G loss: 2.24796 (0.037 sec/batch, 1713.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:13,170] [train step12381] D loss: 0.33423 G loss: 2.52837 (0.037 sec/batch, 1736.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:13,557] [train step12390] D loss: 0.41674 G loss: 3.95597 (0.037 sec/batch, 1732.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:13,952] [train step12400] D loss: 0.76498 G loss: 7.54627 (0.037 sec/batch, 1720.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:14,357] [train step12410] D loss: 0.42487 G loss: 1.62664 (0.039 sec/batch, 1649.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:14,739] [train step12420] D loss: 0.37964 G loss: 3.36181 (0.042 sec/batch, 1510.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:15,122] [train step12431] D loss: 0.34916 G loss: 1.92219 (0.035 sec/batch, 1831.386 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:15,525] [train step12440] D loss: 0.35027 G loss: 2.99757 (0.046 sec/batch, 1378.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:15,905] [train step12450] D loss: 0.36247 G loss: 1.70569 (0.037 sec/batch, 1718.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:16,290] [train step12460] D loss: 0.33170 G loss: 2.46370 (0.044 sec/batch, 1447.521 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:16,683] [train step12471] D loss: 0.33412 G loss: 2.54069 (0.038 sec/batch, 1682.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:17,049] [train step12480] D loss: 0.33005 G loss: 2.32859 (0.034 sec/batch, 1902.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:17,432] [train step12491] D loss: 0.33076 G loss: 2.22478 (0.036 sec/batch, 1782.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:17,804] [train step12501] D loss: 0.33402 G loss: 2.65332 (0.040 sec/batch, 1602.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:18,181] [train step12510] D loss: 0.33079 G loss: 2.23169 (0.040 sec/batch, 1602.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:18,561] [train step12521] D loss: 0.33017 G loss: 2.43826 (0.034 sec/batch, 1871.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:18,930] [train step12531] D loss: 0.32909 G loss: 2.32900 (0.035 sec/batch, 1831.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:19,309] [train step12540] D loss: 0.33032 G loss: 2.38466 (0.035 sec/batch, 1846.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:19,687] [train step12551] D loss: 0.32910 G loss: 2.25511 (0.036 sec/batch, 1801.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:20,064] [train step12560] D loss: 0.32878 G loss: 2.34921 (0.035 sec/batch, 1807.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:20,448] [train step12570] D loss: 0.32930 G loss: 2.23506 (0.044 sec/batch, 1470.420 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:20,832] [train step12581] D loss: 0.32967 G loss: 2.19112 (0.038 sec/batch, 1684.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:21,204] [train step12591] D loss: 0.32989 G loss: 2.44849 (0.037 sec/batch, 1753.391 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:21,588] [train step12600] D loss: 0.32872 G loss: 2.32516 (0.034 sec/batch, 1885.147 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:21,963] [train step12611] D loss: 0.33007 G loss: 2.44355 (0.038 sec/batch, 1703.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:22,338] [train step12620] D loss: 0.32997 G loss: 2.45657 (0.036 sec/batch, 1772.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:22,717] [train step12630] D loss: 0.32735 G loss: 2.33103 (0.038 sec/batch, 1668.534 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:23,089] [train step12640] D loss: 0.33084 G loss: 2.51091 (0.036 sec/batch, 1774.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:23,474] [train step12651] D loss: 0.32825 G loss: 2.25826 (0.042 sec/batch, 1509.353 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:23,860] [train step12660] D loss: 0.32872 G loss: 2.38799 (0.039 sec/batch, 1622.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:24,233] [train step12671] D loss: 0.33065 G loss: 2.44573 (0.033 sec/batch, 1946.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:24,631] [train step12681] D loss: 0.32807 G loss: 2.23791 (0.034 sec/batch, 1868.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:25,008] [train step12690] D loss: 0.33080 G loss: 2.23978 (0.034 sec/batch, 1862.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:25,391] [train step12700] D loss: 0.32963 G loss: 2.39560 (0.036 sec/batch, 1767.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:25,779] [train step12710] D loss: 0.32879 G loss: 2.36284 (0.038 sec/batch, 1678.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:26,163] [train step12720] D loss: 0.32823 G loss: 2.34969 (0.041 sec/batch, 1557.809 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:26,543] [train step12730] D loss: 0.32980 G loss: 2.35671 (0.042 sec/batch, 1525.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:26,929] [train step12741] D loss: 0.32951 G loss: 2.21704 (0.038 sec/batch, 1704.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:27,304] [train step12750] D loss: 0.32914 G loss: 2.43191 (0.035 sec/batch, 1821.025 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:27,697] [train step12761] D loss: 0.32785 G loss: 2.30931 (0.040 sec/batch, 1611.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:28,081] [train step12770] D loss: 0.32756 G loss: 2.35221 (0.037 sec/batch, 1736.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:28,460] [train step12780] D loss: 0.32944 G loss: 2.37890 (0.041 sec/batch, 1558.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:28,850] [train step12790] D loss: 0.32920 G loss: 2.38135 (0.036 sec/batch, 1784.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:29,230] [train step12800] D loss: 0.33145 G loss: 2.55307 (0.037 sec/batch, 1739.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:29,609] [train step12810] D loss: 0.34409 G loss: 1.88603 (0.034 sec/batch, 1893.431 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:30,010] [train step12821] D loss: 0.32877 G loss: 2.47142 (0.038 sec/batch, 1680.863 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:30,394] [train step12831] D loss: 0.32739 G loss: 2.32921 (0.038 sec/batch, 1666.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:30,778] [train step12840] D loss: 0.32877 G loss: 2.35886 (0.039 sec/batch, 1658.319 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:31,159] [train step12851] D loss: 0.32984 G loss: 2.35768 (0.037 sec/batch, 1738.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:31,543] [train step12861] D loss: 0.32775 G loss: 2.30797 (0.038 sec/batch, 1689.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:31,936] [train step12870] D loss: 0.32885 G loss: 2.25381 (0.038 sec/batch, 1672.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:32,313] [train step12881] D loss: 0.33108 G loss: 2.25580 (0.038 sec/batch, 1675.543 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:32,697] [train step12890] D loss: 0.32845 G loss: 2.35187 (0.042 sec/batch, 1529.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:33,084] [train step12900] D loss: 0.32885 G loss: 2.39567 (0.035 sec/batch, 1849.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:33,464] [train step12910] D loss: 0.32843 G loss: 2.37870 (0.037 sec/batch, 1753.368 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:33,851] [train step12921] D loss: 0.32849 G loss: 2.30696 (0.040 sec/batch, 1609.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:34,243] [train step12930] D loss: 0.33087 G loss: 2.15109 (0.037 sec/batch, 1732.848 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:34,619] [train step12941] D loss: 0.33329 G loss: 2.54733 (0.033 sec/batch, 1955.543 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:35,009] [train step12950] D loss: 0.32983 G loss: 2.19210 (0.038 sec/batch, 1694.027 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:35,412] [train step12960] D loss: 0.32690 G loss: 2.29832 (0.041 sec/batch, 1553.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:35,794] [train step12970] D loss: 0.32724 G loss: 2.35300 (0.037 sec/batch, 1727.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:36,187] [train step12980] D loss: 0.32939 G loss: 2.37158 (0.042 sec/batch, 1523.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:36,569] [train step12990] D loss: 0.32820 G loss: 2.16439 (0.044 sec/batch, 1455.014 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:36,956] [train step13001] D loss: 0.33132 G loss: 2.55475 (0.042 sec/batch, 1523.773 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:17:36,957] Saved checkpoint at 13000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:37,531] [train step13011] D loss: 0.32834 G loss: 2.26421 (0.035 sec/batch, 1804.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:37,910] [train step13020] D loss: 0.32699 G loss: 2.29056 (0.036 sec/batch, 1755.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:38,301] [train step13031] D loss: 0.32760 G loss: 2.42901 (0.035 sec/batch, 1849.608 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:38,704] [train step13040] D loss: 0.32750 G loss: 2.34650 (0.039 sec/batch, 1661.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:39,092] [train step13050] D loss: 0.33040 G loss: 2.42934 (0.039 sec/batch, 1622.457 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:39,487] [train step13060] D loss: 0.32780 G loss: 2.19243 (0.044 sec/batch, 1453.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:39,859] [train step13071] D loss: 0.32750 G loss: 2.37702 (0.036 sec/batch, 1792.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:40,263] [train step13080] D loss: 0.32945 G loss: 2.40904 (0.041 sec/batch, 1561.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:40,643] [train step13090] D loss: 0.32808 G loss: 2.37428 (0.041 sec/batch, 1549.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:41,027] [train step13100] D loss: 0.32772 G loss: 2.35355 (0.047 sec/batch, 1353.759 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:41,404] [train step13110] D loss: 0.32862 G loss: 2.31153 (0.040 sec/batch, 1584.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:41,782] [train step13121] D loss: 0.32825 G loss: 2.39758 (0.037 sec/batch, 1709.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:42,171] [train step13130] D loss: 0.33310 G loss: 2.46067 (0.037 sec/batch, 1710.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:42,561] [train step13140] D loss: 0.32944 G loss: 2.43125 (0.037 sec/batch, 1728.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:42,932] [train step13150] D loss: 0.32939 G loss: 2.18750 (0.036 sec/batch, 1778.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:43,326] [train step13160] D loss: 0.32829 G loss: 2.40385 (0.042 sec/batch, 1528.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:43,707] [train step13170] D loss: 0.32628 G loss: 2.27816 (0.036 sec/batch, 1781.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:44,098] [train step13181] D loss: 0.32787 G loss: 2.41982 (0.039 sec/batch, 1621.526 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:44,484] [train step13191] D loss: 0.32964 G loss: 2.39227 (0.037 sec/batch, 1710.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:44,865] [train step13200] D loss: 0.32783 G loss: 2.26597 (0.038 sec/batch, 1682.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:45,259] [train step13211] D loss: 0.32887 G loss: 2.29715 (0.040 sec/batch, 1602.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:45,639] [train step13221] D loss: 0.32690 G loss: 2.36709 (0.036 sec/batch, 1771.430 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:46,036] [train step13230] D loss: 0.32867 G loss: 2.16944 (0.037 sec/batch, 1743.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:46,430] [train step13241] D loss: 0.33265 G loss: 2.03234 (0.041 sec/batch, 1566.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:46,826] [train step13250] D loss: 0.33199 G loss: 2.60767 (0.034 sec/batch, 1872.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:47,213] [train step13260] D loss: 0.34072 G loss: 1.92494 (0.046 sec/batch, 1400.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:47,594] [train step13271] D loss: 0.33659 G loss: 2.56365 (0.037 sec/batch, 1718.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:47,971] [train step13280] D loss: 0.33519 G loss: 2.14415 (0.044 sec/batch, 1452.824 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:48,358] [train step13290] D loss: 0.33028 G loss: 2.40360 (0.035 sec/batch, 1824.887 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:48,731] [train step13301] D loss: 0.33668 G loss: 2.26105 (0.037 sec/batch, 1747.797 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:49,117] [train step13310] D loss: 0.34405 G loss: 2.81478 (0.037 sec/batch, 1725.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:49,495] [train step13320] D loss: 0.32986 G loss: 2.49247 (0.042 sec/batch, 1524.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:49,873] [train step13331] D loss: 0.32809 G loss: 2.34722 (0.037 sec/batch, 1708.800 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:50,246] [train step13340] D loss: 0.32947 G loss: 2.47188 (0.034 sec/batch, 1862.816 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:50,634] [train step13350] D loss: 0.33350 G loss: 2.06607 (0.037 sec/batch, 1706.910 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:51,003] [train step13361] D loss: 0.32966 G loss: 2.52431 (0.038 sec/batch, 1701.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:51,379] [train step13370] D loss: 0.33146 G loss: 2.26374 (0.032 sec/batch, 1974.269 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:51,760] [train step13380] D loss: 0.32969 G loss: 2.24049 (0.035 sec/batch, 1807.403 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:52,133] [train step13391] D loss: 0.32805 G loss: 2.42136 (0.039 sec/batch, 1654.395 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:52,520] [train step13401] D loss: 0.33893 G loss: 2.20181 (0.037 sec/batch, 1715.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:52,897] [train step13410] D loss: 0.32801 G loss: 2.29487 (0.038 sec/batch, 1695.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:53,277] [train step13421] D loss: 0.33005 G loss: 2.49910 (0.039 sec/batch, 1658.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:53,673] [train step13430] D loss: 0.32867 G loss: 2.31844 (0.036 sec/batch, 1787.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:54,060] [train step13440] D loss: 0.32954 G loss: 2.36097 (0.040 sec/batch, 1619.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:54,456] [train step13450] D loss: 0.32826 G loss: 2.30184 (0.045 sec/batch, 1428.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:54,840] [train step13461] D loss: 0.32781 G loss: 2.28132 (0.039 sec/batch, 1653.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:55,205] [train step13470] D loss: 0.32905 G loss: 2.19296 (0.029 sec/batch, 2209.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:55,607] [train step13480] D loss: 0.32930 G loss: 2.42956 (0.046 sec/batch, 1392.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:55,988] [train step13491] D loss: 0.32706 G loss: 2.26688 (0.037 sec/batch, 1731.585 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:56,382] [train step13500] D loss: 0.32803 G loss: 2.42241 (0.057 sec/batch, 1120.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:56,766] [train step13511] D loss: 0.32784 G loss: 2.36325 (0.034 sec/batch, 1877.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:57,149] [train step13520] D loss: 0.33513 G loss: 2.29901 (0.035 sec/batch, 1815.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:57,537] [train step13530] D loss: 0.35073 G loss: 2.66624 (0.035 sec/batch, 1845.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:57,925] [train step13540] D loss: 0.33683 G loss: 2.46125 (0.032 sec/batch, 1972.876 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:58,307] [train step13550] D loss: 0.32779 G loss: 2.45966 (0.040 sec/batch, 1592.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:58,692] [train step13560] D loss: 0.34805 G loss: 1.78599 (0.039 sec/batch, 1654.395 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:59,076] [train step13570] D loss: 0.32941 G loss: 2.36711 (0.038 sec/batch, 1687.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:59,454] [train step13581] D loss: 0.33914 G loss: 2.61567 (0.035 sec/batch, 1805.482 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:17:59,849] [train step13590] D loss: 0.34385 G loss: 2.59473 (0.031 sec/batch, 2062.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:00,233] [train step13601] D loss: 0.32744 G loss: 2.33181 (0.038 sec/batch, 1695.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:00,627] [train step13610] D loss: 0.33616 G loss: 2.55543 (0.045 sec/batch, 1431.244 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:01,010] [train step13620] D loss: 0.33180 G loss: 2.48978 (0.040 sec/batch, 1600.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:01,383] [train step13630] D loss: 0.32807 G loss: 2.26670 (0.026 sec/batch, 2418.185 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:01,780] [train step13641] D loss: 0.33144 G loss: 2.42663 (0.037 sec/batch, 1733.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:02,167] [train step13650] D loss: 0.32853 G loss: 2.33239 (0.034 sec/batch, 1866.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:02,551] [train step13660] D loss: 0.36971 G loss: 3.35753 (0.040 sec/batch, 1602.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:02,942] [train step13670] D loss: 0.33083 G loss: 2.33846 (0.038 sec/batch, 1678.320 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:03,320] [train step13680] D loss: 0.32908 G loss: 2.25854 (0.036 sec/batch, 1798.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:03,705] [train step13690] D loss: 0.32636 G loss: 2.31845 (0.039 sec/batch, 1649.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:04,092] [train step13701] D loss: 0.33494 G loss: 2.52776 (0.037 sec/batch, 1733.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:04,476] [train step13710] D loss: 0.33061 G loss: 2.26514 (0.035 sec/batch, 1821.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:04,865] [train step13721] D loss: 0.32834 G loss: 2.30589 (0.039 sec/batch, 1648.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:05,248] [train step13731] D loss: 0.33072 G loss: 2.32532 (0.036 sec/batch, 1782.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:05,631] [train step13740] D loss: 0.32827 G loss: 2.37964 (0.035 sec/batch, 1813.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:06,020] [train step13750] D loss: 0.33501 G loss: 2.38032 (0.036 sec/batch, 1762.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:06,407] [train step13760] D loss: 0.32832 G loss: 2.38399 (0.035 sec/batch, 1839.078 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:06,797] [train step13770] D loss: 0.32894 G loss: 2.27675 (0.037 sec/batch, 1721.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:07,190] [train step13781] D loss: 0.32705 G loss: 2.27302 (0.037 sec/batch, 1723.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:07,565] [train step13790] D loss: 0.32796 G loss: 2.37026 (0.034 sec/batch, 1881.394 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:07,959] [train step13800] D loss: 0.33148 G loss: 2.39743 (0.038 sec/batch, 1696.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:08,334] [train step13811] D loss: 0.33606 G loss: 2.43737 (0.040 sec/batch, 1615.806 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:08,719] [train step13820] D loss: 0.33416 G loss: 2.23871 (0.034 sec/batch, 1895.838 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:09,110] [train step13830] D loss: 0.33122 G loss: 2.16546 (0.036 sec/batch, 1777.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:09,497] [train step13840] D loss: 0.32924 G loss: 2.21376 (0.036 sec/batch, 1795.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:09,890] [train step13850] D loss: 0.33894 G loss: 2.57496 (0.035 sec/batch, 1828.754 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:10,274] [train step13860] D loss: 0.32886 G loss: 2.39442 (0.036 sec/batch, 1791.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:10,659] [train step13870] D loss: 0.35796 G loss: 2.74675 (0.040 sec/batch, 1610.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:11,054] [train step13881] D loss: 0.34642 G loss: 2.46723 (0.041 sec/batch, 1566.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:11,432] [train step13890] D loss: 0.48163 G loss: 1.30576 (0.037 sec/batch, 1738.234 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:11,809] [train step13901] D loss: 0.36605 G loss: 2.43500 (0.039 sec/batch, 1655.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:12,200] [train step13910] D loss: 0.35858 G loss: 2.46677 (0.036 sec/batch, 1781.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:12,587] [train step13920] D loss: 0.35734 G loss: 2.43816 (0.038 sec/batch, 1689.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:12,983] [train step13931] D loss: 0.34506 G loss: 2.70838 (0.038 sec/batch, 1686.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:13,367] [train step13940] D loss: 0.33601 G loss: 2.17745 (0.040 sec/batch, 1590.314 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:13,753] [train step13950] D loss: 0.35458 G loss: 3.08554 (0.036 sec/batch, 1779.580 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:14,154] [train step13961] D loss: 0.33094 G loss: 2.39285 (0.038 sec/batch, 1706.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:14,528] [train step13970] D loss: 0.33384 G loss: 2.20561 (0.034 sec/batch, 1860.324 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:14,913] [train step13980] D loss: 0.34519 G loss: 2.89379 (0.038 sec/batch, 1674.759 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:15,298] [train step13991] D loss: 0.33917 G loss: 2.74820 (0.037 sec/batch, 1744.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:15,693] [train step14000] D loss: 0.33491 G loss: 2.52409 (0.047 sec/batch, 1361.373 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:18:15,693] Saved checkpoint at 14000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:16,299] [train step14010] D loss: 0.33242 G loss: 2.44967 (0.037 sec/batch, 1721.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:16,693] [train step14021] D loss: 0.33392 G loss: 2.60116 (0.048 sec/batch, 1334.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:17,091] [train step14031] D loss: 0.34168 G loss: 2.79966 (0.036 sec/batch, 1770.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:17,473] [train step14040] D loss: 0.34095 G loss: 2.00941 (0.038 sec/batch, 1669.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:17,868] [train step14050] D loss: 0.34045 G loss: 1.96318 (0.038 sec/batch, 1674.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:18,250] [train step14061] D loss: 0.33449 G loss: 2.23841 (0.036 sec/batch, 1767.534 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:18,630] [train step14070] D loss: 0.33051 G loss: 2.36972 (0.037 sec/batch, 1748.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:19,028] [train step14081] D loss: 0.34220 G loss: 2.86411 (0.043 sec/batch, 1489.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:19,418] [train step14090] D loss: 0.33118 G loss: 2.52426 (0.047 sec/batch, 1347.574 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:19,800] [train step14100] D loss: 0.33057 G loss: 2.15635 (0.038 sec/batch, 1705.283 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:20,186] [train step14110] D loss: 0.32961 G loss: 2.43423 (0.036 sec/batch, 1802.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:20,582] [train step14120] D loss: 0.32931 G loss: 2.32130 (0.040 sec/batch, 1582.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:20,968] [train step14130] D loss: 0.33237 G loss: 2.59023 (0.040 sec/batch, 1588.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:21,345] [train step14140] D loss: 0.34127 G loss: 2.86497 (0.034 sec/batch, 1881.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:21,721] [train step14150] D loss: 0.33269 G loss: 2.09017 (0.038 sec/batch, 1690.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:22,091] [train step14160] D loss: 0.34035 G loss: 2.83737 (0.035 sec/batch, 1807.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:22,474] [train step14170] D loss: 0.33045 G loss: 2.24694 (0.037 sec/batch, 1714.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:22,842] [train step14180] D loss: 0.33116 G loss: 2.10324 (0.036 sec/batch, 1793.240 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:23,221] [train step14190] D loss: 0.32940 G loss: 2.41863 (0.037 sec/batch, 1735.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:23,594] [train step14200] D loss: 0.33115 G loss: 2.15003 (0.031 sec/batch, 2043.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:23,984] [train step14211] D loss: 0.32969 G loss: 2.21520 (0.038 sec/batch, 1696.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:24,359] [train step14220] D loss: 0.32954 G loss: 2.42895 (0.034 sec/batch, 1857.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:24,736] [train step14231] D loss: 0.32778 G loss: 2.39454 (0.038 sec/batch, 1673.277 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:25,115] [train step14240] D loss: 0.32937 G loss: 2.37134 (0.038 sec/batch, 1704.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:25,499] [train step14250] D loss: 0.32918 G loss: 2.30954 (0.041 sec/batch, 1563.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:25,872] [train step14260] D loss: 0.32887 G loss: 2.40162 (0.037 sec/batch, 1746.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:26,260] [train step14271] D loss: 0.32943 G loss: 2.33830 (0.048 sec/batch, 1321.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:26,639] [train step14280] D loss: 0.33003 G loss: 2.38337 (0.036 sec/batch, 1780.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:27,009] [train step14291] D loss: 0.33141 G loss: 2.52135 (0.036 sec/batch, 1771.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:27,397] [train step14301] D loss: 0.32867 G loss: 2.21250 (0.035 sec/batch, 1851.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:27,774] [train step14310] D loss: 0.33036 G loss: 2.50319 (0.036 sec/batch, 1796.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:28,155] [train step14321] D loss: 0.32937 G loss: 2.49456 (0.036 sec/batch, 1768.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:28,562] [train step14331] D loss: 0.32846 G loss: 2.38132 (0.035 sec/batch, 1810.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:28,946] [train step14340] D loss: 0.32859 G loss: 2.47872 (0.039 sec/batch, 1661.172 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:29,328] [train step14351] D loss: 0.34614 G loss: 2.97119 (0.044 sec/batch, 1467.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:29,707] [train step14361] D loss: 0.32860 G loss: 2.44111 (0.043 sec/batch, 1494.397 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:30,088] [train step14370] D loss: 0.32873 G loss: 2.36236 (0.041 sec/batch, 1549.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:30,477] [train step14380] D loss: 0.32795 G loss: 2.35258 (0.035 sec/batch, 1846.554 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:30,861] [train step14390] D loss: 0.33000 G loss: 2.16207 (0.040 sec/batch, 1589.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:31,245] [train step14400] D loss: 0.32798 G loss: 2.43197 (0.038 sec/batch, 1680.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:31,638] [train step14410] D loss: 0.32985 G loss: 2.14909 (0.041 sec/batch, 1569.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:32,023] [train step14421] D loss: 0.32941 G loss: 2.16341 (0.038 sec/batch, 1695.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:32,412] [train step14430] D loss: 0.32863 G loss: 2.38056 (0.038 sec/batch, 1667.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:32,797] [train step14441] D loss: 0.32765 G loss: 2.33933 (0.036 sec/batch, 1774.733 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:33,178] [train step14451] D loss: 0.32878 G loss: 2.27257 (0.042 sec/batch, 1508.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:33,560] [train step14460] D loss: 0.32788 G loss: 2.36082 (0.039 sec/batch, 1656.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:33,939] [train step14471] D loss: 0.32811 G loss: 2.30390 (0.041 sec/batch, 1571.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:34,322] [train step14480] D loss: 0.32792 G loss: 2.28577 (0.039 sec/batch, 1632.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:34,727] [train step14490] D loss: 0.32977 G loss: 2.18745 (0.035 sec/batch, 1837.265 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:35,110] [train step14501] D loss: 0.33053 G loss: 2.11468 (0.039 sec/batch, 1628.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:35,499] [train step14510] D loss: 0.32854 G loss: 2.34931 (0.041 sec/batch, 1576.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:35,885] [train step14520] D loss: 0.32840 G loss: 2.19411 (0.039 sec/batch, 1620.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:36,261] [train step14531] D loss: 0.32715 G loss: 2.29726 (0.036 sec/batch, 1792.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:36,656] [train step14540] D loss: 0.33840 G loss: 2.79153 (0.044 sec/batch, 1461.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:37,036] [train step14550] D loss: 0.32976 G loss: 2.15555 (0.033 sec/batch, 1923.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:37,416] [train step14560] D loss: 0.33162 G loss: 2.51208 (0.041 sec/batch, 1548.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:37,807] [train step14570] D loss: 0.33444 G loss: 2.67813 (0.040 sec/batch, 1591.587 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:38,193] [train step14580] D loss: 0.33046 G loss: 2.17348 (0.038 sec/batch, 1692.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:38,585] [train step14591] D loss: 0.32926 G loss: 2.21114 (0.040 sec/batch, 1587.117 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:38,974] [train step14601] D loss: 0.32773 G loss: 2.40464 (0.046 sec/batch, 1382.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:39,363] [train step14610] D loss: 0.32794 G loss: 2.28258 (0.036 sec/batch, 1788.711 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:39,754] [train step14620] D loss: 0.33043 G loss: 2.11498 (0.034 sec/batch, 1872.078 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:40,146] [train step14631] D loss: 0.33170 G loss: 2.08256 (0.037 sec/batch, 1724.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:40,524] [train step14640] D loss: 0.33033 G loss: 2.54310 (0.038 sec/batch, 1697.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:40,913] [train step14651] D loss: 0.32718 G loss: 2.35654 (0.035 sec/batch, 1844.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:41,290] [train step14660] D loss: 0.33034 G loss: 2.13013 (0.037 sec/batch, 1709.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:41,684] [train step14670] D loss: 0.32883 G loss: 2.32428 (0.035 sec/batch, 1853.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:42,067] [train step14680] D loss: 0.32909 G loss: 2.18645 (0.040 sec/batch, 1592.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:42,442] [train step14691] D loss: 0.33074 G loss: 2.12951 (0.041 sec/batch, 1549.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:42,827] [train step14700] D loss: 0.33297 G loss: 2.63523 (0.036 sec/batch, 1768.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:43,203] [train step14711] D loss: 0.32841 G loss: 2.44007 (0.035 sec/batch, 1837.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:43,597] [train step14721] D loss: 0.32781 G loss: 2.40585 (0.037 sec/batch, 1715.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:43,979] [train step14730] D loss: 0.32783 G loss: 2.29841 (0.031 sec/batch, 2081.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:44,365] [train step14741] D loss: 0.32827 G loss: 2.44457 (0.040 sec/batch, 1605.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:44,752] [train step14751] D loss: 0.32937 G loss: 2.44731 (0.037 sec/batch, 1720.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:45,142] [train step14760] D loss: 0.32920 G loss: 2.16502 (0.040 sec/batch, 1581.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:45,520] [train step14771] D loss: 0.32951 G loss: 2.14330 (0.037 sec/batch, 1730.904 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:45,904] [train step14781] D loss: 0.33095 G loss: 2.09387 (0.035 sec/batch, 1834.678 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:46,291] [train step14790] D loss: 0.32815 G loss: 2.40103 (0.036 sec/batch, 1759.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:46,685] [train step14800] D loss: 0.32730 G loss: 2.33603 (0.040 sec/batch, 1609.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:47,073] [train step14810] D loss: 0.32771 G loss: 2.35881 (0.037 sec/batch, 1710.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:47,443] [train step14820] D loss: 0.32838 G loss: 2.23047 (0.037 sec/batch, 1715.495 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:47,829] [train step14830] D loss: 0.32784 G loss: 2.21174 (0.045 sec/batch, 1412.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:48,205] [train step14840] D loss: 0.32902 G loss: 2.14759 (0.034 sec/batch, 1870.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:48,583] [train step14850] D loss: 0.33005 G loss: 2.54648 (0.036 sec/batch, 1767.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:48,955] [train step14861] D loss: 0.32898 G loss: 2.49333 (0.034 sec/batch, 1861.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:49,327] [train step14870] D loss: 0.32912 G loss: 2.51273 (0.037 sec/batch, 1731.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:49,718] [train step14880] D loss: 0.32753 G loss: 2.22590 (0.036 sec/batch, 1757.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:50,096] [train step14890] D loss: 0.32713 G loss: 2.26730 (0.040 sec/batch, 1610.320 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:50,467] [train step14901] D loss: 0.32888 G loss: 2.23674 (0.035 sec/batch, 1851.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:50,842] [train step14910] D loss: 0.32682 G loss: 2.27786 (0.044 sec/batch, 1461.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:51,229] [train step14921] D loss: 0.32746 G loss: 2.30246 (0.034 sec/batch, 1869.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:51,602] [train step14930] D loss: 0.32813 G loss: 2.16224 (0.042 sec/batch, 1538.815 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:51,983] [train step14940] D loss: 0.33057 G loss: 2.58940 (0.039 sec/batch, 1625.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:52,364] [train step14951] D loss: 0.33351 G loss: 2.67590 (0.036 sec/batch, 1799.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:52,745] [train step14960] D loss: 0.32920 G loss: 2.53022 (0.038 sec/batch, 1677.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:53,124] [train step14970] D loss: 0.32819 G loss: 2.20291 (0.039 sec/batch, 1637.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:53,500] [train step14981] D loss: 0.32709 G loss: 2.26191 (0.036 sec/batch, 1762.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:53,865] [train step14991] D loss: 0.32648 G loss: 2.30861 (0.034 sec/batch, 1865.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:54,260] [train step15000] D loss: 0.32782 G loss: 2.36105 (0.032 sec/batch, 1998.849 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:18:54,260] Saved checkpoint at 15000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:54,834] [train step15010] D loss: 0.32720 G loss: 2.31433 (0.037 sec/batch, 1722.838 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:55,221] [train step15021] D loss: 0.32684 G loss: 2.31823 (0.038 sec/batch, 1679.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:55,593] [train step15030] D loss: 0.32729 G loss: 2.39261 (0.038 sec/batch, 1674.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:55,977] [train step15040] D loss: 0.32968 G loss: 2.56548 (0.038 sec/batch, 1679.433 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:56,359] [train step15051] D loss: 0.33522 G loss: 2.73221 (0.040 sec/batch, 1588.940 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:56,741] [train step15060] D loss: 0.32946 G loss: 2.12273 (0.040 sec/batch, 1598.049 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:57,128] [train step15070] D loss: 0.32890 G loss: 2.48533 (0.040 sec/batch, 1602.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:57,504] [train step15080] D loss: 0.32683 G loss: 2.36210 (0.036 sec/batch, 1755.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:57,881] [train step15090] D loss: 0.32726 G loss: 2.29776 (0.036 sec/batch, 1792.857 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:58,277] [train step15100] D loss: 0.32657 G loss: 2.32103 (0.038 sec/batch, 1683.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:58,656] [train step15110] D loss: 0.32705 G loss: 2.39281 (0.036 sec/batch, 1760.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:59,032] [train step15120] D loss: 0.32829 G loss: 2.44501 (0.033 sec/batch, 1923.166 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:59,418] [train step15131] D loss: 0.33383 G loss: 2.69461 (0.037 sec/batch, 1723.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:18:59,794] [train step15141] D loss: 0.32699 G loss: 2.26370 (0.037 sec/batch, 1716.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:00,200] [train step15150] D loss: 0.32795 G loss: 2.45200 (0.041 sec/batch, 1554.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:00,589] [train step15161] D loss: 0.32803 G loss: 2.46269 (0.043 sec/batch, 1502.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:00,972] [train step15171] D loss: 0.32708 G loss: 2.29727 (0.040 sec/batch, 1601.902 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:01,369] [train step15180] D loss: 0.32683 G loss: 2.33146 (0.037 sec/batch, 1709.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:01,749] [train step15190] D loss: 0.32727 G loss: 2.26703 (0.037 sec/batch, 1716.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:02,127] [train step15200] D loss: 0.32782 G loss: 2.45999 (0.037 sec/batch, 1717.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:02,525] [train step15210] D loss: 0.32712 G loss: 2.22646 (0.043 sec/batch, 1503.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:02,907] [train step15220] D loss: 0.32800 G loss: 2.14784 (0.036 sec/batch, 1791.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:03,301] [train step15231] D loss: 0.32894 G loss: 2.13954 (0.039 sec/batch, 1660.597 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:03,677] [train step15240] D loss: 0.33066 G loss: 2.58373 (0.035 sec/batch, 1832.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:04,064] [train step15251] D loss: 0.32772 G loss: 2.47399 (0.036 sec/batch, 1787.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:04,457] [train step15260] D loss: 0.32694 G loss: 2.41409 (0.043 sec/batch, 1493.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:04,838] [train step15270] D loss: 0.32731 G loss: 2.42744 (0.040 sec/batch, 1592.078 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:05,218] [train step15280] D loss: 0.32717 G loss: 2.44308 (0.037 sec/batch, 1750.533 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:05,610] [train step15291] D loss: 0.32897 G loss: 2.15713 (0.036 sec/batch, 1775.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:05,998] [train step15300] D loss: 0.33094 G loss: 2.60120 (0.041 sec/batch, 1553.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:06,391] [train step15311] D loss: 0.32691 G loss: 2.32775 (0.035 sec/batch, 1841.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:06,775] [train step15320] D loss: 0.32754 G loss: 2.18089 (0.042 sec/batch, 1526.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:07,165] [train step15330] D loss: 0.32836 G loss: 2.50123 (0.040 sec/batch, 1609.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:07,556] [train step15341] D loss: 0.32665 G loss: 2.37447 (0.037 sec/batch, 1727.162 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:07,932] [train step15351] D loss: 0.32670 G loss: 2.26525 (0.041 sec/batch, 1547.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:08,317] [train step15360] D loss: 0.32708 G loss: 2.39713 (0.037 sec/batch, 1728.151 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:08,704] [train step15370] D loss: 0.32733 G loss: 2.22058 (0.035 sec/batch, 1808.913 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:09,087] [train step15381] D loss: 0.32884 G loss: 2.10580 (0.036 sec/batch, 1772.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:09,477] [train step15390] D loss: 0.33238 G loss: 2.64227 (0.036 sec/batch, 1800.143 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:09,865] [train step15400] D loss: 0.32645 G loss: 2.36465 (0.034 sec/batch, 1861.162 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:10,258] [train step15411] D loss: 0.32798 G loss: 2.19691 (0.039 sec/batch, 1627.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:10,659] [train step15420] D loss: 0.32802 G loss: 2.47382 (0.037 sec/batch, 1738.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:11,036] [train step15430] D loss: 0.32747 G loss: 2.42690 (0.032 sec/batch, 1999.772 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:11,436] [train step15441] D loss: 0.32646 G loss: 2.38948 (0.049 sec/batch, 1315.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:11,822] [train step15450] D loss: 0.32643 G loss: 2.28767 (0.040 sec/batch, 1603.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:12,205] [train step15460] D loss: 0.32697 G loss: 2.20271 (0.037 sec/batch, 1707.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:12,609] [train step15470] D loss: 0.32660 G loss: 2.27818 (0.042 sec/batch, 1517.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:13,003] [train step15480] D loss: 0.32689 G loss: 2.29789 (0.039 sec/batch, 1648.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:13,395] [train step15491] D loss: 0.32687 G loss: 2.22881 (0.036 sec/batch, 1755.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:13,784] [train step15501] D loss: 0.32667 G loss: 2.29132 (0.038 sec/batch, 1669.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:14,171] [train step15510] D loss: 0.32639 G loss: 2.25000 (0.035 sec/batch, 1811.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:14,561] [train step15521] D loss: 0.32684 G loss: 2.32896 (0.051 sec/batch, 1243.857 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:14,942] [train step15530] D loss: 0.32799 G loss: 2.15116 (0.038 sec/batch, 1690.080 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:15,326] [train step15540] D loss: 0.32816 G loss: 2.48498 (0.043 sec/batch, 1478.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:15,732] [train step15551] D loss: 0.32728 G loss: 2.36595 (0.037 sec/batch, 1712.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:16,117] [train step15561] D loss: 0.32637 G loss: 2.34489 (0.042 sec/batch, 1540.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:16,501] [train step15570] D loss: 0.32742 G loss: 2.43819 (0.035 sec/batch, 1836.699 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:16,894] [train step15580] D loss: 0.32940 G loss: 2.55751 (0.034 sec/batch, 1881.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:17,281] [train step15591] D loss: 0.32641 G loss: 2.26759 (0.038 sec/batch, 1666.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:17,673] [train step15600] D loss: 0.32802 G loss: 2.49329 (0.036 sec/batch, 1800.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:18,042] [train step15610] D loss: 0.32645 G loss: 2.36428 (0.032 sec/batch, 1982.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:18,422] [train step15621] D loss: 0.32707 G loss: 2.23083 (0.036 sec/batch, 1765.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:18,816] [train step15630] D loss: 0.32676 G loss: 2.38806 (0.036 sec/batch, 1780.985 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:19,189] [train step15640] D loss: 0.32692 G loss: 2.23048 (0.041 sec/batch, 1552.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:19,559] [train step15650] D loss: 0.32690 G loss: 2.38205 (0.033 sec/batch, 1930.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:19,946] [train step15660] D loss: 0.32747 G loss: 2.17237 (0.036 sec/batch, 1790.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:20,324] [train step15670] D loss: 0.32712 G loss: 2.22342 (0.042 sec/batch, 1525.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:20,708] [train step15681] D loss: 0.32912 G loss: 2.12595 (0.034 sec/batch, 1896.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:21,100] [train step15690] D loss: 0.32987 G loss: 2.56832 (0.038 sec/batch, 1701.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:21,495] [train step15700] D loss: 0.32821 G loss: 2.49910 (0.039 sec/batch, 1627.888 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:21,880] [train step15710] D loss: 0.32633 G loss: 2.31204 (0.039 sec/batch, 1624.735 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:22,264] [train step15720] D loss: 0.32659 G loss: 2.32398 (0.041 sec/batch, 1543.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:22,639] [train step15730] D loss: 0.32681 G loss: 2.22752 (0.037 sec/batch, 1723.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:23,020] [train step15741] D loss: 0.32679 G loss: 2.28420 (0.037 sec/batch, 1728.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:23,398] [train step15750] D loss: 0.32690 G loss: 2.28676 (0.034 sec/batch, 1887.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:23,788] [train step15760] D loss: 0.32889 G loss: 2.51399 (0.044 sec/batch, 1454.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:24,177] [train step15771] D loss: 0.33721 G loss: 2.77546 (0.039 sec/batch, 1637.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:24,547] [train step15780] D loss: 0.34017 G loss: 1.89713 (0.037 sec/batch, 1742.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:24,947] [train step15791] D loss: 0.33229 G loss: 2.01746 (0.038 sec/batch, 1697.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:25,325] [train step15800] D loss: 0.32884 G loss: 2.29044 (0.037 sec/batch, 1706.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:25,696] [train step15810] D loss: 0.32783 G loss: 2.27411 (0.036 sec/batch, 1798.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:26,090] [train step15820] D loss: 0.32911 G loss: 2.13195 (0.036 sec/batch, 1765.709 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:26,468] [train step15831] D loss: 0.33077 G loss: 2.08517 (0.041 sec/batch, 1550.915 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:26,847] [train step15840] D loss: 0.32918 G loss: 2.53370 (0.039 sec/batch, 1626.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:27,224] [train step15850] D loss: 0.32723 G loss: 2.33739 (0.032 sec/batch, 1994.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:27,614] [train step15861] D loss: 0.32716 G loss: 2.27302 (0.034 sec/batch, 1860.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:27,997] [train step15870] D loss: 0.32767 G loss: 2.30148 (0.038 sec/batch, 1699.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:28,376] [train step15881] D loss: 0.32697 G loss: 2.31304 (0.046 sec/batch, 1403.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:28,754] [train step15890] D loss: 0.32749 G loss: 2.41189 (0.034 sec/batch, 1856.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:29,153] [train step15900] D loss: 0.32753 G loss: 2.23076 (0.037 sec/batch, 1734.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:29,533] [train step15911] D loss: 0.32861 G loss: 2.17301 (0.033 sec/batch, 1965.423 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:29,914] [train step15920] D loss: 0.33234 G loss: 2.02696 (0.041 sec/batch, 1551.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:30,300] [train step15930] D loss: 0.33339 G loss: 2.67884 (0.035 sec/batch, 1812.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:30,680] [train step15941] D loss: 0.33056 G loss: 2.57441 (0.045 sec/batch, 1412.521 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:31,078] [train step15951] D loss: 0.33342 G loss: 2.68073 (0.039 sec/batch, 1642.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:31,465] [train step15960] D loss: 0.32918 G loss: 2.10783 (0.040 sec/batch, 1614.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:31,860] [train step15971] D loss: 0.32718 G loss: 2.23981 (0.039 sec/batch, 1645.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:32,257] [train step15981] D loss: 0.32716 G loss: 2.38675 (0.038 sec/batch, 1666.856 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:32,632] [train step15990] D loss: 0.32820 G loss: 2.15752 (0.036 sec/batch, 1763.609 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:33,028] [train step16000] D loss: 0.33088 G loss: 2.08120 (0.046 sec/batch, 1398.138 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:19:33,028] Saved checkpoint at 16000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:33,613] [train step16010] D loss: 0.32798 G loss: 2.14160 (0.037 sec/batch, 1720.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:33,997] [train step16020] D loss: 0.32662 G loss: 2.27961 (0.042 sec/batch, 1524.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:34,386] [train step16031] D loss: 0.32769 G loss: 2.18516 (0.034 sec/batch, 1899.419 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:34,765] [train step16041] D loss: 0.32675 G loss: 2.34103 (0.040 sec/batch, 1608.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:35,151] [train step16050] D loss: 0.32632 G loss: 2.34956 (0.039 sec/batch, 1636.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:35,533] [train step16061] D loss: 0.32738 G loss: 2.37472 (0.037 sec/batch, 1743.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:35,914] [train step16071] D loss: 0.33048 G loss: 2.57705 (0.038 sec/batch, 1684.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:36,307] [train step16080] D loss: 0.33091 G loss: 2.04846 (0.037 sec/batch, 1750.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:36,679] [train step16091] D loss: 0.32916 G loss: 2.10790 (0.036 sec/batch, 1761.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:37,078] [train step16100] D loss: 0.32673 G loss: 2.26432 (0.037 sec/batch, 1747.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:37,469] [train step16110] D loss: 0.32752 G loss: 2.43259 (0.037 sec/batch, 1742.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:37,845] [train step16121] D loss: 0.32725 G loss: 2.43582 (0.039 sec/batch, 1662.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:38,239] [train step16131] D loss: 0.32623 G loss: 2.27354 (0.041 sec/batch, 1551.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:38,624] [train step16140] D loss: 0.32877 G loss: 2.51750 (0.042 sec/batch, 1540.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:39,007] [train step16150] D loss: 0.32763 G loss: 2.46081 (0.041 sec/batch, 1567.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:39,395] [train step16160] D loss: 0.32696 G loss: 2.38727 (0.034 sec/batch, 1908.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:39,772] [train step16170] D loss: 0.32703 G loss: 2.41752 (0.033 sec/batch, 1944.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:40,158] [train step16180] D loss: 0.32969 G loss: 2.55578 (0.041 sec/batch, 1578.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:40,555] [train step16191] D loss: 0.32741 G loss: 2.43697 (0.036 sec/batch, 1790.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:40,934] [train step16200] D loss: 0.32668 G loss: 2.32100 (0.038 sec/batch, 1671.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:41,324] [train step16210] D loss: 0.32915 G loss: 2.53883 (0.035 sec/batch, 1811.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:41,713] [train step16220] D loss: 0.33483 G loss: 2.72785 (0.041 sec/batch, 1574.817 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:42,094] [train step16230] D loss: 0.32902 G loss: 2.11656 (0.036 sec/batch, 1757.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:42,501] [train step16240] D loss: 0.32638 G loss: 2.29131 (0.038 sec/batch, 1677.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:42,879] [train step16250] D loss: 0.32711 G loss: 2.39463 (0.039 sec/batch, 1661.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:43,257] [train step16260] D loss: 0.32672 G loss: 2.22428 (0.036 sec/batch, 1771.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:43,640] [train step16270] D loss: 0.32913 G loss: 2.09603 (0.038 sec/batch, 1680.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:44,018] [train step16281] D loss: 0.32795 G loss: 2.15315 (0.033 sec/batch, 1956.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:44,410] [train step16290] D loss: 0.32706 G loss: 2.42445 (0.038 sec/batch, 1684.999 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:44,792] [train step16300] D loss: 0.32664 G loss: 2.35729 (0.037 sec/batch, 1709.824 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:45,179] [train step16311] D loss: 0.32634 G loss: 2.24243 (0.036 sec/batch, 1766.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:45,567] [train step16320] D loss: 0.32733 G loss: 2.44937 (0.037 sec/batch, 1727.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:45,943] [train step16330] D loss: 0.32716 G loss: 2.43449 (0.027 sec/batch, 2333.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:46,343] [train step16341] D loss: 0.32643 G loss: 2.33155 (0.044 sec/batch, 1455.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:46,754] [train step16350] D loss: 0.32692 G loss: 2.39824 (0.042 sec/batch, 1529.512 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:47,142] [train step16361] D loss: 0.33023 G loss: 2.58228 (0.037 sec/batch, 1745.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:47,538] [train step16371] D loss: 0.33388 G loss: 2.70666 (0.038 sec/batch, 1688.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:47,909] [train step16380] D loss: 0.32896 G loss: 2.09713 (0.038 sec/batch, 1676.799 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:48,281] [train step16391] D loss: 0.32663 G loss: 2.22180 (0.039 sec/batch, 1646.106 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:48,668] [train step16401] D loss: 0.32611 G loss: 2.29068 (0.037 sec/batch, 1710.096 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:49,040] [train step16410] D loss: 0.32648 G loss: 2.30620 (0.036 sec/batch, 1775.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:49,422] [train step16420] D loss: 0.32720 G loss: 2.17666 (0.032 sec/batch, 1998.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:49,798] [train step16430] D loss: 0.32778 G loss: 2.17597 (0.033 sec/batch, 1927.640 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:50,179] [train step16440] D loss: 0.32642 G loss: 2.37261 (0.038 sec/batch, 1690.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:50,561] [train step16450] D loss: 0.32655 G loss: 2.36667 (0.036 sec/batch, 1762.984 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:50,943] [train step16461] D loss: 0.32717 G loss: 2.43608 (0.035 sec/batch, 1813.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:51,318] [train step16470] D loss: 0.32636 G loss: 2.31968 (0.037 sec/batch, 1723.923 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:51,700] [train step16481] D loss: 0.32578 G loss: 2.34291 (0.035 sec/batch, 1838.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:52,069] [train step16491] D loss: 0.32619 G loss: 2.33812 (0.037 sec/batch, 1728.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:52,441] [train step16500] D loss: 0.32649 G loss: 2.19836 (0.037 sec/batch, 1747.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:52,827] [train step16510] D loss: 0.32679 G loss: 2.21073 (0.039 sec/batch, 1653.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:53,216] [train step16521] D loss: 0.32662 G loss: 2.21295 (0.035 sec/batch, 1811.709 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:53,599] [train step16530] D loss: 0.32684 G loss: 2.42171 (0.047 sec/batch, 1364.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:53,975] [train step16540] D loss: 0.32638 G loss: 2.33921 (0.038 sec/batch, 1691.710 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:54,367] [train step16551] D loss: 0.32603 G loss: 2.33527 (0.040 sec/batch, 1618.710 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:54,750] [train step16560] D loss: 0.32624 G loss: 2.24147 (0.035 sec/batch, 1807.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:55,123] [train step16571] D loss: 0.32685 G loss: 2.19068 (0.036 sec/batch, 1792.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:55,512] [train step16581] D loss: 0.32707 G loss: 2.19519 (0.038 sec/batch, 1678.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:55,899] [train step16590] D loss: 0.32795 G loss: 2.50282 (0.043 sec/batch, 1483.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:56,270] [train step16600] D loss: 0.32717 G loss: 2.44930 (0.035 sec/batch, 1838.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:56,651] [train step16611] D loss: 0.32627 G loss: 2.30776 (0.041 sec/batch, 1574.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:57,033] [train step16620] D loss: 0.32620 G loss: 2.31209 (0.037 sec/batch, 1735.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:57,406] [train step16630] D loss: 0.32606 G loss: 2.26404 (0.039 sec/batch, 1645.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:57,797] [train step16641] D loss: 0.32615 G loss: 2.26101 (0.038 sec/batch, 1693.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:58,177] [train step16650] D loss: 0.32648 G loss: 2.38539 (0.038 sec/batch, 1675.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:58,561] [train step16660] D loss: 0.32627 G loss: 2.36792 (0.042 sec/batch, 1516.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:58,943] [train step16670] D loss: 0.32604 G loss: 2.25714 (0.036 sec/batch, 1761.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:59,317] [train step16680] D loss: 0.32609 G loss: 2.35125 (0.038 sec/batch, 1705.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:19:59,699] [train step16691] D loss: 0.32693 G loss: 2.22134 (0.037 sec/batch, 1733.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:00,083] [train step16700] D loss: 0.32645 G loss: 2.36269 (0.031 sec/batch, 2082.784 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:00,479] [train step16710] D loss: 0.32682 G loss: 2.17561 (0.035 sec/batch, 1815.495 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:00,869] [train step16720] D loss: 0.32647 G loss: 2.32609 (0.039 sec/batch, 1632.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:01,248] [train step16730] D loss: 0.32606 G loss: 2.32513 (0.043 sec/batch, 1474.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:01,631] [train step16740] D loss: 0.32626 G loss: 2.24361 (0.037 sec/batch, 1708.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:02,025] [train step16750] D loss: 0.32629 G loss: 2.25925 (0.039 sec/batch, 1629.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:02,402] [train step16760] D loss: 0.32598 G loss: 2.28264 (0.033 sec/batch, 1922.766 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:02,787] [train step16770] D loss: 0.32739 G loss: 2.45440 (0.034 sec/batch, 1873.751 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:03,187] [train step16781] D loss: 0.32624 G loss: 2.38064 (0.036 sec/batch, 1759.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:03,569] [train step16791] D loss: 0.32613 G loss: 2.27288 (0.041 sec/batch, 1559.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:03,987] [train step16800] D loss: 0.32677 G loss: 2.38885 (0.038 sec/batch, 1698.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:04,368] [train step16811] D loss: 0.32660 G loss: 2.40164 (0.038 sec/batch, 1704.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:04,753] [train step16821] D loss: 0.32620 G loss: 2.25531 (0.038 sec/batch, 1662.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:05,146] [train step16830] D loss: 0.32610 G loss: 2.31742 (0.036 sec/batch, 1761.006 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:05,524] [train step16840] D loss: 0.32623 G loss: 2.32828 (0.040 sec/batch, 1611.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:05,919] [train step16850] D loss: 0.32624 G loss: 2.23544 (0.047 sec/batch, 1373.128 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:06,299] [train step16860] D loss: 0.32889 G loss: 2.53351 (0.036 sec/batch, 1760.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:06,675] [train step16871] D loss: 0.32778 G loss: 2.47518 (0.038 sec/batch, 1663.293 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:07,063] [train step16881] D loss: 0.32591 G loss: 2.26724 (0.039 sec/batch, 1644.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:07,438] [train step16890] D loss: 0.32590 G loss: 2.29750 (0.037 sec/batch, 1749.688 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:07,823] [train step16900] D loss: 0.32577 G loss: 2.28413 (0.037 sec/batch, 1749.153 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:08,216] [train step16910] D loss: 0.32596 G loss: 2.33339 (0.040 sec/batch, 1615.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:08,591] [train step16920] D loss: 0.32617 G loss: 2.26684 (0.035 sec/batch, 1837.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:08,978] [train step16931] D loss: 0.32593 G loss: 2.25301 (0.045 sec/batch, 1422.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:09,358] [train step16940] D loss: 0.32589 G loss: 2.35564 (0.034 sec/batch, 1863.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:09,737] [train step16950] D loss: 0.32635 G loss: 2.28550 (0.039 sec/batch, 1628.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:10,132] [train step16960] D loss: 0.32608 G loss: 2.33270 (0.040 sec/batch, 1605.312 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:10,509] [train step16971] D loss: 0.32604 G loss: 2.37078 (0.035 sec/batch, 1805.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:10,886] [train step16980] D loss: 0.32979 G loss: 2.08062 (0.038 sec/batch, 1681.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:11,287] [train step16991] D loss: 0.32641 G loss: 2.37247 (0.038 sec/batch, 1675.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:11,671] [train step17001] D loss: 0.32701 G loss: 2.42485 (0.038 sec/batch, 1674.206 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:20:11,671] Saved checkpoint at 17000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:12,259] [train step17010] D loss: 0.32624 G loss: 2.21332 (0.035 sec/batch, 1805.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:12,637] [train step17020] D loss: 0.32641 G loss: 2.38991 (0.032 sec/batch, 1972.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:13,023] [train step17031] D loss: 0.32608 G loss: 2.33708 (0.040 sec/batch, 1609.374 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:13,415] [train step17040] D loss: 0.32647 G loss: 2.20923 (0.037 sec/batch, 1737.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:13,802] [train step17050] D loss: 0.32604 G loss: 2.34417 (0.038 sec/batch, 1697.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:14,211] [train step17060] D loss: 0.32620 G loss: 2.26892 (0.036 sec/batch, 1762.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:14,595] [train step17070] D loss: 0.32718 G loss: 2.44401 (0.039 sec/batch, 1646.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:14,984] [train step17080] D loss: 0.32579 G loss: 2.30563 (0.041 sec/batch, 1561.634 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:15,373] [train step17091] D loss: 0.32629 G loss: 2.31891 (0.037 sec/batch, 1714.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:15,753] [train step17100] D loss: 0.32582 G loss: 2.29978 (0.036 sec/batch, 1789.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:16,136] [train step17110] D loss: 0.32656 G loss: 2.35867 (0.043 sec/batch, 1504.666 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:16,533] [train step17121] D loss: 0.32709 G loss: 2.42098 (0.040 sec/batch, 1606.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:16,914] [train step17130] D loss: 0.32583 G loss: 2.30916 (0.040 sec/batch, 1598.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:17,319] [train step17140] D loss: 0.32626 G loss: 2.30330 (0.047 sec/batch, 1347.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:17,702] [train step17150] D loss: 0.32593 G loss: 2.35294 (0.036 sec/batch, 1802.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:18,086] [train step17160] D loss: 0.32623 G loss: 2.24847 (0.038 sec/batch, 1681.463 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:18,470] [train step17170] D loss: 0.32600 G loss: 2.33801 (0.039 sec/batch, 1632.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:18,842] [train step17180] D loss: 0.32638 G loss: 2.32411 (0.040 sec/batch, 1616.156 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:19,211] [train step17190] D loss: 0.32604 G loss: 2.29878 (0.033 sec/batch, 1938.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:19,605] [train step17201] D loss: 0.32587 G loss: 2.29021 (0.036 sec/batch, 1754.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:19,983] [train step17210] D loss: 0.32653 G loss: 2.39144 (0.039 sec/batch, 1621.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:20,363] [train step17220] D loss: 0.32679 G loss: 2.17109 (0.036 sec/batch, 1775.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:20,739] [train step17231] D loss: 0.32648 G loss: 2.24273 (0.040 sec/batch, 1594.347 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:21,108] [train step17241] D loss: 0.32724 G loss: 2.42280 (0.036 sec/batch, 1791.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:21,488] [train step17250] D loss: 0.32610 G loss: 2.27555 (0.039 sec/batch, 1649.089 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:21,875] [train step17260] D loss: 0.32582 G loss: 2.32324 (0.042 sec/batch, 1515.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:22,255] [train step17271] D loss: 0.32646 G loss: 2.33620 (0.037 sec/batch, 1721.800 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:22,635] [train step17280] D loss: 0.32626 G loss: 2.26902 (0.041 sec/batch, 1546.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:23,008] [train step17290] D loss: 0.32715 G loss: 2.44934 (0.040 sec/batch, 1590.323 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:23,401] [train step17301] D loss: 0.32819 G loss: 2.45074 (0.046 sec/batch, 1379.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:23,775] [train step17310] D loss: 0.32668 G loss: 2.20552 (0.036 sec/batch, 1786.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:24,145] [train step17321] D loss: 0.32624 G loss: 2.29487 (0.036 sec/batch, 1789.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:24,540] [train step17331] D loss: 0.32762 G loss: 2.46761 (0.037 sec/batch, 1748.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:24,929] [train step17340] D loss: 0.32692 G loss: 2.41353 (0.040 sec/batch, 1580.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:25,303] [train step17351] D loss: 0.32763 G loss: 2.33643 (0.036 sec/batch, 1801.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:25,687] [train step17360] D loss: 0.32839 G loss: 2.22944 (0.039 sec/batch, 1628.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:26,061] [train step17370] D loss: 0.35307 G loss: 3.02835 (0.040 sec/batch, 1610.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:26,437] [train step17381] D loss: 0.35896 G loss: 3.19453 (0.037 sec/batch, 1743.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:26,831] [train step17391] D loss: 0.33154 G loss: 2.20574 (0.037 sec/batch, 1711.950 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:27,216] [train step17400] D loss: 0.33797 G loss: 2.80064 (0.041 sec/batch, 1576.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:27,630] [train step17410] D loss: 0.33369 G loss: 2.47559 (0.041 sec/batch, 1562.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:28,086] [train step17421] D loss: 0.32773 G loss: 2.22431 (0.037 sec/batch, 1732.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:28,472] [train step17430] D loss: 0.32917 G loss: 2.39923 (0.036 sec/batch, 1796.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:28,869] [train step17441] D loss: 0.32768 G loss: 2.34811 (0.041 sec/batch, 1572.806 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:29,255] [train step17450] D loss: 0.32753 G loss: 2.23661 (0.037 sec/batch, 1746.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:29,648] [train step17460] D loss: 0.33470 G loss: 2.66613 (0.035 sec/batch, 1805.215 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:30,031] [train step17470] D loss: 0.32721 G loss: 2.20312 (0.039 sec/batch, 1638.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:30,421] [train step17481] D loss: 0.34494 G loss: 1.96323 (0.042 sec/batch, 1508.395 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:30,823] [train step17490] D loss: 0.33019 G loss: 2.28268 (0.041 sec/batch, 1548.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:31,215] [train step17501] D loss: 0.32935 G loss: 2.31990 (0.041 sec/batch, 1566.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:31,602] [train step17510] D loss: 0.33315 G loss: 2.33821 (0.040 sec/batch, 1613.349 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:32,001] [train step17520] D loss: 0.33065 G loss: 2.39900 (0.042 sec/batch, 1535.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:32,388] [train step17531] D loss: 0.33469 G loss: 2.44936 (0.039 sec/batch, 1643.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:32,785] [train step17541] D loss: 0.32689 G loss: 2.28007 (0.036 sec/batch, 1789.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:33,167] [train step17550] D loss: 0.32753 G loss: 2.21820 (0.038 sec/batch, 1665.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:33,569] [train step17560] D loss: 0.32946 G loss: 2.45522 (0.044 sec/batch, 1457.035 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:33,976] [train step17570] D loss: 0.32607 G loss: 2.33060 (0.041 sec/batch, 1548.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:34,355] [train step17580] D loss: 0.38980 G loss: 1.63528 (0.028 sec/batch, 2250.578 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:34,762] [train step17590] D loss: 0.37644 G loss: 3.35680 (0.039 sec/batch, 1635.794 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:35,161] [train step17600] D loss: 0.68084 G loss: 6.79318 (0.039 sec/batch, 1637.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:35,545] [train step17610] D loss: 0.52351 G loss: 1.02735 (0.040 sec/batch, 1586.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:35,938] [train step17620] D loss: 0.63637 G loss: 6.06113 (0.027 sec/batch, 2394.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:36,337] [train step17631] D loss: 1.00241 G loss: 10.02346 (0.039 sec/batch, 1623.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:36,722] [train step17640] D loss: 0.59322 G loss: 1.27318 (0.039 sec/batch, 1652.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:37,108] [train step17651] D loss: 0.79682 G loss: 7.96382 (0.039 sec/batch, 1654.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:37,498] [train step17660] D loss: 0.59957 G loss: 0.94187 (0.036 sec/batch, 1784.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:37,896] [train step17670] D loss: 1.27049 G loss: 12.70487 (0.037 sec/batch, 1749.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:38,283] [train step17681] D loss: 1.15776 G loss: 11.57738 (0.036 sec/batch, 1777.871 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:38,668] [train step17690] D loss: 0.70676 G loss: 0.81431 (0.040 sec/batch, 1613.184 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:39,062] [train step17700] D loss: 0.94700 G loss: 9.46907 (0.037 sec/batch, 1730.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:39,445] [train step17711] D loss: 0.93542 G loss: 9.35307 (0.040 sec/batch, 1590.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:39,853] [train step17720] D loss: 0.34895 G loss: 2.05774 (0.050 sec/batch, 1284.257 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:40,241] [train step17730] D loss: 0.37045 G loss: 1.67489 (0.042 sec/batch, 1507.074 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:40,613] [train step17740] D loss: 0.35900 G loss: 3.18810 (0.038 sec/batch, 1699.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:41,007] [train step17750] D loss: 0.36345 G loss: 3.26305 (0.041 sec/batch, 1549.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:41,396] [train step17760] D loss: 0.34138 G loss: 2.80775 (0.041 sec/batch, 1549.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:41,773] [train step17771] D loss: 0.35209 G loss: 3.05383 (0.036 sec/batch, 1776.577 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:42,164] [train step17781] D loss: 0.34427 G loss: 1.90191 (0.040 sec/batch, 1601.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:42,545] [train step17790] D loss: 0.33080 G loss: 2.50207 (0.035 sec/batch, 1812.284 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:42,934] [train step17800] D loss: 0.33593 G loss: 2.08443 (0.040 sec/batch, 1597.745 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:43,318] [train step17810] D loss: 0.33059 G loss: 2.34651 (0.029 sec/batch, 2222.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:43,711] [train step17820] D loss: 0.33061 G loss: 2.29204 (0.040 sec/batch, 1594.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:44,109] [train step17830] D loss: 0.33225 G loss: 2.45897 (0.034 sec/batch, 1886.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:44,502] [train step17840] D loss: 0.33091 G loss: 2.31578 (0.034 sec/batch, 1883.374 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:44,884] [train step17850] D loss: 0.33156 G loss: 2.37421 (0.038 sec/batch, 1682.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:45,273] [train step17861] D loss: 0.33399 G loss: 2.55355 (0.035 sec/batch, 1807.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:45,659] [train step17870] D loss: 0.33102 G loss: 2.31505 (0.043 sec/batch, 1472.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:46,060] [train step17880] D loss: 0.33014 G loss: 2.39057 (0.038 sec/batch, 1668.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:46,447] [train step17890] D loss: 0.32994 G loss: 2.33031 (0.043 sec/batch, 1481.882 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:46,816] [train step17900] D loss: 0.33171 G loss: 2.35001 (0.028 sec/batch, 2301.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:47,222] [train step17910] D loss: 0.33039 G loss: 2.25980 (0.036 sec/batch, 1790.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:47,616] [train step17921] D loss: 0.33185 G loss: 2.47089 (0.039 sec/batch, 1620.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:48,011] [train step17930] D loss: 0.33258 G loss: 2.59136 (0.048 sec/batch, 1345.797 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:48,409] [train step17940] D loss: 0.33137 G loss: 2.40874 (0.035 sec/batch, 1839.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:48,784] [train step17951] D loss: 0.33174 G loss: 2.51711 (0.039 sec/batch, 1643.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:49,163] [train step17960] D loss: 0.33348 G loss: 2.58159 (0.036 sec/batch, 1773.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:49,550] [train step17970] D loss: 0.32968 G loss: 2.26420 (0.032 sec/batch, 2007.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:49,931] [train step17981] D loss: 0.33404 G loss: 2.65851 (0.038 sec/batch, 1705.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:50,330] [train step17990] D loss: 0.32971 G loss: 2.34260 (0.037 sec/batch, 1721.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:50,706] [train step18000] D loss: 0.33082 G loss: 2.45429 (0.037 sec/batch, 1740.827 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:20:50,707] Saved checkpoint at 18000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:51,299] [train step18010] D loss: 0.32948 G loss: 2.29433 (0.040 sec/batch, 1585.645 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:51,666] [train step18021] D loss: 0.33113 G loss: 2.19705 (0.040 sec/batch, 1585.252 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:52,044] [train step18030] D loss: 0.33081 G loss: 2.49515 (0.037 sec/batch, 1714.224 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:52,435] [train step18040] D loss: 0.33671 G loss: 2.72775 (0.043 sec/batch, 1486.337 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:52,809] [train step18051] D loss: 0.32955 G loss: 2.44983 (0.037 sec/batch, 1714.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:53,190] [train step18060] D loss: 0.33051 G loss: 2.25425 (0.037 sec/batch, 1742.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:53,574] [train step18071] D loss: 0.32955 G loss: 2.29135 (0.041 sec/batch, 1578.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:53,947] [train step18080] D loss: 0.32960 G loss: 2.37104 (0.037 sec/batch, 1752.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:54,337] [train step18090] D loss: 0.33102 G loss: 2.42432 (0.041 sec/batch, 1579.813 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:54,735] [train step18100] D loss: 0.33033 G loss: 2.44217 (0.040 sec/batch, 1618.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:55,116] [train step18111] D loss: 0.32988 G loss: 2.29990 (0.040 sec/batch, 1609.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:55,505] [train step18120] D loss: 0.33142 G loss: 2.43391 (0.037 sec/batch, 1731.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:55,880] [train step18130] D loss: 0.33057 G loss: 2.19132 (0.037 sec/batch, 1736.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:56,270] [train step18141] D loss: 0.33305 G loss: 2.60616 (0.045 sec/batch, 1433.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:56,664] [train step18150] D loss: 0.33107 G loss: 2.33982 (0.035 sec/batch, 1814.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:57,047] [train step18161] D loss: 0.33463 G loss: 2.66378 (0.035 sec/batch, 1831.711 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:57,435] [train step18171] D loss: 0.32913 G loss: 2.25556 (0.032 sec/batch, 1973.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:57,821] [train step18180] D loss: 0.33607 G loss: 2.72163 (0.037 sec/batch, 1740.161 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:58,193] [train step18191] D loss: 0.33373 G loss: 2.62219 (0.036 sec/batch, 1764.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:58,597] [train step18200] D loss: 0.32982 G loss: 2.45425 (0.031 sec/batch, 2037.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:58,982] [train step18210] D loss: 0.33222 G loss: 2.13469 (0.036 sec/batch, 1766.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:59,374] [train step18220] D loss: 0.32950 G loss: 2.32285 (0.044 sec/batch, 1441.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:20:59,754] [train step18230] D loss: 0.33072 G loss: 2.53807 (0.040 sec/batch, 1595.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:00,141] [train step18240] D loss: 0.33102 G loss: 2.20231 (0.038 sec/batch, 1688.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:00,533] [train step18251] D loss: 0.32936 G loss: 2.39532 (0.034 sec/batch, 1872.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:00,910] [train step18261] D loss: 0.33159 G loss: 2.55795 (0.036 sec/batch, 1769.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:01,290] [train step18270] D loss: 0.33344 G loss: 2.09399 (0.036 sec/batch, 1791.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:01,692] [train step18280] D loss: 0.32903 G loss: 2.24921 (0.037 sec/batch, 1735.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:02,077] [train step18291] D loss: 0.32964 G loss: 2.45914 (0.035 sec/batch, 1843.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:02,484] [train step18300] D loss: 0.32963 G loss: 2.25684 (0.042 sec/batch, 1514.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:02,873] [train step18311] D loss: 0.32971 G loss: 2.24009 (0.041 sec/batch, 1567.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:03,262] [train step18321] D loss: 0.32893 G loss: 2.24944 (0.042 sec/batch, 1511.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:03,662] [train step18330] D loss: 0.32902 G loss: 2.34903 (0.037 sec/batch, 1734.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:04,049] [train step18341] D loss: 0.33130 G loss: 2.22646 (0.038 sec/batch, 1677.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:04,443] [train step18350] D loss: 0.33057 G loss: 2.15216 (0.038 sec/batch, 1685.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:04,842] [train step18360] D loss: 0.33058 G loss: 2.43041 (0.040 sec/batch, 1596.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:05,234] [train step18371] D loss: 0.32908 G loss: 2.21432 (0.040 sec/batch, 1617.832 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:05,640] [train step18380] D loss: 0.32767 G loss: 2.29020 (0.043 sec/batch, 1502.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:06,028] [train step18390] D loss: 0.32914 G loss: 2.24934 (0.038 sec/batch, 1683.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:06,412] [train step18401] D loss: 0.33052 G loss: 2.14884 (0.039 sec/batch, 1646.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:06,830] [train step18411] D loss: 0.32824 G loss: 2.33984 (0.039 sec/batch, 1623.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:07,218] [train step18420] D loss: 0.32831 G loss: 2.34273 (0.038 sec/batch, 1680.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:07,606] [train step18431] D loss: 0.32894 G loss: 2.44775 (0.043 sec/batch, 1487.268 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:07,997] [train step18441] D loss: 0.32867 G loss: 2.31230 (0.037 sec/batch, 1751.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:08,382] [train step18450] D loss: 0.32749 G loss: 2.30972 (0.035 sec/batch, 1851.981 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:08,781] [train step18460] D loss: 0.32852 G loss: 2.29469 (0.043 sec/batch, 1491.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:09,192] [train step18471] D loss: 0.32784 G loss: 2.27062 (0.039 sec/batch, 1622.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:09,583] [train step18480] D loss: 0.32899 G loss: 2.45020 (0.041 sec/batch, 1561.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:09,977] [train step18491] D loss: 0.32849 G loss: 2.45704 (0.035 sec/batch, 1809.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:10,372] [train step18500] D loss: 0.32844 G loss: 2.29550 (0.042 sec/batch, 1521.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:10,755] [train step18510] D loss: 0.32929 G loss: 2.41663 (0.035 sec/batch, 1815.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:11,145] [train step18521] D loss: 0.32920 G loss: 2.37649 (0.036 sec/batch, 1754.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:11,524] [train step18530] D loss: 0.32932 G loss: 2.20498 (0.039 sec/batch, 1647.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:11,918] [train step18540] D loss: 0.32724 G loss: 2.35768 (0.035 sec/batch, 1844.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:12,304] [train step18551] D loss: 0.32902 G loss: 2.19367 (0.035 sec/batch, 1810.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:12,688] [train step18560] D loss: 0.33060 G loss: 2.13684 (0.037 sec/batch, 1741.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:13,075] [train step18570] D loss: 0.32822 G loss: 2.43431 (0.036 sec/batch, 1781.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:13,464] [train step18581] D loss: 0.32984 G loss: 2.18014 (0.036 sec/batch, 1781.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:13,850] [train step18591] D loss: 0.32860 G loss: 2.45271 (0.037 sec/batch, 1712.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:14,242] [train step18600] D loss: 0.32901 G loss: 2.18895 (0.037 sec/batch, 1730.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:14,617] [train step18611] D loss: 0.33116 G loss: 2.12640 (0.035 sec/batch, 1843.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:15,010] [train step18620] D loss: 0.32897 G loss: 2.17762 (0.037 sec/batch, 1724.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:15,404] [train step18630] D loss: 0.32797 G loss: 2.41610 (0.037 sec/batch, 1718.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:15,800] [train step18641] D loss: 0.32789 G loss: 2.39942 (0.041 sec/batch, 1572.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:16,188] [train step18651] D loss: 0.32801 G loss: 2.33701 (0.038 sec/batch, 1688.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:16,570] [train step18660] D loss: 0.32805 G loss: 2.25557 (0.035 sec/batch, 1811.269 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:16,960] [train step18670] D loss: 0.32837 G loss: 2.25239 (0.039 sec/batch, 1626.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:17,353] [train step18681] D loss: 0.32805 G loss: 2.34954 (0.037 sec/batch, 1723.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:17,743] [train step18690] D loss: 0.32801 G loss: 2.31840 (0.042 sec/batch, 1541.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:18,138] [train step18700] D loss: 0.32890 G loss: 2.48223 (0.039 sec/batch, 1646.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:18,525] [train step18711] D loss: 0.32759 G loss: 2.27495 (0.043 sec/batch, 1481.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:18,917] [train step18720] D loss: 0.33076 G loss: 2.55899 (0.039 sec/batch, 1653.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:19,298] [train step18731] D loss: 0.32778 G loss: 2.33569 (0.036 sec/batch, 1792.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:19,682] [train step18741] D loss: 0.33134 G loss: 2.08341 (0.046 sec/batch, 1390.137 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:20,069] [train step18750] D loss: 0.33226 G loss: 2.60346 (0.038 sec/batch, 1669.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:20,462] [train step18761] D loss: 0.32801 G loss: 2.29077 (0.045 sec/batch, 1436.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:20,852] [train step18771] D loss: 0.32751 G loss: 2.33952 (0.039 sec/batch, 1644.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:21,260] [train step18780] D loss: 0.32931 G loss: 2.22000 (0.041 sec/batch, 1578.030 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:21,637] [train step18790] D loss: 0.32792 G loss: 2.21172 (0.041 sec/batch, 1577.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:22,027] [train step18801] D loss: 0.33022 G loss: 2.49752 (0.036 sec/batch, 1774.874 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:22,424] [train step18810] D loss: 0.33320 G loss: 2.03723 (0.038 sec/batch, 1698.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:22,816] [train step18820] D loss: 0.33298 G loss: 2.04370 (0.041 sec/batch, 1579.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:23,221] [train step18831] D loss: 0.32982 G loss: 2.52592 (0.039 sec/batch, 1646.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:23,611] [train step18840] D loss: 0.33618 G loss: 1.95536 (0.042 sec/batch, 1515.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:24,020] [train step18851] D loss: 0.32914 G loss: 2.19101 (0.044 sec/batch, 1463.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:24,425] [train step18860] D loss: 0.33317 G loss: 2.65628 (0.045 sec/batch, 1435.100 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:24,815] [train step18870] D loss: 0.32962 G loss: 2.13850 (0.041 sec/batch, 1573.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:25,204] [train step18880] D loss: 0.32743 G loss: 2.33596 (0.035 sec/batch, 1816.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:25,612] [train step18891] D loss: 0.32704 G loss: 2.31099 (0.041 sec/batch, 1575.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:25,989] [train step18900] D loss: 0.32793 G loss: 2.20830 (0.040 sec/batch, 1605.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:26,386] [train step18910] D loss: 0.32747 G loss: 2.23340 (0.039 sec/batch, 1622.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:26,772] [train step18920] D loss: 0.32698 G loss: 2.34383 (0.038 sec/batch, 1683.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:27,164] [train step18930] D loss: 0.32780 G loss: 2.22139 (0.042 sec/batch, 1534.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:27,555] [train step18940] D loss: 0.33114 G loss: 2.08686 (0.040 sec/batch, 1618.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:27,946] [train step18951] D loss: 0.32917 G loss: 2.14896 (0.038 sec/batch, 1668.068 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:28,348] [train step18960] D loss: 0.33002 G loss: 2.54953 (0.039 sec/batch, 1623.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:28,741] [train step18970] D loss: 0.32695 G loss: 2.30029 (0.038 sec/batch, 1694.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:29,128] [train step18981] D loss: 0.33116 G loss: 2.06526 (0.033 sec/batch, 1963.166 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:29,520] [train step18990] D loss: 0.33791 G loss: 2.79098 (0.035 sec/batch, 1826.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:29,924] [train step19000] D loss: 0.32765 G loss: 2.30204 (0.043 sec/batch, 1478.380 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:21:29,925] Saved checkpoint at 19000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:30,545] [train step19011] D loss: 0.33026 G loss: 2.08728 (0.039 sec/batch, 1629.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:30,953] [train step19020] D loss: 0.33207 G loss: 2.61720 (0.043 sec/batch, 1492.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:31,360] [train step19031] D loss: 0.32777 G loss: 2.29636 (0.038 sec/batch, 1666.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:31,746] [train step19041] D loss: 0.32835 G loss: 2.13103 (0.041 sec/batch, 1549.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:32,138] [train step19050] D loss: 0.32810 G loss: 2.39038 (0.038 sec/batch, 1683.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:32,534] [train step19061] D loss: 0.32755 G loss: 2.30865 (0.041 sec/batch, 1557.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:32,926] [train step19070] D loss: 0.32835 G loss: 2.32911 (0.032 sec/batch, 2025.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:33,329] [train step19080] D loss: 0.32730 G loss: 2.35977 (0.043 sec/batch, 1493.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:33,718] [train step19091] D loss: 0.32761 G loss: 2.27885 (0.041 sec/batch, 1545.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:34,114] [train step19100] D loss: 0.32725 G loss: 2.17496 (0.038 sec/batch, 1675.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:34,505] [train step19110] D loss: 0.32862 G loss: 2.46628 (0.035 sec/batch, 1808.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:34,903] [train step19121] D loss: 0.32740 G loss: 2.21888 (0.036 sec/batch, 1793.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:35,296] [train step19130] D loss: 0.32844 G loss: 2.36652 (0.039 sec/batch, 1656.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:35,718] [train step19140] D loss: 0.32714 G loss: 2.27886 (0.040 sec/batch, 1588.225 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:36,105] [train step19150] D loss: 0.32696 G loss: 2.23895 (0.039 sec/batch, 1628.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:36,502] [train step19160] D loss: 0.32971 G loss: 2.12146 (0.043 sec/batch, 1483.208 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:36,888] [train step19170] D loss: 0.32950 G loss: 2.53099 (0.042 sec/batch, 1511.172 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:37,267] [train step19181] D loss: 0.32817 G loss: 2.18259 (0.036 sec/batch, 1791.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:37,656] [train step19191] D loss: 0.32782 G loss: 2.21149 (0.034 sec/batch, 1900.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:38,035] [train step19200] D loss: 0.32687 G loss: 2.34763 (0.041 sec/batch, 1564.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:38,422] [train step19211] D loss: 0.32700 G loss: 2.29225 (0.043 sec/batch, 1489.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:38,808] [train step19220] D loss: 0.32782 G loss: 2.30274 (0.037 sec/batch, 1712.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:39,183] [train step19230] D loss: 0.32673 G loss: 2.36032 (0.038 sec/batch, 1679.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:39,582] [train step19241] D loss: 0.32960 G loss: 2.53376 (0.033 sec/batch, 1932.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:39,969] [train step19250] D loss: 0.32792 G loss: 2.34663 (0.038 sec/batch, 1664.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:40,358] [train step19260] D loss: 0.32772 G loss: 2.43340 (0.039 sec/batch, 1623.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:40,743] [train step19270] D loss: 0.32972 G loss: 2.54960 (0.030 sec/batch, 2112.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:41,155] [train step19281] D loss: 0.32882 G loss: 2.17581 (0.038 sec/batch, 1703.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:41,543] [train step19290] D loss: 0.32979 G loss: 2.55404 (0.039 sec/batch, 1656.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:41,920] [train step19301] D loss: 0.32711 G loss: 2.31047 (0.037 sec/batch, 1714.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:42,308] [train step19310] D loss: 0.32802 G loss: 2.24576 (0.038 sec/batch, 1690.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:42,707] [train step19320] D loss: 0.32705 G loss: 2.37260 (0.040 sec/batch, 1606.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:43,090] [train step19331] D loss: 0.32702 G loss: 2.31917 (0.037 sec/batch, 1741.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:43,476] [train step19341] D loss: 0.32751 G loss: 2.40451 (0.037 sec/batch, 1740.838 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:43,864] [train step19350] D loss: 0.32785 G loss: 2.28558 (0.040 sec/batch, 1618.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:44,253] [train step19360] D loss: 0.32736 G loss: 2.26326 (0.043 sec/batch, 1485.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:44,645] [train step19371] D loss: 0.32736 G loss: 2.35466 (0.038 sec/batch, 1672.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:45,030] [train step19380] D loss: 0.32711 G loss: 2.36672 (0.037 sec/batch, 1735.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:45,414] [train step19391] D loss: 0.32824 G loss: 2.46780 (0.037 sec/batch, 1713.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:45,806] [train step19401] D loss: 0.32986 G loss: 2.55915 (0.036 sec/batch, 1789.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:46,187] [train step19410] D loss: 0.32950 G loss: 2.12949 (0.037 sec/batch, 1726.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:46,568] [train step19420] D loss: 0.32786 G loss: 2.19898 (0.034 sec/batch, 1859.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:46,960] [train step19431] D loss: 0.32714 G loss: 2.23063 (0.036 sec/batch, 1773.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:47,339] [train step19440] D loss: 0.32703 G loss: 2.39089 (0.035 sec/batch, 1810.035 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:47,726] [train step19450] D loss: 0.32767 G loss: 2.18641 (0.045 sec/batch, 1426.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:48,105] [train step19461] D loss: 0.32758 G loss: 2.23876 (0.040 sec/batch, 1600.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:48,483] [train step19470] D loss: 0.32642 G loss: 2.33917 (0.038 sec/batch, 1674.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:48,876] [train step19481] D loss: 0.32703 G loss: 2.32580 (0.045 sec/batch, 1413.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:49,257] [train step19490] D loss: 0.32873 G loss: 2.50795 (0.039 sec/batch, 1638.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:49,643] [train step19500] D loss: 0.32652 G loss: 2.25392 (0.037 sec/batch, 1707.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:50,027] [train step19511] D loss: 0.32673 G loss: 2.29545 (0.037 sec/batch, 1734.102 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:50,399] [train step19521] D loss: 0.32693 G loss: 2.37920 (0.037 sec/batch, 1719.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:50,780] [train step19530] D loss: 0.32709 G loss: 2.26745 (0.041 sec/batch, 1567.351 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:51,154] [train step19540] D loss: 0.32719 G loss: 2.43318 (0.036 sec/batch, 1796.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:51,531] [train step19551] D loss: 0.32779 G loss: 2.42992 (0.039 sec/batch, 1647.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:51,939] [train step19560] D loss: 0.32645 G loss: 2.31447 (0.041 sec/batch, 1564.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:52,324] [train step19571] D loss: 0.32833 G loss: 2.50513 (0.037 sec/batch, 1749.962 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:52,701] [train step19581] D loss: 0.32909 G loss: 2.50379 (0.041 sec/batch, 1575.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:53,092] [train step19590] D loss: 0.32759 G loss: 2.31182 (0.041 sec/batch, 1565.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:53,470] [train step19600] D loss: 0.33020 G loss: 2.57317 (0.036 sec/batch, 1758.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:53,844] [train step19611] D loss: 0.32868 G loss: 2.50950 (0.037 sec/batch, 1729.699 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:54,231] [train step19620] D loss: 0.32678 G loss: 2.29585 (0.035 sec/batch, 1807.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:54,609] [train step19631] D loss: 0.32802 G loss: 2.46898 (0.033 sec/batch, 1910.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:55,007] [train step19641] D loss: 0.32825 G loss: 2.17067 (0.035 sec/batch, 1808.730 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:55,387] [train step19650] D loss: 0.33056 G loss: 2.58296 (0.037 sec/batch, 1726.129 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:55,783] [train step19660] D loss: 0.33023 G loss: 2.58793 (0.042 sec/batch, 1518.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:56,167] [train step19671] D loss: 0.32712 G loss: 2.28932 (0.035 sec/batch, 1841.677 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:56,552] [train step19680] D loss: 0.32713 G loss: 2.37668 (0.036 sec/batch, 1754.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:56,948] [train step19691] D loss: 0.32713 G loss: 2.37359 (0.043 sec/batch, 1485.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:57,329] [train step19700] D loss: 0.33149 G loss: 2.03739 (0.038 sec/batch, 1699.884 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:57,702] [train step19710] D loss: 0.33194 G loss: 2.64528 (0.037 sec/batch, 1732.054 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:58,095] [train step19720] D loss: 0.32706 G loss: 2.43966 (0.038 sec/batch, 1688.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:58,473] [train step19731] D loss: 0.32730 G loss: 2.27143 (0.038 sec/batch, 1701.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:58,844] [train step19740] D loss: 0.32691 G loss: 2.32431 (0.030 sec/batch, 2161.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:59,243] [train step19751] D loss: 0.32749 G loss: 2.19686 (0.036 sec/batch, 1781.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:59,625] [train step19761] D loss: 0.32695 G loss: 2.27175 (0.036 sec/batch, 1764.409 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:21:59,999] [train step19770] D loss: 0.32658 G loss: 2.29195 (0.034 sec/batch, 1876.646 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:00,401] [train step19780] D loss: 0.32650 G loss: 2.33349 (0.038 sec/batch, 1676.014 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:00,789] [train step19790] D loss: 0.32730 G loss: 2.40298 (0.042 sec/batch, 1527.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:01,173] [train step19800] D loss: 0.32660 G loss: 2.28022 (0.034 sec/batch, 1859.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:01,556] [train step19811] D loss: 0.32690 G loss: 2.29385 (0.040 sec/batch, 1605.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:01,942] [train step19820] D loss: 0.32679 G loss: 2.37448 (0.044 sec/batch, 1442.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:02,351] [train step19830] D loss: 0.32776 G loss: 2.21924 (0.035 sec/batch, 1852.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:02,735] [train step19840] D loss: 0.32790 G loss: 2.17492 (0.037 sec/batch, 1741.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:03,131] [train step19850] D loss: 0.32969 G loss: 2.09726 (0.036 sec/batch, 1755.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:03,521] [train step19860] D loss: 0.33327 G loss: 2.67623 (0.035 sec/batch, 1827.583 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:03,905] [train step19870] D loss: 0.32898 G loss: 2.53300 (0.030 sec/batch, 2107.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:04,305] [train step19880] D loss: 0.32737 G loss: 2.41568 (0.039 sec/batch, 1644.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:04,706] [train step19890] D loss: 0.32742 G loss: 2.28548 (0.038 sec/batch, 1685.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:05,088] [train step19900] D loss: 0.32849 G loss: 2.16327 (0.038 sec/batch, 1665.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:05,474] [train step19910] D loss: 0.32926 G loss: 2.10222 (0.039 sec/batch, 1659.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:05,858] [train step19920] D loss: 0.33097 G loss: 2.60504 (0.041 sec/batch, 1553.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:06,238] [train step19930] D loss: 0.32708 G loss: 2.41637 (0.038 sec/batch, 1679.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:06,625] [train step19940] D loss: 0.32896 G loss: 2.52433 (0.037 sec/batch, 1726.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:07,002] [train step19950] D loss: 0.33006 G loss: 2.08512 (0.036 sec/batch, 1798.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:07,394] [train step19961] D loss: 0.32844 G loss: 2.17865 (0.040 sec/batch, 1588.610 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:07,765] [train step19971] D loss: 0.32674 G loss: 2.28738 (0.038 sec/batch, 1683.371 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:08,148] [train step19980] D loss: 0.32873 G loss: 2.13186 (0.040 sec/batch, 1590.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:08,536] [train step19990] D loss: 0.32778 G loss: 2.16283 (0.035 sec/batch, 1821.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:08,923] [train step20001] D loss: 0.32702 G loss: 2.28809 (0.036 sec/batch, 1786.128 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:22:08,923] Saved checkpoint at 20000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:09,510] [train step20010] D loss: 0.32657 G loss: 2.36971 (0.036 sec/batch, 1770.472 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:09,896] [train step20021] D loss: 0.32695 G loss: 2.36371 (0.036 sec/batch, 1766.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:10,271] [train step20030] D loss: 0.32731 G loss: 2.43261 (0.035 sec/batch, 1822.434 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:10,661] [train step20040] D loss: 0.32739 G loss: 2.20417 (0.034 sec/batch, 1897.875 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:11,049] [train step20050] D loss: 0.32808 G loss: 2.16365 (0.038 sec/batch, 1667.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:11,438] [train step20060] D loss: 0.32745 G loss: 2.21666 (0.038 sec/batch, 1678.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:11,818] [train step20070] D loss: 0.32674 G loss: 2.38739 (0.035 sec/batch, 1816.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:12,200] [train step20081] D loss: 0.32707 G loss: 2.44008 (0.036 sec/batch, 1782.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:12,583] [train step20091] D loss: 0.32743 G loss: 2.45166 (0.040 sec/batch, 1615.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:12,976] [train step20100] D loss: 0.32652 G loss: 2.31270 (0.037 sec/batch, 1737.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:13,352] [train step20111] D loss: 0.32721 G loss: 2.41256 (0.043 sec/batch, 1479.709 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:13,738] [train step20120] D loss: 0.32671 G loss: 2.26263 (0.033 sec/batch, 1953.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:14,129] [train step20130] D loss: 0.32660 G loss: 2.32483 (0.038 sec/batch, 1706.411 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:14,527] [train step20141] D loss: 0.32676 G loss: 2.24692 (0.037 sec/batch, 1750.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:14,907] [train step20151] D loss: 0.32708 G loss: 2.28716 (0.040 sec/batch, 1593.041 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:15,284] [train step20160] D loss: 0.32673 G loss: 2.31630 (0.040 sec/batch, 1595.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:15,675] [train step20171] D loss: 0.32686 G loss: 2.23785 (0.038 sec/batch, 1701.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:16,054] [train step20180] D loss: 0.32767 G loss: 2.40974 (0.038 sec/batch, 1662.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:16,439] [train step20190] D loss: 0.32724 G loss: 2.19728 (0.039 sec/batch, 1622.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:16,831] [train step20201] D loss: 0.32646 G loss: 2.31658 (0.033 sec/batch, 1943.494 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:17,213] [train step20211] D loss: 0.32694 G loss: 2.40153 (0.034 sec/batch, 1856.618 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:17,597] [train step20220] D loss: 0.32700 G loss: 2.24060 (0.042 sec/batch, 1508.107 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:17,988] [train step20231] D loss: 0.32700 G loss: 2.25704 (0.040 sec/batch, 1588.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:18,363] [train step20241] D loss: 0.32700 G loss: 2.20668 (0.032 sec/batch, 2012.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:18,755] [train step20250] D loss: 0.32823 G loss: 2.49325 (0.041 sec/batch, 1554.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:19,132] [train step20261] D loss: 0.32660 G loss: 2.37390 (0.035 sec/batch, 1814.035 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:19,504] [train step20271] D loss: 0.32696 G loss: 2.40111 (0.031 sec/batch, 2034.666 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:19,901] [train step20280] D loss: 0.32674 G loss: 2.32770 (0.035 sec/batch, 1807.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:20,270] [train step20290] D loss: 0.32620 G loss: 2.30413 (0.034 sec/batch, 1872.797 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:20,640] [train step20300] D loss: 0.32677 G loss: 2.26341 (0.032 sec/batch, 1987.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:21,013] [train step20310] D loss: 0.32645 G loss: 2.31739 (0.034 sec/batch, 1875.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:21,383] [train step20320] D loss: 0.32625 G loss: 2.28563 (0.032 sec/batch, 1988.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:21,766] [train step20331] D loss: 0.32630 G loss: 2.32323 (0.037 sec/batch, 1714.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:22,144] [train step20340] D loss: 0.32634 G loss: 2.33481 (0.043 sec/batch, 1475.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:22,518] [train step20350] D loss: 0.32653 G loss: 2.28639 (0.036 sec/batch, 1761.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:22,901] [train step20361] D loss: 0.32648 G loss: 2.37304 (0.041 sec/batch, 1555.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:23,287] [train step20370] D loss: 0.32630 G loss: 2.30575 (0.037 sec/batch, 1751.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:23,687] [train step20380] D loss: 0.32687 G loss: 2.40846 (0.048 sec/batch, 1344.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:24,066] [train step20390] D loss: 0.32661 G loss: 2.31952 (0.040 sec/batch, 1606.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:24,440] [train step20400] D loss: 0.32656 G loss: 2.30158 (0.032 sec/batch, 1996.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:24,827] [train step20411] D loss: 0.32628 G loss: 2.29217 (0.037 sec/batch, 1709.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:25,223] [train step20421] D loss: 0.32675 G loss: 2.38815 (0.037 sec/batch, 1712.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:25,587] [train step20430] D loss: 0.32627 G loss: 2.30486 (0.032 sec/batch, 1995.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:25,973] [train step20441] D loss: 0.32699 G loss: 2.41043 (0.033 sec/batch, 1914.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:26,343] [train step20451] D loss: 0.32659 G loss: 2.27818 (0.038 sec/batch, 1662.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:26,711] [train step20460] D loss: 0.32800 G loss: 2.48372 (0.037 sec/batch, 1727.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:27,095] [train step20470] D loss: 0.33051 G loss: 2.60171 (0.039 sec/batch, 1654.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:27,472] [train step20480] D loss: 0.32719 G loss: 2.42958 (0.037 sec/batch, 1712.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:27,851] [train step20490] D loss: 0.32666 G loss: 2.21619 (0.033 sec/batch, 1911.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:28,242] [train step20500] D loss: 0.32664 G loss: 2.37127 (0.042 sec/batch, 1515.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:28,621] [train step20510] D loss: 0.32746 G loss: 2.48199 (0.037 sec/batch, 1717.690 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:29,004] [train step20520] D loss: 0.32781 G loss: 2.16238 (0.038 sec/batch, 1674.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:29,381] [train step20531] D loss: 0.32699 G loss: 2.20304 (0.037 sec/batch, 1714.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:29,759] [train step20541] D loss: 0.32684 G loss: 2.28866 (0.038 sec/batch, 1670.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:30,151] [train step20550] D loss: 0.32602 G loss: 2.32408 (0.036 sec/batch, 1771.781 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:30,533] [train step20561] D loss: 0.32688 G loss: 2.37999 (0.043 sec/batch, 1499.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:30,918] [train step20571] D loss: 0.32830 G loss: 2.47939 (0.036 sec/batch, 1780.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:31,297] [train step20580] D loss: 0.32745 G loss: 2.17023 (0.036 sec/batch, 1802.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:31,681] [train step20590] D loss: 0.32660 G loss: 2.29682 (0.048 sec/batch, 1325.156 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:32,070] [train step20601] D loss: 0.32695 G loss: 2.22685 (0.041 sec/batch, 1552.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:32,454] [train step20610] D loss: 0.32648 G loss: 2.33221 (0.036 sec/batch, 1784.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:32,837] [train step20620] D loss: 0.32742 G loss: 2.42510 (0.043 sec/batch, 1471.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:33,233] [train step20630] D loss: 0.32670 G loss: 2.22103 (0.036 sec/batch, 1754.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:33,611] [train step20640] D loss: 0.32683 G loss: 2.40700 (0.041 sec/batch, 1555.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:34,018] [train step20651] D loss: 0.32996 G loss: 2.58330 (0.044 sec/batch, 1442.395 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:34,398] [train step20661] D loss: 0.33495 G loss: 2.73422 (0.037 sec/batch, 1748.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:34,783] [train step20670] D loss: 0.32919 G loss: 2.09144 (0.040 sec/batch, 1589.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:35,177] [train step20680] D loss: 0.32777 G loss: 2.15257 (0.039 sec/batch, 1646.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:35,555] [train step20691] D loss: 0.32677 G loss: 2.24571 (0.040 sec/batch, 1608.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:35,932] [train step20700] D loss: 0.32750 G loss: 2.46707 (0.035 sec/batch, 1854.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:36,320] [train step20710] D loss: 0.32609 G loss: 2.33249 (0.035 sec/batch, 1810.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:36,701] [train step20720] D loss: 0.32661 G loss: 2.38319 (0.033 sec/batch, 1936.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:37,091] [train step20730] D loss: 0.32627 G loss: 2.28857 (0.038 sec/batch, 1674.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:37,469] [train step20740] D loss: 0.32770 G loss: 2.46106 (0.044 sec/batch, 1457.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:37,845] [train step20751] D loss: 0.32804 G loss: 2.48801 (0.037 sec/batch, 1707.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:38,231] [train step20760] D loss: 0.32671 G loss: 2.22305 (0.036 sec/batch, 1797.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:38,613] [train step20771] D loss: 0.32618 G loss: 2.31094 (0.036 sec/batch, 1776.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:38,992] [train step20780] D loss: 0.32654 G loss: 2.28921 (0.037 sec/batch, 1723.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:39,379] [train step20790] D loss: 0.32607 G loss: 2.26999 (0.036 sec/batch, 1796.817 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:39,758] [train step20801] D loss: 0.32710 G loss: 2.20162 (0.036 sec/batch, 1756.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:40,153] [train step20811] D loss: 0.32719 G loss: 2.20146 (0.036 sec/batch, 1782.464 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:40,539] [train step20820] D loss: 0.32688 G loss: 2.39799 (0.038 sec/batch, 1667.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:40,914] [train step20831] D loss: 0.32695 G loss: 2.43810 (0.038 sec/batch, 1694.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:41,314] [train step20840] D loss: 0.32668 G loss: 2.26835 (0.035 sec/batch, 1853.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:41,698] [train step20850] D loss: 0.32769 G loss: 2.18187 (0.039 sec/batch, 1642.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:42,084] [train step20860] D loss: 0.33188 G loss: 2.02810 (0.037 sec/batch, 1722.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:42,470] [train step20871] D loss: 0.32635 G loss: 2.22161 (0.039 sec/batch, 1627.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:42,847] [train step20880] D loss: 0.32629 G loss: 2.32157 (0.042 sec/batch, 1517.751 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:43,240] [train step20890] D loss: 0.32696 G loss: 2.35563 (0.048 sec/batch, 1332.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:43,622] [train step20900] D loss: 0.32703 G loss: 2.40946 (0.038 sec/batch, 1677.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:44,003] [train step20910] D loss: 0.32619 G loss: 2.31690 (0.042 sec/batch, 1538.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:44,392] [train step20920] D loss: 0.32844 G loss: 2.51303 (0.039 sec/batch, 1660.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:44,789] [train step20931] D loss: 0.32833 G loss: 2.52081 (0.037 sec/batch, 1715.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:45,173] [train step20940] D loss: 0.32731 G loss: 2.16549 (0.041 sec/batch, 1577.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:45,568] [train step20951] D loss: 0.32680 G loss: 2.20247 (0.039 sec/batch, 1639.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:45,954] [train step20960] D loss: 0.32616 G loss: 2.27271 (0.038 sec/batch, 1706.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:46,353] [train step20970] D loss: 0.32633 G loss: 2.34722 (0.039 sec/batch, 1653.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:46,734] [train step20980] D loss: 0.32706 G loss: 2.43678 (0.039 sec/batch, 1661.162 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:47,113] [train step20990] D loss: 0.32700 G loss: 2.42692 (0.036 sec/batch, 1788.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:47,504] [train step21000] D loss: 0.32685 G loss: 2.22395 (0.038 sec/batch, 1690.857 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:22:47,504] Saved checkpoint at 21000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:48,084] [train step21011] D loss: 0.32623 G loss: 2.31770 (0.030 sec/batch, 2108.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:48,481] [train step21020] D loss: 0.32661 G loss: 2.39228 (0.044 sec/batch, 1467.269 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:48,863] [train step21030] D loss: 0.32655 G loss: 2.28592 (0.037 sec/batch, 1745.536 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:49,243] [train step21041] D loss: 0.32643 G loss: 2.29353 (0.040 sec/batch, 1605.754 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:49,629] [train step21051] D loss: 0.32633 G loss: 2.23888 (0.037 sec/batch, 1751.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:50,012] [train step21060] D loss: 0.32615 G loss: 2.24985 (0.036 sec/batch, 1782.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:50,396] [train step21071] D loss: 0.32614 G loss: 2.24140 (0.046 sec/batch, 1391.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:50,782] [train step21081] D loss: 0.32670 G loss: 2.22938 (0.036 sec/batch, 1761.353 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:51,158] [train step21090] D loss: 0.32640 G loss: 2.41142 (0.038 sec/batch, 1667.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:51,538] [train step21100] D loss: 0.32618 G loss: 2.36955 (0.037 sec/batch, 1724.344 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:51,913] [train step21111] D loss: 0.32829 G loss: 2.11574 (0.038 sec/batch, 1676.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:52,284] [train step21120] D loss: 0.32749 G loss: 2.47774 (0.039 sec/batch, 1624.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:52,674] [train step21130] D loss: 0.32830 G loss: 2.51224 (0.038 sec/batch, 1688.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:53,046] [train step21140] D loss: 0.32679 G loss: 2.42394 (0.033 sec/batch, 1926.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:53,414] [train step21150] D loss: 0.32613 G loss: 2.30472 (0.033 sec/batch, 1959.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:53,794] [train step21161] D loss: 0.32772 G loss: 2.47873 (0.033 sec/batch, 1951.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:54,174] [train step21170] D loss: 0.32642 G loss: 2.30444 (0.035 sec/batch, 1808.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:54,557] [train step21180] D loss: 0.32634 G loss: 2.32958 (0.035 sec/batch, 1809.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:54,931] [train step21191] D loss: 0.32625 G loss: 2.37894 (0.035 sec/batch, 1811.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:55,330] [train step21201] D loss: 0.32710 G loss: 2.18795 (0.035 sec/batch, 1809.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:55,714] [train step21210] D loss: 0.32801 G loss: 2.51470 (0.038 sec/batch, 1676.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:56,079] [train step21221] D loss: 0.32673 G loss: 2.41243 (0.035 sec/batch, 1815.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:56,456] [train step21231] D loss: 0.32623 G loss: 2.36908 (0.036 sec/batch, 1755.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:56,834] [train step21240] D loss: 0.32676 G loss: 2.20917 (0.041 sec/batch, 1542.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:57,203] [train step21251] D loss: 0.32630 G loss: 2.22112 (0.036 sec/batch, 1795.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:57,590] [train step21260] D loss: 0.32652 G loss: 2.26320 (0.054 sec/batch, 1182.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:57,962] [train step21270] D loss: 0.32683 G loss: 2.43584 (0.037 sec/batch, 1731.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:58,344] [train step21281] D loss: 0.32598 G loss: 2.30330 (0.046 sec/batch, 1393.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:58,721] [train step21290] D loss: 0.32622 G loss: 2.28586 (0.034 sec/batch, 1903.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:59,104] [train step21300] D loss: 0.32636 G loss: 2.36911 (0.041 sec/batch, 1566.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:59,478] [train step21311] D loss: 0.32632 G loss: 2.36394 (0.039 sec/batch, 1627.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:22:59,870] [train step21320] D loss: 0.32744 G loss: 2.46941 (0.040 sec/batch, 1605.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:00,251] [train step21330] D loss: 0.32694 G loss: 2.20059 (0.039 sec/batch, 1626.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:00,627] [train step21340] D loss: 0.32610 G loss: 2.28572 (0.038 sec/batch, 1684.142 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:01,012] [train step21351] D loss: 0.32627 G loss: 2.24311 (0.038 sec/batch, 1682.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:01,387] [train step21360] D loss: 0.32591 G loss: 2.29174 (0.037 sec/batch, 1732.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:01,771] [train step21370] D loss: 0.32651 G loss: 2.23787 (0.036 sec/batch, 1756.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:02,157] [train step21381] D loss: 0.32624 G loss: 2.22610 (0.036 sec/batch, 1766.174 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:02,539] [train step21390] D loss: 0.32622 G loss: 2.37708 (0.038 sec/batch, 1690.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:02,929] [train step21400] D loss: 0.32612 G loss: 2.34380 (0.039 sec/batch, 1626.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:03,312] [train step21411] D loss: 0.32628 G loss: 2.25771 (0.044 sec/batch, 1450.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:03,701] [train step21420] D loss: 0.32709 G loss: 2.43846 (0.040 sec/batch, 1590.210 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:04,082] [train step21431] D loss: 0.32667 G loss: 2.42641 (0.037 sec/batch, 1743.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:04,464] [train step21440] D loss: 0.32649 G loss: 2.39942 (0.043 sec/batch, 1475.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:04,851] [train step21450] D loss: 0.32617 G loss: 2.23767 (0.041 sec/batch, 1543.824 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:05,235] [train step21461] D loss: 0.32631 G loss: 2.23621 (0.036 sec/batch, 1771.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:05,631] [train step21471] D loss: 0.32609 G loss: 2.28362 (0.045 sec/batch, 1411.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:06,019] [train step21480] D loss: 0.32625 G loss: 2.40565 (0.036 sec/batch, 1801.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:06,410] [train step21491] D loss: 0.32603 G loss: 2.33752 (0.037 sec/batch, 1750.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:06,791] [train step21501] D loss: 0.32651 G loss: 2.21180 (0.037 sec/batch, 1742.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:07,186] [train step21510] D loss: 0.32683 G loss: 2.45077 (0.039 sec/batch, 1652.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:07,570] [train step21521] D loss: 0.32583 G loss: 2.28037 (0.041 sec/batch, 1557.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:07,972] [train step21530] D loss: 0.32622 G loss: 2.28464 (0.042 sec/batch, 1516.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:08,354] [train step21540] D loss: 0.32606 G loss: 2.36308 (0.036 sec/batch, 1767.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:08,734] [train step21550] D loss: 0.32605 G loss: 2.27078 (0.037 sec/batch, 1710.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:09,128] [train step21560] D loss: 0.32629 G loss: 2.29650 (0.038 sec/batch, 1699.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:09,510] [train step21570] D loss: 0.32604 G loss: 2.32940 (0.038 sec/batch, 1682.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:09,892] [train step21581] D loss: 0.32628 G loss: 2.21170 (0.039 sec/batch, 1622.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:10,299] [train step21590] D loss: 0.32569 G loss: 2.30605 (0.040 sec/batch, 1604.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:10,684] [train step21600] D loss: 0.32603 G loss: 2.28718 (0.035 sec/batch, 1839.809 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:11,075] [train step21611] D loss: 0.32596 G loss: 2.34918 (0.040 sec/batch, 1615.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:11,453] [train step21620] D loss: 0.32604 G loss: 2.36837 (0.035 sec/batch, 1806.114 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:11,844] [train step21630] D loss: 0.32611 G loss: 2.25354 (0.038 sec/batch, 1666.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:12,233] [train step21640] D loss: 0.32570 G loss: 2.30573 (0.035 sec/batch, 1842.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:12,620] [train step21650] D loss: 0.32708 G loss: 2.44849 (0.037 sec/batch, 1714.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:13,006] [train step21660] D loss: 0.32678 G loss: 2.17776 (0.044 sec/batch, 1443.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:13,393] [train step21671] D loss: 0.32653 G loss: 2.22947 (0.038 sec/batch, 1687.106 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:13,786] [train step21681] D loss: 0.32623 G loss: 2.36170 (0.035 sec/batch, 1810.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:14,182] [train step21690] D loss: 0.32607 G loss: 2.30508 (0.042 sec/batch, 1523.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:14,568] [train step21700] D loss: 0.32627 G loss: 2.36653 (0.042 sec/batch, 1523.678 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:14,954] [train step21711] D loss: 0.32642 G loss: 2.36397 (0.034 sec/batch, 1864.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:15,347] [train step21720] D loss: 0.32668 G loss: 2.42888 (0.039 sec/batch, 1642.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:15,728] [train step21731] D loss: 0.32566 G loss: 2.30728 (0.033 sec/batch, 1913.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:16,122] [train step21741] D loss: 0.32594 G loss: 2.36569 (0.035 sec/batch, 1810.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:16,528] [train step21750] D loss: 0.32623 G loss: 2.30930 (0.037 sec/batch, 1707.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:16,907] [train step21760] D loss: 0.32604 G loss: 2.38414 (0.033 sec/batch, 1917.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:17,297] [train step21771] D loss: 0.32578 G loss: 2.28255 (0.037 sec/batch, 1732.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:17,682] [train step21780] D loss: 0.32576 G loss: 2.30940 (0.038 sec/batch, 1675.418 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:18,060] [train step21790] D loss: 0.32599 G loss: 2.32233 (0.030 sec/batch, 2167.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:18,456] [train step21801] D loss: 0.32578 G loss: 2.34686 (0.031 sec/batch, 2046.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:18,843] [train step21810] D loss: 0.32592 G loss: 2.29834 (0.037 sec/batch, 1749.312 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:19,246] [train step21820] D loss: 0.32562 G loss: 2.34172 (0.039 sec/batch, 1643.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:19,639] [train step21831] D loss: 0.32575 G loss: 2.28300 (0.040 sec/batch, 1583.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:20,007] [train step21840] D loss: 0.32609 G loss: 2.33751 (0.027 sec/batch, 2387.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:20,400] [train step21851] D loss: 0.32673 G loss: 2.38176 (0.035 sec/batch, 1819.420 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:20,769] [train step21860] D loss: 0.32600 G loss: 2.28279 (0.034 sec/batch, 1883.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:21,161] [train step21870] D loss: 0.32652 G loss: 2.38970 (0.041 sec/batch, 1558.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:21,556] [train step21880] D loss: 0.32695 G loss: 2.25206 (0.039 sec/batch, 1633.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:21,946] [train step21890] D loss: 0.32600 G loss: 2.34980 (0.041 sec/batch, 1546.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:22,335] [train step21900] D loss: 0.32777 G loss: 2.12355 (0.039 sec/batch, 1642.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:22,721] [train step21910] D loss: 0.32687 G loss: 2.41595 (0.038 sec/batch, 1670.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:23,096] [train step21920] D loss: 0.32634 G loss: 2.22286 (0.035 sec/batch, 1823.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:23,506] [train step21930] D loss: 0.33042 G loss: 2.51427 (0.038 sec/batch, 1695.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:23,909] [train step21940] D loss: 0.32656 G loss: 2.23020 (0.042 sec/batch, 1506.642 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:24,290] [train step21950] D loss: 5.21070 G loss: 6.55358 (0.035 sec/batch, 1804.875 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:24,687] [train step21960] D loss: 1.30237 G loss: 1.23863 (0.046 sec/batch, 1381.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:25,067] [train step21970] D loss: 0.53440 G loss: 4.31980 (0.037 sec/batch, 1708.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:25,470] [train step21981] D loss: 0.60171 G loss: 1.34551 (0.037 sec/batch, 1733.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:25,849] [train step21990] D loss: 1.32363 G loss: 13.23620 (0.038 sec/batch, 1694.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:26,237] [train step22000] D loss: 0.55119 G loss: 5.46523 (0.039 sec/batch, 1629.687 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:23:26,238] Saved checkpoint at 22000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:26,835] [train step22011] D loss: 1.05015 G loss: 10.49714 (0.033 sec/batch, 1928.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:27,230] [train step22020] D loss: 1.01350 G loss: 10.12794 (0.038 sec/batch, 1700.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:27,620] [train step22031] D loss: 0.38033 G loss: 1.87994 (0.040 sec/batch, 1587.277 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:27,993] [train step22041] D loss: 0.39543 G loss: 3.69225 (0.035 sec/batch, 1828.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:28,365] [train step22050] D loss: 0.34450 G loss: 2.22995 (0.035 sec/batch, 1820.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:28,746] [train step22060] D loss: 0.34781 G loss: 2.38050 (0.040 sec/batch, 1606.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:29,130] [train step22071] D loss: 0.34726 G loss: 2.52840 (0.034 sec/batch, 1895.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:29,514] [train step22080] D loss: 0.34585 G loss: 2.35570 (0.038 sec/batch, 1676.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:29,890] [train step22091] D loss: 0.36878 G loss: 2.04389 (0.035 sec/batch, 1843.511 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:30,266] [train step22101] D loss: 0.34213 G loss: 2.25274 (0.044 sec/batch, 1457.431 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:30,658] [train step22110] D loss: 0.37736 G loss: 3.43522 (0.038 sec/batch, 1677.386 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:31,041] [train step22121] D loss: 0.33884 G loss: 2.33923 (0.036 sec/batch, 1761.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:31,429] [train step22130] D loss: 0.35382 G loss: 1.99683 (0.040 sec/batch, 1605.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:31,821] [train step22140] D loss: 0.56976 G loss: 5.66069 (0.039 sec/batch, 1638.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:32,201] [train step22151] D loss: 0.71178 G loss: 0.77293 (0.042 sec/batch, 1533.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:32,585] [train step22161] D loss: 1.29664 G loss: 12.96200 (0.039 sec/batch, 1642.269 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:32,967] [train step22170] D loss: 0.91934 G loss: 9.19236 (0.036 sec/batch, 1793.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:33,351] [train step22180] D loss: 0.37473 G loss: 1.73531 (0.036 sec/batch, 1786.782 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:33,741] [train step22191] D loss: 0.33941 G loss: 2.75787 (0.039 sec/batch, 1631.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:34,124] [train step22200] D loss: 0.33579 G loss: 2.21125 (0.037 sec/batch, 1726.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:34,508] [train step22211] D loss: 0.33270 G loss: 2.32836 (0.041 sec/batch, 1543.336 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:34,897] [train step22221] D loss: 0.33209 G loss: 2.37584 (0.035 sec/batch, 1829.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:35,283] [train step22230] D loss: 0.33546 G loss: 2.57909 (0.037 sec/batch, 1751.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:35,675] [train step22241] D loss: 0.33249 G loss: 2.46841 (0.037 sec/batch, 1750.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:36,056] [train step22250] D loss: 0.33813 G loss: 2.28946 (0.034 sec/batch, 1867.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:36,442] [train step22260] D loss: 0.33220 G loss: 2.33523 (0.037 sec/batch, 1719.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:36,835] [train step22271] D loss: 0.33330 G loss: 2.52468 (0.037 sec/batch, 1751.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:37,216] [train step22280] D loss: 0.33321 G loss: 2.32661 (0.037 sec/batch, 1740.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:37,609] [train step22290] D loss: 0.33385 G loss: 2.23727 (0.048 sec/batch, 1321.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:37,994] [train step22301] D loss: 0.33384 G loss: 2.32595 (0.036 sec/batch, 1798.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:38,383] [train step22311] D loss: 0.33156 G loss: 2.33857 (0.038 sec/batch, 1675.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:38,774] [train step22320] D loss: 0.33873 G loss: 2.55757 (0.045 sec/batch, 1416.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:39,149] [train step22330] D loss: 0.33332 G loss: 2.45422 (0.042 sec/batch, 1538.568 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:39,531] [train step22340] D loss: 0.34388 G loss: 1.97849 (0.036 sec/batch, 1774.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:39,925] [train step22350] D loss: 0.33238 G loss: 2.55753 (0.037 sec/batch, 1709.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:40,305] [train step22360] D loss: 0.33267 G loss: 2.15327 (0.033 sec/batch, 1922.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:40,693] [train step22371] D loss: 0.33180 G loss: 2.35238 (0.042 sec/batch, 1535.698 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:41,082] [train step22380] D loss: 0.33046 G loss: 2.45820 (0.035 sec/batch, 1826.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:41,462] [train step22390] D loss: 0.33276 G loss: 2.44614 (0.039 sec/batch, 1649.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:41,851] [train step22401] D loss: 0.33193 G loss: 2.23124 (0.034 sec/batch, 1859.512 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:42,236] [train step22410] D loss: 0.33062 G loss: 2.45741 (0.034 sec/batch, 1877.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:42,619] [train step22421] D loss: 0.33315 G loss: 2.46734 (0.039 sec/batch, 1634.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:43,010] [train step22430] D loss: 0.33296 G loss: 2.48637 (0.037 sec/batch, 1718.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:43,389] [train step22440] D loss: 0.33240 G loss: 2.20222 (0.036 sec/batch, 1762.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:43,768] [train step22451] D loss: 0.33209 G loss: 2.50187 (0.036 sec/batch, 1755.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:44,166] [train step22461] D loss: 0.32893 G loss: 2.31757 (0.038 sec/batch, 1696.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:44,550] [train step22470] D loss: 0.32947 G loss: 2.27116 (0.037 sec/batch, 1742.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:44,944] [train step22481] D loss: 0.33156 G loss: 2.44157 (0.040 sec/batch, 1594.404 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:45,328] [train step22490] D loss: 0.33238 G loss: 2.20445 (0.042 sec/batch, 1533.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:45,713] [train step22500] D loss: 0.33043 G loss: 2.46260 (0.045 sec/batch, 1422.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:46,101] [train step22510] D loss: 0.33039 G loss: 2.37320 (0.038 sec/batch, 1675.794 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:46,488] [train step22520] D loss: 0.33043 G loss: 2.37442 (0.039 sec/batch, 1630.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:46,865] [train step22530] D loss: 0.33076 G loss: 2.49614 (0.035 sec/batch, 1818.397 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:47,257] [train step22540] D loss: 0.32937 G loss: 2.23034 (0.041 sec/batch, 1568.413 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:47,631] [train step22551] D loss: 0.32969 G loss: 2.27471 (0.041 sec/batch, 1565.386 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:48,038] [train step22560] D loss: 0.32978 G loss: 2.36476 (0.037 sec/batch, 1727.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:48,426] [train step22570] D loss: 0.33010 G loss: 2.29881 (0.038 sec/batch, 1664.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:48,810] [train step22580] D loss: 0.32919 G loss: 2.34925 (0.037 sec/batch, 1752.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:49,202] [train step22590] D loss: 0.32947 G loss: 2.35110 (0.036 sec/batch, 1755.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:49,582] [train step22601] D loss: 0.33001 G loss: 2.35868 (0.034 sec/batch, 1856.066 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:49,971] [train step22611] D loss: 0.33227 G loss: 2.20315 (0.037 sec/batch, 1747.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:50,360] [train step22620] D loss: 0.32876 G loss: 2.33617 (0.037 sec/batch, 1709.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:50,736] [train step22630] D loss: 0.33014 G loss: 2.31822 (0.039 sec/batch, 1652.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:51,119] [train step22641] D loss: 0.33036 G loss: 2.46627 (0.038 sec/batch, 1680.631 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:51,498] [train step22650] D loss: 0.32954 G loss: 2.38252 (0.037 sec/batch, 1716.394 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:51,871] [train step22660] D loss: 0.33058 G loss: 2.19819 (0.039 sec/batch, 1645.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:52,252] [train step22670] D loss: 0.32800 G loss: 2.31238 (0.036 sec/batch, 1784.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:52,622] [train step22680] D loss: 0.33301 G loss: 2.06445 (0.036 sec/batch, 1777.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:52,994] [train step22690] D loss: 0.33072 G loss: 2.27443 (0.037 sec/batch, 1742.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:53,385] [train step22701] D loss: 0.32946 G loss: 2.41363 (0.037 sec/batch, 1724.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:53,767] [train step22710] D loss: 0.32998 G loss: 2.47154 (0.042 sec/batch, 1506.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:54,156] [train step22721] D loss: 0.33048 G loss: 2.40322 (0.044 sec/batch, 1457.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:54,530] [train step22731] D loss: 0.33023 G loss: 2.47833 (0.036 sec/batch, 1793.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:54,901] [train step22740] D loss: 0.33049 G loss: 2.31654 (0.036 sec/batch, 1791.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:55,281] [train step22751] D loss: 0.32924 G loss: 2.37920 (0.034 sec/batch, 1901.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:55,670] [train step22761] D loss: 0.33021 G loss: 2.42558 (0.042 sec/batch, 1526.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:56,052] [train step22770] D loss: 0.32916 G loss: 2.37464 (0.033 sec/batch, 1926.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:56,435] [train step22780] D loss: 0.32942 G loss: 2.24976 (0.038 sec/batch, 1692.126 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:56,814] [train step22791] D loss: 0.32915 G loss: 2.34129 (0.037 sec/batch, 1723.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:57,189] [train step22800] D loss: 0.32985 G loss: 2.22916 (0.039 sec/batch, 1644.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:57,563] [train step22810] D loss: 0.32921 G loss: 2.45031 (0.029 sec/batch, 2241.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:57,945] [train step22821] D loss: 0.32985 G loss: 2.38646 (0.038 sec/batch, 1664.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:58,322] [train step22830] D loss: 0.32975 G loss: 2.29302 (0.036 sec/batch, 1784.858 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:58,702] [train step22840] D loss: 0.32952 G loss: 2.24109 (0.040 sec/batch, 1606.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:59,097] [train step22851] D loss: 0.32902 G loss: 2.39917 (0.036 sec/batch, 1774.287 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:59,489] [train step22860] D loss: 0.32992 G loss: 2.24963 (0.035 sec/batch, 1806.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:23:59,868] [train step22870] D loss: 0.32812 G loss: 2.30177 (0.042 sec/batch, 1535.813 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:00,242] [train step22881] D loss: 0.33011 G loss: 2.49222 (0.038 sec/batch, 1704.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:00,626] [train step22890] D loss: 0.32953 G loss: 2.32450 (0.035 sec/batch, 1805.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:01,002] [train step22901] D loss: 0.32827 G loss: 2.33483 (0.036 sec/batch, 1758.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:01,384] [train step22911] D loss: 0.32775 G loss: 2.26808 (0.034 sec/batch, 1906.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:01,768] [train step22920] D loss: 0.32834 G loss: 2.25764 (0.039 sec/batch, 1625.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:02,146] [train step22930] D loss: 0.32800 G loss: 2.28706 (0.044 sec/batch, 1459.547 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:02,524] [train step22940] D loss: 0.32991 G loss: 2.35107 (0.027 sec/batch, 2414.487 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:02,928] [train step22950] D loss: 0.32835 G loss: 2.40046 (0.040 sec/batch, 1599.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:03,308] [train step22961] D loss: 0.32938 G loss: 2.21757 (0.038 sec/batch, 1686.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:03,705] [train step22970] D loss: 0.32955 G loss: 2.45328 (0.042 sec/batch, 1531.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:04,089] [train step22980] D loss: 0.32921 G loss: 2.27385 (0.043 sec/batch, 1483.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:04,483] [train step22990] D loss: 0.32739 G loss: 2.28512 (0.040 sec/batch, 1617.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:04,863] [train step23000] D loss: 0.32990 G loss: 2.49052 (0.033 sec/batch, 1918.438 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:24:04,864] Saved checkpoint at 23000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:05,464] [train step23010] D loss: 0.32856 G loss: 2.36614 (0.043 sec/batch, 1492.162 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:05,847] [train step23021] D loss: 0.32934 G loss: 2.43452 (0.039 sec/batch, 1652.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:06,225] [train step23031] D loss: 0.32806 G loss: 2.30151 (0.037 sec/batch, 1732.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:06,613] [train step23040] D loss: 0.33181 G loss: 2.51650 (0.036 sec/batch, 1801.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:07,011] [train step23051] D loss: 0.32996 G loss: 2.31500 (0.043 sec/batch, 1492.328 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:07,397] [train step23060] D loss: 0.32915 G loss: 2.30284 (0.037 sec/batch, 1729.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:07,807] [train step23070] D loss: 0.32857 G loss: 2.31090 (0.037 sec/batch, 1741.425 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:08,187] [train step23080] D loss: 0.32821 G loss: 2.38479 (0.039 sec/batch, 1651.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:08,568] [train step23090] D loss: 0.32828 G loss: 2.33975 (0.038 sec/batch, 1671.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:08,958] [train step23100] D loss: 0.32861 G loss: 2.21409 (0.044 sec/batch, 1470.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:09,347] [train step23111] D loss: 0.33030 G loss: 2.46173 (0.037 sec/batch, 1719.285 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:09,739] [train step23121] D loss: 0.32875 G loss: 2.31742 (0.034 sec/batch, 1881.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:10,135] [train step23130] D loss: 0.33893 G loss: 2.21859 (0.038 sec/batch, 1691.881 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:10,517] [train step23141] D loss: 0.37386 G loss: 3.37971 (0.036 sec/batch, 1767.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:10,907] [train step23151] D loss: 1.49390 G loss: 14.93893 (0.038 sec/batch, 1700.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:11,291] [train step23160] D loss: 0.88026 G loss: 8.79985 (0.039 sec/batch, 1658.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:11,687] [train step23170] D loss: 0.43777 G loss: 1.32517 (0.038 sec/batch, 1674.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:12,067] [train step23180] D loss: 0.45925 G loss: 4.43006 (0.038 sec/batch, 1676.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:12,455] [train step23190] D loss: 0.33192 G loss: 2.35585 (0.044 sec/batch, 1460.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:12,840] [train step23201] D loss: 0.37253 G loss: 3.40625 (0.036 sec/batch, 1802.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:13,220] [train step23211] D loss: 0.34995 G loss: 1.85096 (0.037 sec/batch, 1719.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:13,599] [train step23220] D loss: 0.33757 G loss: 2.67571 (0.038 sec/batch, 1679.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:13,990] [train step23231] D loss: 0.33843 G loss: 2.10535 (0.035 sec/batch, 1845.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:14,377] [train step23241] D loss: 0.33136 G loss: 2.46598 (0.037 sec/batch, 1707.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:14,767] [train step23250] D loss: 0.33140 G loss: 2.34848 (0.034 sec/batch, 1878.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:15,149] [train step23260] D loss: 0.33161 G loss: 2.40704 (0.038 sec/batch, 1683.678 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:15,531] [train step23271] D loss: 0.33142 G loss: 2.28420 (0.045 sec/batch, 1435.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:15,916] [train step23280] D loss: 0.33180 G loss: 2.50364 (0.036 sec/batch, 1755.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:16,302] [train step23290] D loss: 0.33561 G loss: 2.56310 (0.035 sec/batch, 1815.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:16,687] [train step23300] D loss: 0.33295 G loss: 2.33118 (0.044 sec/batch, 1470.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:17,076] [train step23310] D loss: 0.33033 G loss: 2.44721 (0.039 sec/batch, 1628.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:17,461] [train step23321] D loss: 0.33152 G loss: 2.43011 (0.038 sec/batch, 1694.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:17,867] [train step23331] D loss: 0.33148 G loss: 2.26912 (0.039 sec/batch, 1644.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:18,247] [train step23340] D loss: 0.33275 G loss: 2.28174 (0.036 sec/batch, 1759.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:18,634] [train step23350] D loss: 0.33084 G loss: 2.23179 (0.045 sec/batch, 1407.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:19,023] [train step23360] D loss: 0.33301 G loss: 2.35381 (0.034 sec/batch, 1906.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:19,414] [train step23370] D loss: 0.32996 G loss: 2.22814 (0.039 sec/batch, 1659.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:19,789] [train step23381] D loss: 0.33196 G loss: 2.40336 (0.038 sec/batch, 1682.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:20,196] [train step23391] D loss: 0.33332 G loss: 2.16064 (0.037 sec/batch, 1751.184 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:20,578] [train step23400] D loss: 0.33793 G loss: 2.06876 (0.048 sec/batch, 1336.897 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:20,958] [train step23410] D loss: 0.33007 G loss: 2.45492 (0.038 sec/batch, 1691.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:21,332] [train step23420] D loss: 0.33534 G loss: 2.63629 (0.038 sec/batch, 1689.719 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:21,710] [train step23430] D loss: 0.33113 G loss: 2.33950 (0.039 sec/batch, 1640.112 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:22,106] [train step23441] D loss: 0.33121 G loss: 2.33044 (0.042 sec/batch, 1531.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:22,476] [train step23451] D loss: 0.33091 G loss: 2.28942 (0.034 sec/batch, 1868.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:22,854] [train step23460] D loss: 0.33143 G loss: 2.19085 (0.041 sec/batch, 1565.185 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:23,230] [train step23471] D loss: 0.33063 G loss: 2.36752 (0.036 sec/batch, 1770.554 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:23,608] [train step23480] D loss: 0.33068 G loss: 2.49076 (0.038 sec/batch, 1668.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:23,991] [train step23490] D loss: 0.33024 G loss: 2.21989 (0.040 sec/batch, 1601.347 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:24,369] [train step23501] D loss: 0.33221 G loss: 2.57782 (0.037 sec/batch, 1710.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:24,751] [train step23510] D loss: 0.33121 G loss: 2.28496 (0.036 sec/batch, 1779.427 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:25,135] [train step23520] D loss: 0.32988 G loss: 2.35088 (0.038 sec/batch, 1664.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:25,507] [train step23531] D loss: 0.33082 G loss: 2.38173 (0.035 sec/batch, 1839.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:25,890] [train step23540] D loss: 0.32891 G loss: 2.36819 (0.031 sec/batch, 2040.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:26,285] [train step23550] D loss: 0.33081 G loss: 2.50257 (0.036 sec/batch, 1800.263 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:26,671] [train step23560] D loss: 0.33175 G loss: 2.42853 (0.041 sec/batch, 1562.797 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:27,072] [train step23570] D loss: 0.32987 G loss: 2.24723 (0.039 sec/batch, 1652.246 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:27,441] [train step23580] D loss: 0.33134 G loss: 2.49085 (0.038 sec/batch, 1682.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:27,836] [train step23590] D loss: 0.32959 G loss: 2.25323 (0.038 sec/batch, 1663.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:28,235] [train step23601] D loss: 0.33003 G loss: 2.26429 (0.037 sec/batch, 1718.449 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:28,617] [train step23610] D loss: 0.32936 G loss: 2.36115 (0.050 sec/batch, 1274.308 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:28,995] [train step23621] D loss: 0.33097 G loss: 2.43800 (0.038 sec/batch, 1680.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:29,384] [train step23630] D loss: 0.33195 G loss: 2.21413 (0.037 sec/batch, 1712.463 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:29,778] [train step23640] D loss: 0.33161 G loss: 2.45126 (0.042 sec/batch, 1518.549 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:30,165] [train step23651] D loss: 0.33079 G loss: 2.23702 (0.037 sec/batch, 1724.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:30,564] [train step23661] D loss: 0.33017 G loss: 2.36023 (0.036 sec/batch, 1759.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:30,942] [train step23670] D loss: 0.33174 G loss: 2.15518 (0.037 sec/batch, 1708.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:31,328] [train step23680] D loss: 0.33220 G loss: 2.53392 (0.037 sec/batch, 1741.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:31,720] [train step23690] D loss: 0.33192 G loss: 2.33696 (0.040 sec/batch, 1588.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:32,093] [train step23700] D loss: 0.32980 G loss: 2.39444 (0.035 sec/batch, 1854.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:32,490] [train step23710] D loss: 0.33049 G loss: 2.24224 (0.044 sec/batch, 1453.021 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:32,872] [train step23721] D loss: 0.32944 G loss: 2.27364 (0.040 sec/batch, 1611.122 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:33,257] [train step23730] D loss: 0.32987 G loss: 2.39578 (0.042 sec/batch, 1527.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:33,644] [train step23740] D loss: 0.32969 G loss: 2.41969 (0.039 sec/batch, 1646.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:34,036] [train step23750] D loss: 0.32999 G loss: 2.41447 (0.038 sec/batch, 1674.101 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:34,433] [train step23760] D loss: 0.32937 G loss: 2.27563 (0.037 sec/batch, 1730.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:34,821] [train step23770] D loss: 0.33009 G loss: 2.36684 (0.038 sec/batch, 1696.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:35,199] [train step23780] D loss: 0.32865 G loss: 2.36963 (0.037 sec/batch, 1720.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:35,609] [train step23790] D loss: 0.32887 G loss: 2.38615 (0.035 sec/batch, 1805.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:35,998] [train step23800] D loss: 0.33013 G loss: 2.36904 (0.040 sec/batch, 1612.419 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:36,400] [train step23811] D loss: 0.33026 G loss: 2.37520 (0.050 sec/batch, 1270.514 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:36,779] [train step23820] D loss: 0.33049 G loss: 2.31441 (0.035 sec/batch, 1816.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:37,164] [train step23831] D loss: 0.33042 G loss: 2.36997 (0.033 sec/batch, 1948.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:37,560] [train step23841] D loss: 0.33026 G loss: 2.44473 (0.036 sec/batch, 1759.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:37,947] [train step23850] D loss: 0.32944 G loss: 2.44791 (0.039 sec/batch, 1652.287 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:38,334] [train step23861] D loss: 0.33074 G loss: 2.28369 (0.044 sec/batch, 1441.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:38,716] [train step23870] D loss: 0.33103 G loss: 2.39445 (0.038 sec/batch, 1703.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:39,106] [train step23880] D loss: 0.33025 G loss: 2.32995 (0.038 sec/batch, 1668.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:39,501] [train step23891] D loss: 0.33007 G loss: 2.30289 (0.035 sec/batch, 1803.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:39,951] [train step23901] D loss: 0.32999 G loss: 2.34976 (0.060 sec/batch, 1073.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:40,372] [train step23910] D loss: 0.33028 G loss: 2.38839 (0.048 sec/batch, 1325.293 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:40,765] [train step23920] D loss: 0.33010 G loss: 2.21098 (0.036 sec/batch, 1798.454 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:41,169] [train step23931] D loss: 0.33118 G loss: 2.53052 (0.036 sec/batch, 1770.858 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:41,560] [train step23940] D loss: 0.32958 G loss: 2.33722 (0.037 sec/batch, 1721.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:41,950] [train step23951] D loss: 0.32994 G loss: 2.30003 (0.047 sec/batch, 1352.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:42,338] [train step23960] D loss: 0.32905 G loss: 2.29067 (0.036 sec/batch, 1762.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:42,740] [train step23970] D loss: 0.33046 G loss: 2.22048 (0.039 sec/batch, 1651.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:43,134] [train step23981] D loss: 0.33003 G loss: 2.22961 (0.039 sec/batch, 1657.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:43,523] [train step23991] D loss: 0.32884 G loss: 2.45720 (0.040 sec/batch, 1608.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:43,906] [train step24000] D loss: 0.32875 G loss: 2.33760 (0.036 sec/batch, 1790.047 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:24:43,907] Saved checkpoint at 24000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:44,504] [train step24011] D loss: 0.32886 G loss: 2.39713 (0.043 sec/batch, 1484.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:44,884] [train step24020] D loss: 0.32875 G loss: 2.34755 (0.037 sec/batch, 1725.807 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:45,263] [train step24030] D loss: 0.33017 G loss: 2.44569 (0.033 sec/batch, 1917.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:45,662] [train step24041] D loss: 0.33011 G loss: 2.26077 (0.036 sec/batch, 1766.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:46,046] [train step24051] D loss: 0.32958 G loss: 2.21152 (0.038 sec/batch, 1670.102 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:46,435] [train step24060] D loss: 0.32905 G loss: 2.36065 (0.037 sec/batch, 1717.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:46,817] [train step24070] D loss: 0.32844 G loss: 2.25093 (0.034 sec/batch, 1879.590 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:47,204] [train step24080] D loss: 0.32897 G loss: 2.27768 (0.039 sec/batch, 1631.489 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:47,594] [train step24090] D loss: 0.33056 G loss: 2.42612 (0.041 sec/batch, 1545.735 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:47,981] [train step24101] D loss: 0.32942 G loss: 2.42793 (0.035 sec/batch, 1853.337 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:48,361] [train step24110] D loss: 0.32926 G loss: 2.36290 (0.038 sec/batch, 1663.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:48,743] [train step24120] D loss: 0.32903 G loss: 2.32475 (0.040 sec/batch, 1592.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:49,134] [train step24130] D loss: 0.32947 G loss: 2.25007 (0.045 sec/batch, 1436.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:49,513] [train step24140] D loss: 0.33154 G loss: 2.52617 (0.035 sec/batch, 1804.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:49,907] [train step24150] D loss: 0.32798 G loss: 2.26611 (0.041 sec/batch, 1564.282 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:50,281] [train step24160] D loss: 0.33099 G loss: 2.41495 (0.032 sec/batch, 1977.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:50,683] [train step24171] D loss: 0.32936 G loss: 2.23988 (0.037 sec/batch, 1751.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:51,073] [train step24180] D loss: 0.32912 G loss: 2.43368 (0.038 sec/batch, 1691.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:51,455] [train step24190] D loss: 0.32895 G loss: 2.27891 (0.039 sec/batch, 1651.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:51,854] [train step24201] D loss: 0.32966 G loss: 2.45670 (0.038 sec/batch, 1682.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:52,226] [train step24210] D loss: 0.32838 G loss: 2.24160 (0.036 sec/batch, 1778.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:52,593] [train step24221] D loss: 0.32952 G loss: 2.34554 (0.035 sec/batch, 1831.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:52,968] [train step24230] D loss: 0.32987 G loss: 2.28010 (0.032 sec/batch, 1998.328 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:53,346] [train step24240] D loss: 0.32887 G loss: 2.29730 (0.038 sec/batch, 1687.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:53,749] [train step24251] D loss: 0.32962 G loss: 2.35818 (0.039 sec/batch, 1660.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:54,132] [train step24261] D loss: 0.32934 G loss: 2.32519 (0.034 sec/batch, 1868.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:54,513] [train step24270] D loss: 0.32804 G loss: 2.37002 (0.035 sec/batch, 1846.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:54,892] [train step24281] D loss: 0.33006 G loss: 2.37042 (0.037 sec/batch, 1715.199 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:55,274] [train step24290] D loss: 0.32902 G loss: 2.20814 (0.038 sec/batch, 1682.095 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:55,648] [train step24300] D loss: 0.33017 G loss: 2.27233 (0.036 sec/batch, 1790.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:56,050] [train step24311] D loss: 0.33058 G loss: 2.44984 (0.039 sec/batch, 1645.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:56,432] [train step24321] D loss: 0.33098 G loss: 2.16262 (0.039 sec/batch, 1622.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:56,818] [train step24330] D loss: 0.33174 G loss: 2.56352 (0.046 sec/batch, 1395.108 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:57,193] [train step24340] D loss: 0.32954 G loss: 2.25927 (0.038 sec/batch, 1706.324 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:57,569] [train step24351] D loss: 0.32927 G loss: 2.29243 (0.037 sec/batch, 1726.284 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:57,956] [train step24360] D loss: 0.32893 G loss: 2.27490 (0.041 sec/batch, 1562.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:58,331] [train step24370] D loss: 0.32857 G loss: 2.30452 (0.039 sec/batch, 1657.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:58,709] [train step24380] D loss: 0.32881 G loss: 2.36176 (0.039 sec/batch, 1637.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:59,102] [train step24390] D loss: 0.32910 G loss: 2.25835 (0.037 sec/batch, 1734.370 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:59,482] [train step24400] D loss: 0.32892 G loss: 2.31856 (0.044 sec/batch, 1445.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:24:59,863] [train step24410] D loss: 0.32932 G loss: 2.43951 (0.035 sec/batch, 1843.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:00,264] [train step24420] D loss: 0.32866 G loss: 2.39985 (0.039 sec/batch, 1639.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:00,652] [train step24430] D loss: 0.32909 G loss: 2.41443 (0.040 sec/batch, 1590.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:01,039] [train step24441] D loss: 0.32846 G loss: 2.22637 (0.037 sec/batch, 1737.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:01,425] [train step24450] D loss: 0.32925 G loss: 2.48247 (0.038 sec/batch, 1678.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:01,803] [train step24460] D loss: 0.32840 G loss: 2.33159 (0.039 sec/batch, 1645.692 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:02,203] [train step24471] D loss: 0.32883 G loss: 2.41458 (0.038 sec/batch, 1698.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:02,581] [train step24480] D loss: 0.32766 G loss: 2.29246 (0.039 sec/batch, 1627.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:02,963] [train step24491] D loss: 0.33123 G loss: 2.49436 (0.041 sec/batch, 1562.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:03,355] [train step24501] D loss: 0.32973 G loss: 2.25704 (0.039 sec/batch, 1630.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:03,732] [train step24510] D loss: 0.32824 G loss: 2.39527 (0.037 sec/batch, 1734.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:04,121] [train step24521] D loss: 0.32865 G loss: 2.35539 (0.037 sec/batch, 1734.920 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:04,513] [train step24531] D loss: 0.33013 G loss: 2.51909 (0.037 sec/batch, 1716.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:04,905] [train step24540] D loss: 0.32798 G loss: 2.29399 (0.042 sec/batch, 1515.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:05,296] [train step24550] D loss: 0.32920 G loss: 2.27638 (0.036 sec/batch, 1767.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:05,697] [train step24560] D loss: 0.32973 G loss: 2.40189 (0.045 sec/batch, 1413.264 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:06,097] [train step24570] D loss: 0.32962 G loss: 2.39292 (0.045 sec/batch, 1433.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:06,494] [train step24581] D loss: 0.32870 G loss: 2.41434 (0.038 sec/batch, 1701.112 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:06,888] [train step24590] D loss: 0.32762 G loss: 2.28475 (0.033 sec/batch, 1929.455 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:07,287] [train step24600] D loss: 0.32787 G loss: 2.39793 (0.038 sec/batch, 1690.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:07,685] [train step24610] D loss: 0.32921 G loss: 2.41375 (0.049 sec/batch, 1293.690 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:08,065] [train step24620] D loss: 0.32925 G loss: 2.31773 (0.037 sec/batch, 1710.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:08,467] [train step24630] D loss: 0.32846 G loss: 2.32622 (0.038 sec/batch, 1684.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:08,849] [train step24640] D loss: 0.32819 G loss: 2.31217 (0.037 sec/batch, 1710.041 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:09,244] [train step24650] D loss: 0.32830 G loss: 2.37830 (0.043 sec/batch, 1497.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:09,635] [train step24660] D loss: 0.32912 G loss: 2.23640 (0.039 sec/batch, 1649.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:10,024] [train step24670] D loss: 0.32945 G loss: 2.42937 (0.040 sec/batch, 1591.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:10,419] [train step24680] D loss: 0.32843 G loss: 2.38301 (0.037 sec/batch, 1711.371 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:10,812] [train step24690] D loss: 0.32839 G loss: 2.35599 (0.036 sec/batch, 1794.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:11,211] [train step24701] D loss: 0.32836 G loss: 2.36361 (0.046 sec/batch, 1397.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:11,605] [train step24711] D loss: 0.32764 G loss: 2.33817 (0.042 sec/batch, 1533.312 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:11,992] [train step24720] D loss: 0.32906 G loss: 2.29996 (0.041 sec/batch, 1567.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:12,384] [train step24730] D loss: 0.32796 G loss: 2.25150 (0.038 sec/batch, 1671.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:12,774] [train step24741] D loss: 0.32929 G loss: 2.40922 (0.035 sec/batch, 1837.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:13,176] [train step24750] D loss: 0.33068 G loss: 2.22827 (0.036 sec/batch, 1763.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:13,570] [train step24761] D loss: 0.32810 G loss: 2.39798 (0.035 sec/batch, 1814.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:13,953] [train step24770] D loss: 0.33317 G loss: 2.63285 (0.036 sec/batch, 1779.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:14,351] [train step24780] D loss: 0.32791 G loss: 2.24834 (0.041 sec/batch, 1551.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:14,740] [train step24791] D loss: 0.32842 G loss: 2.32241 (0.036 sec/batch, 1757.985 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:15,125] [train step24800] D loss: 0.32731 G loss: 2.26495 (0.042 sec/batch, 1532.962 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:15,511] [train step24810] D loss: 0.32889 G loss: 2.38384 (0.040 sec/batch, 1605.533 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:15,904] [train step24821] D loss: 0.32798 G loss: 2.31247 (0.041 sec/batch, 1550.888 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:16,291] [train step24831] D loss: 0.33074 G loss: 2.57688 (0.037 sec/batch, 1753.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:16,692] [train step24840] D loss: 0.32846 G loss: 2.23414 (0.039 sec/batch, 1646.126 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:17,079] [train step24851] D loss: 0.32896 G loss: 2.47367 (0.038 sec/batch, 1669.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:17,473] [train step24861] D loss: 0.32765 G loss: 2.28670 (0.036 sec/batch, 1771.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:17,865] [train step24870] D loss: 0.32880 G loss: 2.41588 (0.046 sec/batch, 1386.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:18,244] [train step24881] D loss: 0.32875 G loss: 2.27490 (0.039 sec/batch, 1641.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:18,633] [train step24890] D loss: 0.32768 G loss: 2.34765 (0.040 sec/batch, 1582.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:19,018] [train step24900] D loss: 0.32849 G loss: 2.24447 (0.039 sec/batch, 1639.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:19,395] [train step24911] D loss: 0.32717 G loss: 2.37824 (0.034 sec/batch, 1886.631 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:19,799] [train step24921] D loss: 0.32854 G loss: 2.41463 (0.037 sec/batch, 1737.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:20,185] [train step24930] D loss: 0.32810 G loss: 2.33757 (0.037 sec/batch, 1711.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:20,585] [train step24941] D loss: 0.32898 G loss: 2.46849 (0.039 sec/batch, 1650.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:20,973] [train step24951] D loss: 0.32923 G loss: 2.18281 (0.038 sec/batch, 1706.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:21,352] [train step24960] D loss: 0.33152 G loss: 2.56251 (0.035 sec/batch, 1826.191 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:21,755] [train step24971] D loss: 0.32830 G loss: 2.31842 (0.035 sec/batch, 1819.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:22,136] [train step24980] D loss: 0.32761 G loss: 2.30539 (0.038 sec/batch, 1703.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:22,520] [train step24990] D loss: 0.32758 G loss: 2.29836 (0.046 sec/batch, 1393.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:22,887] [train step25000] D loss: 0.33046 G loss: 2.17731 (0.038 sec/batch, 1694.273 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:25:22,888] Saved checkpoint at 25000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:23,476] [train step25011] D loss: 0.32797 G loss: 2.35053 (0.041 sec/batch, 1571.857 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:23,861] [train step25020] D loss: 0.32870 G loss: 2.33223 (0.038 sec/batch, 1670.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:24,234] [train step25030] D loss: 0.32993 G loss: 2.15823 (0.037 sec/batch, 1744.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:24,621] [train step25040] D loss: 0.32770 G loss: 2.30533 (0.036 sec/batch, 1797.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:25,011] [train step25050] D loss: 0.32938 G loss: 2.47658 (0.037 sec/batch, 1715.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:25,394] [train step25060] D loss: 0.32904 G loss: 2.34586 (0.045 sec/batch, 1425.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:25,788] [train step25071] D loss: 0.33369 G loss: 2.12116 (0.037 sec/batch, 1750.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:26,181] [train step25080] D loss: 0.35404 G loss: 3.09933 (0.040 sec/batch, 1598.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:26,562] [train step25090] D loss: 0.36766 G loss: 3.33600 (0.037 sec/batch, 1721.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:26,942] [train step25101] D loss: 0.32842 G loss: 2.33290 (0.035 sec/batch, 1842.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:27,316] [train step25110] D loss: 0.36324 G loss: 3.25308 (0.037 sec/batch, 1715.473 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:27,695] [train step25121] D loss: 0.46220 G loss: 4.51489 (0.035 sec/batch, 1807.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:28,078] [train step25130] D loss: 0.39670 G loss: 1.41853 (0.035 sec/batch, 1827.285 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:28,458] [train step25140] D loss: 0.40154 G loss: 3.80331 (0.038 sec/batch, 1706.356 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:28,839] [train step25151] D loss: 0.34891 G loss: 1.78589 (0.037 sec/batch, 1742.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:29,220] [train step25161] D loss: 0.33332 G loss: 2.67635 (0.040 sec/batch, 1616.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:29,603] [train step25170] D loss: 0.32829 G loss: 2.29261 (0.049 sec/batch, 1299.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:29,995] [train step25181] D loss: 0.32926 G loss: 2.49154 (0.036 sec/batch, 1756.662 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:30,376] [train step25191] D loss: 0.33006 G loss: 2.18875 (0.037 sec/batch, 1751.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:30,757] [train step25200] D loss: 0.32779 G loss: 2.28493 (0.040 sec/batch, 1598.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:31,143] [train step25210] D loss: 0.32702 G loss: 2.27823 (0.034 sec/batch, 1862.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:31,539] [train step25221] D loss: 0.32746 G loss: 2.38095 (0.034 sec/batch, 1892.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:31,930] [train step25230] D loss: 0.32751 G loss: 2.32579 (0.037 sec/batch, 1722.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:32,309] [train step25241] D loss: 0.32796 G loss: 2.34883 (0.039 sec/batch, 1621.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:32,689] [train step25251] D loss: 0.32766 G loss: 2.29512 (0.035 sec/batch, 1834.703 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:33,075] [train step25260] D loss: 0.32890 G loss: 2.44972 (0.034 sec/batch, 1891.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:33,462] [train step25270] D loss: 0.32743 G loss: 2.22425 (0.039 sec/batch, 1641.285 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:33,851] [train step25280] D loss: 0.32722 G loss: 2.31658 (0.046 sec/batch, 1386.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:34,256] [train step25290] D loss: 0.32813 G loss: 2.29112 (0.037 sec/batch, 1717.657 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:34,633] [train step25300] D loss: 0.32741 G loss: 2.26116 (0.036 sec/batch, 1799.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:35,023] [train step25311] D loss: 0.32813 G loss: 2.40140 (0.040 sec/batch, 1616.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:35,411] [train step25320] D loss: 0.32796 G loss: 2.24576 (0.043 sec/batch, 1500.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:35,795] [train step25330] D loss: 0.32727 G loss: 2.36171 (0.034 sec/batch, 1871.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:36,190] [train step25341] D loss: 0.32751 G loss: 2.29036 (0.036 sec/batch, 1764.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:36,569] [train step25350] D loss: 0.32834 G loss: 2.43211 (0.037 sec/batch, 1745.661 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:36,961] [train step25360] D loss: 0.32720 G loss: 2.23365 (0.041 sec/batch, 1562.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:37,343] [train step25371] D loss: 0.32737 G loss: 2.31505 (0.035 sec/batch, 1843.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:37,718] [train step25380] D loss: 0.32700 G loss: 2.29285 (0.037 sec/batch, 1728.329 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:38,114] [train step25391] D loss: 0.32770 G loss: 2.38587 (0.036 sec/batch, 1773.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:38,499] [train step25401] D loss: 0.32768 G loss: 2.28210 (0.037 sec/batch, 1710.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:38,875] [train step25410] D loss: 0.32742 G loss: 2.27373 (0.039 sec/batch, 1655.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:39,265] [train step25420] D loss: 0.32738 G loss: 2.31832 (0.034 sec/batch, 1875.191 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:39,648] [train step25431] D loss: 0.32851 G loss: 2.22947 (0.036 sec/batch, 1768.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:40,041] [train step25440] D loss: 0.32763 G loss: 2.22814 (0.038 sec/batch, 1668.897 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:40,424] [train step25450] D loss: 0.32801 G loss: 2.36616 (0.038 sec/batch, 1684.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:40,815] [train step25460] D loss: 0.32720 G loss: 2.28624 (0.038 sec/batch, 1663.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:41,204] [train step25470] D loss: 0.32769 G loss: 2.30013 (0.037 sec/batch, 1716.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:41,586] [train step25480] D loss: 0.32793 G loss: 2.35455 (0.041 sec/batch, 1578.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:41,968] [train step25491] D loss: 0.32765 G loss: 2.30313 (0.044 sec/batch, 1470.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:42,357] [train step25500] D loss: 0.32715 G loss: 2.35694 (0.041 sec/batch, 1579.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:42,739] [train step25511] D loss: 0.32752 G loss: 2.27437 (0.038 sec/batch, 1696.703 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:43,127] [train step25521] D loss: 0.32746 G loss: 2.40627 (0.038 sec/batch, 1703.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:43,508] [train step25530] D loss: 0.32730 G loss: 2.27181 (0.039 sec/batch, 1642.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:43,886] [train step25541] D loss: 0.32749 G loss: 2.40745 (0.037 sec/batch, 1751.184 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:44,281] [train step25551] D loss: 0.32691 G loss: 2.31440 (0.035 sec/batch, 1808.876 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:44,673] [train step25560] D loss: 0.32692 G loss: 2.26932 (0.040 sec/batch, 1613.320 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:45,070] [train step25570] D loss: 0.32716 G loss: 2.28667 (0.038 sec/batch, 1690.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:45,465] [train step25580] D loss: 0.32698 G loss: 2.37220 (0.040 sec/batch, 1615.981 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:45,860] [train step25590] D loss: 0.32701 G loss: 2.29524 (0.041 sec/batch, 1547.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:46,249] [train step25601] D loss: 0.32718 G loss: 2.38587 (0.034 sec/batch, 1867.338 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:46,631] [train step25611] D loss: 0.32693 G loss: 2.36479 (0.038 sec/batch, 1685.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:47,018] [train step25620] D loss: 0.32769 G loss: 2.38133 (0.040 sec/batch, 1613.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:47,407] [train step25631] D loss: 0.32826 G loss: 2.42434 (0.036 sec/batch, 1775.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:47,793] [train step25640] D loss: 0.32739 G loss: 2.26299 (0.039 sec/batch, 1652.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:48,176] [train step25650] D loss: 0.32739 G loss: 2.37251 (0.037 sec/batch, 1739.867 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:48,577] [train step25660] D loss: 0.32736 G loss: 2.31766 (0.051 sec/batch, 1258.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:48,967] [train step25671] D loss: 0.32713 G loss: 2.29703 (0.043 sec/batch, 1478.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:49,355] [train step25680] D loss: 0.32753 G loss: 2.25090 (0.032 sec/batch, 2020.606 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:49,745] [train step25690] D loss: 0.32707 G loss: 2.33196 (0.044 sec/batch, 1453.871 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:50,151] [train step25700] D loss: 0.32816 G loss: 2.40678 (0.036 sec/batch, 1756.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:50,547] [train step25710] D loss: 0.32781 G loss: 2.30209 (0.033 sec/batch, 1928.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:50,938] [train step25721] D loss: 0.32684 G loss: 2.35897 (0.037 sec/batch, 1748.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:51,330] [train step25731] D loss: 0.32690 G loss: 2.35029 (0.045 sec/batch, 1416.411 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:51,725] [train step25740] D loss: 0.32804 G loss: 2.38657 (0.039 sec/batch, 1626.487 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:52,109] [train step25751] D loss: 0.32745 G loss: 2.30085 (0.036 sec/batch, 1761.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:52,488] [train step25761] D loss: 0.32747 G loss: 2.36576 (0.034 sec/batch, 1900.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:52,862] [train step25770] D loss: 0.32812 G loss: 2.36411 (0.038 sec/batch, 1686.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:53,236] [train step25781] D loss: 0.32938 G loss: 2.51775 (0.038 sec/batch, 1705.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:53,612] [train step25791] D loss: 0.32726 G loss: 2.25133 (0.031 sec/batch, 2079.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:53,991] [train step25800] D loss: 0.32698 G loss: 2.30346 (0.041 sec/batch, 1561.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:54,369] [train step25811] D loss: 0.32682 G loss: 2.27205 (0.044 sec/batch, 1468.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:54,748] [train step25821] D loss: 0.32771 G loss: 2.31571 (0.039 sec/batch, 1648.431 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:55,132] [train step25830] D loss: 0.32708 G loss: 2.39474 (0.037 sec/batch, 1712.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:55,530] [train step25841] D loss: 0.32692 G loss: 2.27978 (0.036 sec/batch, 1767.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:55,930] [train step25851] D loss: 0.32674 G loss: 2.32964 (0.037 sec/batch, 1749.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:56,315] [train step25860] D loss: 0.32717 G loss: 2.28543 (0.035 sec/batch, 1830.025 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:56,699] [train step25870] D loss: 0.32665 G loss: 2.32850 (0.041 sec/batch, 1579.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:57,078] [train step25880] D loss: 0.32682 G loss: 2.29706 (0.041 sec/batch, 1548.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:57,467] [train step25890] D loss: 0.32763 G loss: 2.42551 (0.045 sec/batch, 1426.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:57,843] [train step25901] D loss: 0.32679 G loss: 2.35855 (0.035 sec/batch, 1844.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:58,221] [train step25910] D loss: 0.32759 G loss: 2.28847 (0.035 sec/batch, 1828.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:58,611] [train step25920] D loss: 0.32738 G loss: 2.39457 (0.031 sec/batch, 2082.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:59,003] [train step25931] D loss: 0.32817 G loss: 2.45621 (0.036 sec/batch, 1794.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:59,388] [train step25940] D loss: 0.32895 G loss: 2.14484 (0.035 sec/batch, 1810.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:25:59,781] [train step25950] D loss: 0.32736 G loss: 2.39333 (0.042 sec/batch, 1534.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:00,163] [train step25960] D loss: 0.32711 G loss: 2.24488 (0.035 sec/batch, 1842.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:00,559] [train step25971] D loss: 0.32700 G loss: 2.37793 (0.046 sec/batch, 1381.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:00,932] [train step25980] D loss: 0.32745 G loss: 2.28675 (0.038 sec/batch, 1706.053 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:01,315] [train step25991] D loss: 0.32686 G loss: 2.26247 (0.043 sec/batch, 1505.797 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:01,696] [train step26000] D loss: 0.32648 G loss: 2.27304 (0.035 sec/batch, 1823.499 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:26:01,697] Saved checkpoint at 26000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:02,279] [train step26010] D loss: 0.32783 G loss: 2.18242 (0.043 sec/batch, 1490.133 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:02,670] [train step26021] D loss: 0.32681 G loss: 2.29975 (0.036 sec/batch, 1791.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:03,047] [train step26031] D loss: 0.32731 G loss: 2.40748 (0.035 sec/batch, 1851.419 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:03,436] [train step26040] D loss: 0.32658 G loss: 2.22624 (0.038 sec/batch, 1699.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:03,833] [train step26050] D loss: 0.32683 G loss: 2.32678 (0.038 sec/batch, 1679.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:04,214] [train step26061] D loss: 0.32805 G loss: 2.45793 (0.037 sec/batch, 1708.018 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:04,589] [train step26070] D loss: 0.32728 G loss: 2.24897 (0.032 sec/batch, 2010.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:04,988] [train step26080] D loss: 0.32722 G loss: 2.40619 (0.040 sec/batch, 1583.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:05,362] [train step26090] D loss: 0.32709 G loss: 2.25046 (0.038 sec/batch, 1673.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:05,758] [train step26100] D loss: 0.32648 G loss: 2.35365 (0.037 sec/batch, 1749.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:06,160] [train step26111] D loss: 0.32680 G loss: 2.42485 (0.044 sec/batch, 1453.312 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:06,540] [train step26120] D loss: 0.32700 G loss: 2.33282 (0.038 sec/batch, 1706.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:06,932] [train step26130] D loss: 0.32707 G loss: 2.37712 (0.040 sec/batch, 1600.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:07,320] [train step26141] D loss: 0.32710 G loss: 2.24714 (0.040 sec/batch, 1606.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:07,695] [train step26151] D loss: 0.32705 G loss: 2.26168 (0.040 sec/batch, 1585.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:08,078] [train step26160] D loss: 0.32652 G loss: 2.35858 (0.033 sec/batch, 1924.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:08,464] [train step26171] D loss: 0.32726 G loss: 2.37182 (0.038 sec/batch, 1706.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:08,858] [train step26181] D loss: 0.32691 G loss: 2.27799 (0.040 sec/batch, 1614.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:09,247] [train step26190] D loss: 0.32832 G loss: 2.47512 (0.039 sec/batch, 1623.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:09,625] [train step26201] D loss: 0.32795 G loss: 2.26118 (0.041 sec/batch, 1568.431 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:10,013] [train step26210] D loss: 0.32636 G loss: 2.29785 (0.039 sec/batch, 1648.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:10,395] [train step26220] D loss: 0.32702 G loss: 2.31631 (0.038 sec/batch, 1663.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:10,775] [train step26231] D loss: 0.32677 G loss: 2.25455 (0.039 sec/batch, 1650.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:11,168] [train step26241] D loss: 0.32640 G loss: 2.33423 (0.038 sec/batch, 1674.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:11,549] [train step26250] D loss: 0.32666 G loss: 2.32516 (0.039 sec/batch, 1648.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:11,943] [train step26261] D loss: 0.32656 G loss: 2.35456 (0.035 sec/batch, 1845.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:12,323] [train step26270] D loss: 0.32652 G loss: 2.25803 (0.039 sec/batch, 1662.016 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:12,708] [train step26280] D loss: 0.32696 G loss: 2.37751 (0.040 sec/batch, 1584.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:13,107] [train step26290] D loss: 0.32700 G loss: 2.31996 (0.040 sec/batch, 1591.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:13,482] [train step26301] D loss: 0.32658 G loss: 2.29689 (0.036 sec/batch, 1783.719 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:13,864] [train step26310] D loss: 0.32693 G loss: 2.25116 (0.036 sec/batch, 1799.913 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:14,252] [train step26321] D loss: 0.32670 G loss: 2.35031 (0.037 sec/batch, 1751.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:14,636] [train step26331] D loss: 0.32652 G loss: 2.37375 (0.037 sec/batch, 1740.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:15,021] [train step26340] D loss: 0.32722 G loss: 2.26434 (0.040 sec/batch, 1602.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:15,409] [train step26350] D loss: 0.32731 G loss: 2.39401 (0.039 sec/batch, 1637.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:15,796] [train step26361] D loss: 0.32665 G loss: 2.27756 (0.042 sec/batch, 1505.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:16,188] [train step26370] D loss: 0.32704 G loss: 2.40912 (0.044 sec/batch, 1443.822 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:16,571] [train step26381] D loss: 0.32711 G loss: 2.28478 (0.037 sec/batch, 1721.722 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:16,973] [train step26390] D loss: 0.32672 G loss: 2.30704 (0.042 sec/batch, 1514.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:17,357] [train step26400] D loss: 0.32669 G loss: 2.30004 (0.037 sec/batch, 1751.984 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:17,746] [train step26410] D loss: 0.32640 G loss: 2.30444 (0.037 sec/batch, 1736.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:18,145] [train step26420] D loss: 0.32718 G loss: 2.36820 (0.036 sec/batch, 1778.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:18,524] [train step26430] D loss: 0.32652 G loss: 2.34145 (0.036 sec/batch, 1766.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:18,906] [train step26441] D loss: 0.32666 G loss: 2.31221 (0.037 sec/batch, 1715.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:19,295] [train step26451] D loss: 0.32700 G loss: 2.34459 (0.035 sec/batch, 1812.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:19,679] [train step26460] D loss: 0.32677 G loss: 2.32703 (0.035 sec/batch, 1852.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:20,072] [train step26471] D loss: 0.32675 G loss: 2.35824 (0.047 sec/batch, 1374.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:20,459] [train step26480] D loss: 0.32657 G loss: 2.25536 (0.044 sec/batch, 1459.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:20,845] [train step26490] D loss: 0.32734 G loss: 2.45797 (0.044 sec/batch, 1466.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:21,231] [train step26501] D loss: 0.32629 G loss: 2.30279 (0.035 sec/batch, 1824.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:21,624] [train step26510] D loss: 0.32651 G loss: 2.28064 (0.039 sec/batch, 1653.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:22,018] [train step26520] D loss: 0.32611 G loss: 2.28776 (0.040 sec/batch, 1589.100 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:22,419] [train step26531] D loss: 0.32673 G loss: 2.33450 (0.042 sec/batch, 1534.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:22,785] [train step26540] D loss: 0.32708 G loss: 2.21102 (0.041 sec/batch, 1543.495 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:23,165] [train step26550] D loss: 0.32826 G loss: 2.47819 (0.049 sec/batch, 1308.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:23,540] [train step26560] D loss: 0.32708 G loss: 2.38235 (0.038 sec/batch, 1696.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:23,917] [train step26570] D loss: 0.32660 G loss: 2.25474 (0.037 sec/batch, 1720.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:24,295] [train step26580] D loss: 0.32668 G loss: 2.32936 (0.034 sec/batch, 1878.314 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:24,677] [train step26590] D loss: 0.32655 G loss: 2.33504 (0.037 sec/batch, 1707.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:25,051] [train step26600] D loss: 0.32663 G loss: 2.35316 (0.031 sec/batch, 2076.163 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:25,453] [train step26610] D loss: 0.32703 G loss: 2.28193 (0.036 sec/batch, 1795.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:25,825] [train step26621] D loss: 0.32654 G loss: 2.27127 (0.036 sec/batch, 1790.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:26,206] [train step26630] D loss: 0.32713 G loss: 2.41198 (0.035 sec/batch, 1813.349 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:26,601] [train step26640] D loss: 0.32698 G loss: 2.21453 (0.035 sec/batch, 1823.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:26,976] [train step26650] D loss: 0.32633 G loss: 2.37178 (0.038 sec/batch, 1686.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:27,360] [train step26660] D loss: 0.32654 G loss: 2.33166 (0.037 sec/batch, 1751.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:27,745] [train step26670] D loss: 0.32773 G loss: 2.36121 (0.034 sec/batch, 1857.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:28,126] [train step26681] D loss: 0.32710 G loss: 2.42488 (0.036 sec/batch, 1771.699 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:28,507] [train step26691] D loss: 0.32652 G loss: 2.34906 (0.034 sec/batch, 1868.391 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:28,883] [train step26700] D loss: 0.32699 G loss: 2.36028 (0.037 sec/batch, 1747.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:29,257] [train step26711] D loss: 0.32685 G loss: 2.30591 (0.034 sec/batch, 1904.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:29,641] [train step26721] D loss: 0.32665 G loss: 2.32451 (0.036 sec/batch, 1788.950 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:30,015] [train step26730] D loss: 0.32712 G loss: 2.22429 (0.039 sec/batch, 1660.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:30,407] [train step26741] D loss: 0.32767 G loss: 2.19959 (0.035 sec/batch, 1854.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:30,790] [train step26751] D loss: 0.32693 G loss: 2.30278 (0.042 sec/batch, 1530.881 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:31,163] [train step26760] D loss: 0.32724 G loss: 2.32647 (0.036 sec/batch, 1775.449 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:31,548] [train step26770] D loss: 0.32681 G loss: 2.31564 (0.037 sec/batch, 1734.920 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:31,936] [train step26780] D loss: 0.32924 G loss: 2.53562 (0.039 sec/batch, 1623.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:32,313] [train step26790] D loss: 0.32774 G loss: 2.18523 (0.041 sec/batch, 1546.065 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:32,699] [train step26801] D loss: 0.32671 G loss: 2.38057 (0.039 sec/batch, 1639.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:33,075] [train step26811] D loss: 0.32967 G loss: 2.54832 (0.037 sec/batch, 1724.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:33,460] [train step26820] D loss: 0.33098 G loss: 2.07263 (0.041 sec/batch, 1575.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:33,848] [train step26830] D loss: 0.32964 G loss: 2.13213 (0.038 sec/batch, 1687.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:34,232] [train step26841] D loss: 0.32857 G loss: 2.17019 (0.037 sec/batch, 1733.329 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:34,617] [train step26850] D loss: 0.32983 G loss: 2.54209 (0.032 sec/batch, 1999.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:34,994] [train step26861] D loss: 0.32837 G loss: 2.48784 (0.038 sec/batch, 1690.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:35,383] [train step26870] D loss: 0.32734 G loss: 2.37513 (0.041 sec/batch, 1561.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:35,773] [train step26880] D loss: 0.32704 G loss: 2.29141 (0.038 sec/batch, 1688.539 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:36,202] [train step26891] D loss: 0.32866 G loss: 2.15666 (0.036 sec/batch, 1772.366 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:36,621] [train step26900] D loss: 0.32788 G loss: 2.45252 (0.042 sec/batch, 1537.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:37,011] [train step26910] D loss: 0.32688 G loss: 2.32098 (0.038 sec/batch, 1695.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:37,401] [train step26920] D loss: 0.32850 G loss: 2.47170 (0.039 sec/batch, 1655.129 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:37,793] [train step26930] D loss: 0.32718 G loss: 2.25080 (0.038 sec/batch, 1689.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:38,201] [train step26940] D loss: 0.32760 G loss: 2.40908 (0.036 sec/batch, 1757.985 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:38,600] [train step26951] D loss: 0.32705 G loss: 2.33666 (0.044 sec/batch, 1458.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:38,989] [train step26960] D loss: 0.32796 G loss: 2.19938 (0.036 sec/batch, 1795.231 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:39,385] [train step26970] D loss: 0.32700 G loss: 2.36626 (0.042 sec/batch, 1525.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:39,785] [train step26981] D loss: 0.32744 G loss: 2.20535 (0.039 sec/batch, 1654.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:40,177] [train step26991] D loss: 0.32728 G loss: 2.26110 (0.042 sec/batch, 1532.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:40,571] [train step27000] D loss: 0.32776 G loss: 2.42626 (0.040 sec/batch, 1588.112 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:26:40,571] Saved checkpoint at 27000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:41,174] [train step27010] D loss: 0.32709 G loss: 2.24997 (0.039 sec/batch, 1625.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:41,572] [train step27021] D loss: 0.32969 G loss: 2.10910 (0.040 sec/batch, 1602.983 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:41,968] [train step27030] D loss: 0.33177 G loss: 2.63863 (0.037 sec/batch, 1717.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:42,356] [train step27041] D loss: 0.32769 G loss: 2.19316 (0.036 sec/batch, 1761.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:42,752] [train step27051] D loss: 0.32692 G loss: 2.26617 (0.036 sec/batch, 1762.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:43,155] [train step27060] D loss: 0.32675 G loss: 2.29984 (0.039 sec/batch, 1623.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:43,541] [train step27070] D loss: 0.32669 G loss: 2.31733 (0.036 sec/batch, 1756.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:43,952] [train step27081] D loss: 0.32683 G loss: 2.32294 (0.038 sec/batch, 1683.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:44,344] [train step27090] D loss: 0.32700 G loss: 2.30721 (0.038 sec/batch, 1696.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:44,747] [train step27101] D loss: 0.32702 G loss: 2.27422 (0.047 sec/batch, 1366.633 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:45,138] [train step27110] D loss: 0.32707 G loss: 2.27259 (0.039 sec/batch, 1653.101 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:45,531] [train step27120] D loss: 0.32690 G loss: 2.26812 (0.040 sec/batch, 1595.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:45,926] [train step27131] D loss: 0.32720 G loss: 2.39353 (0.037 sec/batch, 1731.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:46,318] [train step27140] D loss: 0.32678 G loss: 2.25041 (0.036 sec/batch, 1773.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:46,712] [train step27150] D loss: 0.32643 G loss: 2.35219 (0.036 sec/batch, 1760.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:47,109] [train step27160] D loss: 0.32671 G loss: 2.29147 (0.037 sec/batch, 1732.568 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:47,485] [train step27171] D loss: 0.32712 G loss: 2.27703 (0.041 sec/batch, 1556.915 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:47,886] [train step27180] D loss: 0.32739 G loss: 2.38225 (0.039 sec/batch, 1646.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:48,268] [train step27191] D loss: 0.32738 G loss: 2.42889 (0.040 sec/batch, 1588.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:48,660] [train step27200] D loss: 0.32680 G loss: 2.36279 (0.035 sec/batch, 1813.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:49,058] [train step27210] D loss: 0.32712 G loss: 2.32316 (0.044 sec/batch, 1456.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:49,440] [train step27220] D loss: 0.32640 G loss: 2.39399 (0.038 sec/batch, 1699.765 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:49,829] [train step27230] D loss: 0.32694 G loss: 2.34450 (0.037 sec/batch, 1748.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:50,216] [train step27240] D loss: 0.32673 G loss: 2.31594 (0.035 sec/batch, 1824.143 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:50,607] [train step27250] D loss: 0.32696 G loss: 2.38843 (0.035 sec/batch, 1826.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:50,997] [train step27261] D loss: 0.32713 G loss: 2.42915 (0.034 sec/batch, 1863.863 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:51,382] [train step27270] D loss: 0.32671 G loss: 2.28453 (0.046 sec/batch, 1402.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:51,770] [train step27280] D loss: 0.32722 G loss: 2.18444 (0.043 sec/batch, 1493.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:52,157] [train step27291] D loss: 0.32644 G loss: 2.33361 (0.035 sec/batch, 1809.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:52,545] [train step27300] D loss: 0.32680 G loss: 2.39315 (0.040 sec/batch, 1592.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:52,939] [train step27311] D loss: 0.32609 G loss: 2.28278 (0.052 sec/batch, 1220.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:53,318] [train step27320] D loss: 0.32715 G loss: 2.42403 (0.036 sec/batch, 1757.800 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:53,680] [train step27330] D loss: 0.32649 G loss: 2.29593 (0.026 sec/batch, 2484.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:54,073] [train step27341] D loss: 0.32672 G loss: 2.37730 (0.040 sec/batch, 1618.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:54,452] [train step27350] D loss: 0.32649 G loss: 2.30599 (0.039 sec/batch, 1621.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:54,830] [train step27360] D loss: 0.32646 G loss: 2.30328 (0.041 sec/batch, 1543.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:55,213] [train step27371] D loss: 0.32679 G loss: 2.30151 (0.037 sec/batch, 1724.554 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:55,580] [train step27381] D loss: 0.32636 G loss: 2.30230 (0.033 sec/batch, 1952.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:55,957] [train step27390] D loss: 0.32657 G loss: 2.24438 (0.035 sec/batch, 1807.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:56,352] [train step27400] D loss: 0.32707 G loss: 2.20907 (0.047 sec/batch, 1363.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:56,723] [train step27410] D loss: 0.32671 G loss: 2.38352 (0.034 sec/batch, 1893.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:57,111] [train step27420] D loss: 0.32704 G loss: 2.21789 (0.036 sec/batch, 1776.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:57,493] [train step27431] D loss: 0.32662 G loss: 2.28735 (0.042 sec/batch, 1529.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:57,865] [train step27441] D loss: 0.32789 G loss: 2.48342 (0.035 sec/batch, 1806.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:58,251] [train step27450] D loss: 0.32710 G loss: 2.19682 (0.037 sec/batch, 1737.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:58,620] [train step27460] D loss: 0.32616 G loss: 2.27944 (0.035 sec/batch, 1833.100 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:59,001] [train step27470] D loss: 0.32622 G loss: 2.25770 (0.039 sec/batch, 1650.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:59,398] [train step27480] D loss: 0.32636 G loss: 2.33451 (0.038 sec/batch, 1667.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:26:59,776] [train step27491] D loss: 0.32634 G loss: 2.32817 (0.035 sec/batch, 1816.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:00,159] [train step27501] D loss: 0.32628 G loss: 2.34057 (0.038 sec/batch, 1695.407 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:00,540] [train step27510] D loss: 0.32616 G loss: 2.32714 (0.037 sec/batch, 1718.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:00,913] [train step27521] D loss: 0.32671 G loss: 2.25429 (0.039 sec/batch, 1621.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:01,299] [train step27531] D loss: 0.32883 G loss: 2.53195 (0.035 sec/batch, 1843.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:01,673] [train step27540] D loss: 0.32719 G loss: 2.18777 (0.038 sec/batch, 1674.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:02,052] [train step27551] D loss: 0.32652 G loss: 2.28317 (0.041 sec/batch, 1573.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:02,435] [train step27561] D loss: 0.32780 G loss: 2.14755 (0.039 sec/batch, 1656.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:02,822] [train step27570] D loss: 0.32843 G loss: 2.50613 (0.038 sec/batch, 1683.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:03,207] [train step27581] D loss: 0.32672 G loss: 2.29597 (0.041 sec/batch, 1562.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:03,588] [train step27591] D loss: 0.32660 G loss: 2.24651 (0.042 sec/batch, 1511.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:03,970] [train step27600] D loss: 0.32650 G loss: 2.26571 (0.034 sec/batch, 1871.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:04,367] [train step27611] D loss: 0.32635 G loss: 2.36943 (0.036 sec/batch, 1787.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:04,751] [train step27620] D loss: 0.32743 G loss: 2.46301 (0.038 sec/batch, 1676.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:05,137] [train step27630] D loss: 0.32629 G loss: 2.30111 (0.039 sec/batch, 1627.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:05,521] [train step27640] D loss: 0.32727 G loss: 2.46379 (0.034 sec/batch, 1882.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:05,899] [train step27651] D loss: 0.32622 G loss: 2.33947 (0.036 sec/batch, 1799.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:06,288] [train step27660] D loss: 0.32651 G loss: 2.30774 (0.044 sec/batch, 1461.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:06,674] [train step27671] D loss: 0.32593 G loss: 2.33425 (0.041 sec/batch, 1560.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:07,063] [train step27680] D loss: 0.32622 G loss: 2.33419 (0.039 sec/batch, 1643.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:07,457] [train step27690] D loss: 0.32753 G loss: 2.16324 (0.039 sec/batch, 1656.631 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:07,840] [train step27701] D loss: 0.32630 G loss: 2.26076 (0.036 sec/batch, 1760.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:08,220] [train step27710] D loss: 0.32615 G loss: 2.34768 (0.028 sec/batch, 2307.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:08,615] [train step27720] D loss: 0.32699 G loss: 2.22909 (0.035 sec/batch, 1852.096 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:09,005] [train step27731] D loss: 0.32672 G loss: 2.35038 (0.040 sec/batch, 1583.391 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:09,400] [train step27740] D loss: 0.32622 G loss: 2.31084 (0.046 sec/batch, 1387.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:09,785] [train step27750] D loss: 0.32721 G loss: 2.38253 (0.045 sec/batch, 1425.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:10,192] [train step27761] D loss: 0.32652 G loss: 2.32319 (0.040 sec/batch, 1584.915 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:10,579] [train step27770] D loss: 0.32646 G loss: 2.22383 (0.037 sec/batch, 1741.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:10,978] [train step27780] D loss: 0.32644 G loss: 2.32446 (0.036 sec/batch, 1793.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:11,358] [train step27790] D loss: 0.32635 G loss: 2.35039 (0.035 sec/batch, 1838.423 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:11,757] [train step27801] D loss: 0.32673 G loss: 2.42177 (0.040 sec/batch, 1597.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:12,153] [train step27810] D loss: 0.32645 G loss: 2.26875 (0.038 sec/batch, 1692.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:12,554] [train step27821] D loss: 0.32613 G loss: 2.32689 (0.042 sec/batch, 1537.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:12,942] [train step27831] D loss: 0.32724 G loss: 2.22021 (0.036 sec/batch, 1774.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:13,332] [train step27840] D loss: 0.32639 G loss: 2.36298 (0.037 sec/batch, 1731.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:13,723] [train step27850] D loss: 0.32660 G loss: 2.20391 (0.038 sec/batch, 1691.699 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:14,103] [train step27861] D loss: 0.32630 G loss: 2.28323 (0.034 sec/batch, 1908.983 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:14,485] [train step27870] D loss: 0.32627 G loss: 2.28873 (0.035 sec/batch, 1803.614 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:14,878] [train step27881] D loss: 0.32660 G loss: 2.33059 (0.037 sec/batch, 1722.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:15,261] [train step27891] D loss: 0.32588 G loss: 2.29837 (0.033 sec/batch, 1930.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:15,657] [train step27900] D loss: 0.32623 G loss: 2.32805 (0.040 sec/batch, 1615.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:16,038] [train step27910] D loss: 0.32616 G loss: 2.33832 (0.039 sec/batch, 1633.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:16,415] [train step27920] D loss: 0.32632 G loss: 2.28520 (0.028 sec/batch, 2317.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:16,815] [train step27930] D loss: 0.32637 G loss: 2.36481 (0.042 sec/batch, 1517.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:17,195] [train step27940] D loss: 0.32675 G loss: 2.35868 (0.041 sec/batch, 1560.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:17,592] [train step27951] D loss: 0.32636 G loss: 2.33315 (0.046 sec/batch, 1387.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:17,981] [train step27960] D loss: 0.32603 G loss: 2.30199 (0.033 sec/batch, 1910.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:18,371] [train step27971] D loss: 0.32657 G loss: 2.34258 (0.038 sec/batch, 1698.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:18,775] [train step27981] D loss: 0.32640 G loss: 2.25058 (0.041 sec/batch, 1554.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:19,155] [train step27990] D loss: 0.32672 G loss: 2.40319 (0.037 sec/batch, 1740.138 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:19,544] [train step28001] D loss: 0.32625 G loss: 2.33394 (0.045 sec/batch, 1437.967 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:27:19,545] Saved checkpoint at 28000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:20,151] [train step28011] D loss: 0.32674 G loss: 2.23387 (0.039 sec/batch, 1624.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:20,535] [train step28020] D loss: 0.32663 G loss: 2.39561 (0.031 sec/batch, 2053.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:20,928] [train step28031] D loss: 0.32702 G loss: 2.43698 (0.036 sec/batch, 1782.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:21,310] [train step28040] D loss: 0.32634 G loss: 2.23628 (0.037 sec/batch, 1715.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:21,696] [train step28050] D loss: 0.32692 G loss: 2.34417 (0.038 sec/batch, 1696.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:22,078] [train step28061] D loss: 0.32652 G loss: 2.22415 (0.036 sec/batch, 1770.040 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:22,458] [train step28071] D loss: 0.32625 G loss: 2.27060 (0.041 sec/batch, 1578.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:22,857] [train step28080] D loss: 0.32692 G loss: 2.43979 (0.043 sec/batch, 1473.455 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:23,244] [train step28091] D loss: 0.32606 G loss: 2.32535 (0.046 sec/batch, 1389.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:23,610] [train step28100] D loss: 0.32619 G loss: 2.27982 (0.037 sec/batch, 1738.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:23,997] [train step28110] D loss: 0.32650 G loss: 2.35675 (0.039 sec/batch, 1643.526 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:24,371] [train step28121] D loss: 0.32673 G loss: 2.24837 (0.036 sec/batch, 1783.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:24,754] [train step28130] D loss: 0.32627 G loss: 2.32206 (0.043 sec/batch, 1496.112 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:25,136] [train step28140] D loss: 0.32647 G loss: 2.26878 (0.039 sec/batch, 1641.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:25,505] [train step28150] D loss: 0.32660 G loss: 2.28859 (0.039 sec/batch, 1645.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:25,889] [train step28161] D loss: 0.32623 G loss: 2.37127 (0.050 sec/batch, 1289.514 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:26,269] [train step28170] D loss: 0.32618 G loss: 2.33111 (0.037 sec/batch, 1737.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:26,643] [train step28180] D loss: 0.32606 G loss: 2.37476 (0.035 sec/batch, 1847.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:27,032] [train step28190] D loss: 0.32620 G loss: 2.26927 (0.036 sec/batch, 1792.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:27,412] [train step28200] D loss: 0.32635 G loss: 2.33624 (0.038 sec/batch, 1685.337 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:27,786] [train step28211] D loss: 0.32608 G loss: 2.33153 (0.036 sec/batch, 1771.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:28,167] [train step28221] D loss: 0.32645 G loss: 2.38812 (0.035 sec/batch, 1854.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:28,538] [train step28230] D loss: 0.32607 G loss: 2.28289 (0.034 sec/batch, 1860.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:28,916] [train step28241] D loss: 0.32615 G loss: 2.34939 (0.040 sec/batch, 1595.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:29,287] [train step28250] D loss: 0.32603 G loss: 2.36243 (0.030 sec/batch, 2123.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:29,664] [train step28260] D loss: 0.32625 G loss: 2.24591 (0.034 sec/batch, 1865.677 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:30,057] [train step28270] D loss: 0.32633 G loss: 2.31255 (0.037 sec/batch, 1707.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:30,442] [train step28281] D loss: 0.32622 G loss: 2.25995 (0.040 sec/batch, 1595.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:30,811] [train step28290] D loss: 0.32709 G loss: 2.40217 (0.033 sec/batch, 1940.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:31,211] [train step28301] D loss: 0.32599 G loss: 2.34462 (0.034 sec/batch, 1882.371 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:31,599] [train step28311] D loss: 0.32605 G loss: 2.30295 (0.035 sec/batch, 1850.513 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:31,997] [train step28320] D loss: 0.32666 G loss: 2.27173 (0.047 sec/batch, 1359.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:32,373] [train step28331] D loss: 0.32618 G loss: 2.25851 (0.037 sec/batch, 1709.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:32,750] [train step28340] D loss: 0.32632 G loss: 2.32923 (0.032 sec/batch, 2028.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:33,148] [train step28350] D loss: 0.32610 G loss: 2.28412 (0.038 sec/batch, 1686.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:33,525] [train step28360] D loss: 0.32627 G loss: 2.39383 (0.032 sec/batch, 1990.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:33,911] [train step28370] D loss: 0.32615 G loss: 2.33913 (0.039 sec/batch, 1622.457 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:34,309] [train step28380] D loss: 0.32620 G loss: 2.36696 (0.045 sec/batch, 1430.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:34,689] [train step28390] D loss: 0.32635 G loss: 2.39245 (0.041 sec/batch, 1564.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:35,071] [train step28400] D loss: 0.32630 G loss: 2.37336 (0.047 sec/batch, 1368.284 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:35,449] [train step28410] D loss: 0.32631 G loss: 2.30989 (0.036 sec/batch, 1796.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:35,827] [train step28420] D loss: 0.32612 G loss: 2.30684 (0.040 sec/batch, 1598.848 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:36,226] [train step28430] D loss: 0.32611 G loss: 2.26460 (0.038 sec/batch, 1697.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:36,606] [train step28440] D loss: 0.32599 G loss: 2.29936 (0.042 sec/batch, 1528.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:36,991] [train step28451] D loss: 0.32639 G loss: 2.25370 (0.041 sec/batch, 1543.433 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:37,376] [train step28460] D loss: 0.32618 G loss: 2.29213 (0.034 sec/batch, 1904.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:37,763] [train step28470] D loss: 0.32605 G loss: 2.29362 (0.038 sec/batch, 1674.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:38,144] [train step28481] D loss: 0.32641 G loss: 2.25624 (0.042 sec/batch, 1519.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:38,539] [train step28490] D loss: 0.32591 G loss: 2.32752 (0.041 sec/batch, 1567.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:38,918] [train step28500] D loss: 0.32706 G loss: 2.17003 (0.037 sec/batch, 1709.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:39,302] [train step28510] D loss: 0.32612 G loss: 2.35467 (0.032 sec/batch, 1994.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:39,694] [train step28521] D loss: 0.32618 G loss: 2.33514 (0.037 sec/batch, 1743.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:40,078] [train step28530] D loss: 0.32583 G loss: 2.30683 (0.034 sec/batch, 1856.772 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:40,473] [train step28540] D loss: 0.32621 G loss: 2.38578 (0.038 sec/batch, 1693.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:40,859] [train step28550] D loss: 0.32664 G loss: 2.38689 (0.037 sec/batch, 1731.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:41,243] [train step28560] D loss: 0.32610 G loss: 2.27854 (0.046 sec/batch, 1398.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:41,625] [train step28570] D loss: 0.32629 G loss: 2.27251 (0.041 sec/batch, 1542.378 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:42,022] [train step28580] D loss: 0.32611 G loss: 2.30273 (0.040 sec/batch, 1592.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:42,416] [train step28590] D loss: 0.32607 G loss: 2.26352 (0.037 sec/batch, 1716.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:42,816] [train step28600] D loss: 0.32628 G loss: 2.23712 (0.044 sec/batch, 1463.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:43,199] [train step28611] D loss: 0.32611 G loss: 2.34559 (0.038 sec/batch, 1665.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:43,592] [train step28620] D loss: 0.32613 G loss: 2.25049 (0.038 sec/batch, 1679.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:43,976] [train step28631] D loss: 0.32601 G loss: 2.26671 (0.041 sec/batch, 1554.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:44,367] [train step28640] D loss: 0.32627 G loss: 2.34347 (0.038 sec/batch, 1688.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:44,749] [train step28650] D loss: 0.32595 G loss: 2.33286 (0.035 sec/batch, 1838.662 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:45,133] [train step28661] D loss: 0.32590 G loss: 2.31935 (0.034 sec/batch, 1901.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:45,515] [train step28670] D loss: 0.32596 G loss: 2.33782 (0.028 sec/batch, 2304.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:45,909] [train step28680] D loss: 0.32587 G loss: 2.35700 (0.039 sec/batch, 1645.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:46,292] [train step28690] D loss: 0.32578 G loss: 2.32114 (0.041 sec/batch, 1560.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:46,685] [train step28701] D loss: 0.32632 G loss: 2.22998 (0.036 sec/batch, 1757.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:47,064] [train step28710] D loss: 0.32609 G loss: 2.37230 (0.038 sec/batch, 1673.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:47,448] [train step28721] D loss: 0.32596 G loss: 2.28584 (0.036 sec/batch, 1791.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:47,833] [train step28730] D loss: 0.32618 G loss: 2.37461 (0.038 sec/batch, 1681.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:48,218] [train step28740] D loss: 0.32592 G loss: 2.27892 (0.035 sec/batch, 1809.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:48,608] [train step28751] D loss: 0.32654 G loss: 2.20316 (0.036 sec/batch, 1766.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:48,998] [train step28761] D loss: 0.32640 G loss: 2.23863 (0.037 sec/batch, 1726.962 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:49,383] [train step28770] D loss: 0.32678 G loss: 2.21389 (0.039 sec/batch, 1626.231 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:49,768] [train step28780] D loss: 0.32606 G loss: 2.35632 (0.036 sec/batch, 1756.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:50,166] [train step28790] D loss: 0.32747 G loss: 2.45537 (0.037 sec/batch, 1710.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:50,560] [train step28800] D loss: 0.32614 G loss: 2.28305 (0.038 sec/batch, 1677.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:50,949] [train step28810] D loss: 0.32610 G loss: 2.32219 (0.039 sec/batch, 1620.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:51,333] [train step28820] D loss: 0.32658 G loss: 2.22848 (0.040 sec/batch, 1612.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:51,721] [train step28830] D loss: 0.32633 G loss: 2.38963 (0.036 sec/batch, 1781.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:52,101] [train step28841] D loss: 0.32606 G loss: 2.33251 (0.039 sec/batch, 1637.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:52,501] [train step28850] D loss: 0.32605 G loss: 2.28713 (0.042 sec/batch, 1522.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:52,890] [train step28860] D loss: 0.32622 G loss: 2.38153 (0.036 sec/batch, 1794.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:53,277] [train step28871] D loss: 0.32624 G loss: 2.39879 (0.037 sec/batch, 1733.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:53,659] [train step28881] D loss: 0.32715 G loss: 2.17818 (0.042 sec/batch, 1529.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:54,033] [train step28890] D loss: 0.32631 G loss: 2.37602 (0.034 sec/batch, 1892.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:54,416] [train step28901] D loss: 0.32633 G loss: 2.21591 (0.038 sec/batch, 1686.068 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:54,795] [train step28911] D loss: 0.32600 G loss: 2.31426 (0.036 sec/batch, 1798.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:55,166] [train step28920] D loss: 0.32603 G loss: 2.26951 (0.042 sec/batch, 1538.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:55,541] [train step28930] D loss: 0.32595 G loss: 2.24070 (0.041 sec/batch, 1570.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:55,922] [train step28940] D loss: 0.32585 G loss: 2.30935 (0.036 sec/batch, 1766.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:56,288] [train step28950] D loss: 0.32591 G loss: 2.31662 (0.034 sec/batch, 1877.526 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:56,690] [train step28960] D loss: 0.32626 G loss: 2.40349 (0.036 sec/batch, 1774.076 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:57,064] [train step28970] D loss: 0.32645 G loss: 2.22858 (0.040 sec/batch, 1603.356 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:57,440] [train step28980] D loss: 0.32660 G loss: 2.41744 (0.039 sec/batch, 1644.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:57,832] [train step28991] D loss: 0.32606 G loss: 2.33916 (0.040 sec/batch, 1617.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:58,211] [train step29000] D loss: 0.32648 G loss: 2.27778 (0.038 sec/batch, 1700.929 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:27:58,211] Saved checkpoint at 29000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:58,798] [train step29010] D loss: 0.32699 G loss: 2.45212 (0.035 sec/batch, 1805.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:59,174] [train step29020] D loss: 0.32590 G loss: 2.31326 (0.038 sec/batch, 1700.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:59,549] [train step29030] D loss: 0.32607 G loss: 2.23631 (0.041 sec/batch, 1558.940 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:27:59,938] [train step29040] D loss: 0.32597 G loss: 2.30707 (0.042 sec/batch, 1511.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:00,322] [train step29051] D loss: 0.32608 G loss: 2.31099 (0.037 sec/batch, 1725.574 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:00,703] [train step29060] D loss: 0.32593 G loss: 2.27041 (0.038 sec/batch, 1683.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:01,093] [train step29070] D loss: 0.32592 G loss: 2.33287 (0.038 sec/batch, 1678.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:01,476] [train step29080] D loss: 0.32602 G loss: 2.25866 (0.044 sec/batch, 1443.287 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:01,858] [train step29091] D loss: 0.32617 G loss: 2.29232 (0.039 sec/batch, 1642.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:02,236] [train step29100] D loss: 0.32637 G loss: 2.37821 (0.034 sec/batch, 1882.608 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:02,615] [train step29111] D loss: 0.32648 G loss: 2.41705 (0.034 sec/batch, 1883.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:03,004] [train step29121] D loss: 0.32621 G loss: 2.36119 (0.040 sec/batch, 1615.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:03,402] [train step29130] D loss: 0.32600 G loss: 2.29404 (0.033 sec/batch, 1912.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:03,781] [train step29141] D loss: 0.32585 G loss: 2.29757 (0.039 sec/batch, 1657.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:04,168] [train step29150] D loss: 0.32690 G loss: 2.45311 (0.038 sec/batch, 1699.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:04,549] [train step29160] D loss: 0.32630 G loss: 2.22256 (0.037 sec/batch, 1732.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:04,947] [train step29171] D loss: 0.32591 G loss: 2.31537 (0.037 sec/batch, 1721.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:05,325] [train step29180] D loss: 0.32620 G loss: 2.37203 (0.036 sec/batch, 1762.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:05,713] [train step29190] D loss: 0.32612 G loss: 2.25128 (0.034 sec/batch, 1886.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:06,112] [train step29201] D loss: 0.32624 G loss: 2.30362 (0.041 sec/batch, 1561.734 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:06,489] [train step29211] D loss: 0.32731 G loss: 2.46690 (0.037 sec/batch, 1726.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:06,870] [train step29220] D loss: 0.32599 G loss: 2.27347 (0.040 sec/batch, 1600.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:07,261] [train step29231] D loss: 0.32720 G loss: 2.47739 (0.037 sec/batch, 1734.696 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:07,656] [train step29241] D loss: 0.32654 G loss: 2.42507 (0.040 sec/batch, 1605.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:08,054] [train step29250] D loss: 0.32614 G loss: 2.22879 (0.040 sec/batch, 1607.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:08,447] [train step29261] D loss: 0.32632 G loss: 2.26806 (0.038 sec/batch, 1681.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:08,842] [train step29271] D loss: 0.32630 G loss: 2.22983 (0.034 sec/batch, 1894.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:09,233] [train step29280] D loss: 0.32585 G loss: 2.27925 (0.036 sec/batch, 1793.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:09,605] [train step29291] D loss: 0.32616 G loss: 2.27978 (0.036 sec/batch, 1792.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:09,991] [train step29301] D loss: 0.32662 G loss: 2.41360 (0.040 sec/batch, 1616.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:10,388] [train step29310] D loss: 0.32637 G loss: 2.20954 (0.038 sec/batch, 1668.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:10,771] [train step29320] D loss: 0.32588 G loss: 2.26977 (0.037 sec/batch, 1718.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:11,167] [train step29331] D loss: 0.32619 G loss: 2.40574 (0.041 sec/batch, 1553.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:11,549] [train step29340] D loss: 0.32651 G loss: 2.23992 (0.041 sec/batch, 1576.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:11,932] [train step29351] D loss: 0.32596 G loss: 2.26831 (0.038 sec/batch, 1691.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:12,336] [train step29360] D loss: 0.32582 G loss: 2.29960 (0.038 sec/batch, 1685.983 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:12,719] [train step29370] D loss: 0.32588 G loss: 2.33132 (0.037 sec/batch, 1709.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:13,108] [train step29381] D loss: 0.32596 G loss: 2.27499 (0.045 sec/batch, 1418.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:13,495] [train step29391] D loss: 0.32627 G loss: 2.22993 (0.040 sec/batch, 1590.521 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:13,900] [train step29400] D loss: 0.32707 G loss: 2.45140 (0.040 sec/batch, 1587.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:14,296] [train step29410] D loss: 0.32567 G loss: 2.31081 (0.043 sec/batch, 1487.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:14,681] [train step29420] D loss: 0.32601 G loss: 2.29262 (0.042 sec/batch, 1509.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:15,071] [train step29430] D loss: 0.32615 G loss: 2.26331 (0.035 sec/batch, 1820.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:15,472] [train step29441] D loss: 0.32603 G loss: 2.26993 (0.042 sec/batch, 1509.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:15,856] [train step29450] D loss: 0.32593 G loss: 2.25883 (0.045 sec/batch, 1407.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:16,244] [train step29460] D loss: 0.32626 G loss: 2.32610 (0.031 sec/batch, 2058.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:16,637] [train step29471] D loss: 0.32609 G loss: 2.29330 (0.044 sec/batch, 1462.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:17,011] [train step29481] D loss: 0.32582 G loss: 2.27054 (0.040 sec/batch, 1614.698 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:17,394] [train step29490] D loss: 0.32593 G loss: 2.28736 (0.033 sec/batch, 1942.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:17,789] [train step29501] D loss: 0.32602 G loss: 2.25941 (0.038 sec/batch, 1703.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:18,175] [train step29511] D loss: 0.32575 G loss: 2.32881 (0.041 sec/batch, 1551.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:18,559] [train step29520] D loss: 0.32596 G loss: 2.27727 (0.037 sec/batch, 1733.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:18,941] [train step29531] D loss: 0.32573 G loss: 2.30887 (0.035 sec/batch, 1807.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:19,331] [train step29540] D loss: 0.32645 G loss: 2.40682 (0.041 sec/batch, 1551.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:19,712] [train step29550] D loss: 0.32599 G loss: 2.28720 (0.036 sec/batch, 1777.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:20,088] [train step29560] D loss: 0.32647 G loss: 2.41164 (0.038 sec/batch, 1690.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:20,482] [train step29571] D loss: 0.32610 G loss: 2.33730 (0.033 sec/batch, 1948.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:20,872] [train step29580] D loss: 0.32609 G loss: 2.36805 (0.039 sec/batch, 1629.894 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:21,259] [train step29590] D loss: 0.32713 G loss: 2.46642 (0.042 sec/batch, 1541.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:21,658] [train step29600] D loss: 0.32579 G loss: 2.30606 (0.038 sec/batch, 1663.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:22,038] [train step29610] D loss: 0.32626 G loss: 2.39782 (0.037 sec/batch, 1713.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:22,433] [train step29621] D loss: 0.32619 G loss: 2.38939 (0.036 sec/batch, 1770.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:22,814] [train step29630] D loss: 0.32624 G loss: 2.39604 (0.040 sec/batch, 1606.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:23,196] [train step29640] D loss: 0.32591 G loss: 2.32858 (0.037 sec/batch, 1728.886 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:23,589] [train step29650] D loss: 0.32606 G loss: 2.28524 (0.035 sec/batch, 1806.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:23,970] [train step29660] D loss: 0.32590 G loss: 2.34815 (0.039 sec/batch, 1658.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:24,339] [train step29670] D loss: 0.32665 G loss: 2.21439 (0.035 sec/batch, 1829.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:24,742] [train step29680] D loss: 0.32593 G loss: 2.28730 (0.036 sec/batch, 1769.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:25,114] [train step29690] D loss: 0.32632 G loss: 2.41226 (0.036 sec/batch, 1785.772 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:25,500] [train step29700] D loss: 0.32655 G loss: 2.19150 (0.039 sec/batch, 1656.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:25,876] [train step29711] D loss: 0.32607 G loss: 2.25432 (0.035 sec/batch, 1852.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:26,251] [train step29720] D loss: 0.32609 G loss: 2.32902 (0.036 sec/batch, 1793.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:26,675] [train step29730] D loss: 0.32596 G loss: 2.29570 (0.039 sec/batch, 1645.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:27,063] [train step29741] D loss: 0.32582 G loss: 2.26170 (0.038 sec/batch, 1667.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:27,443] [train step29751] D loss: 0.32574 G loss: 2.31718 (0.037 sec/batch, 1733.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:27,837] [train step29760] D loss: 0.32578 G loss: 2.30022 (0.031 sec/batch, 2032.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:28,225] [train step29770] D loss: 0.32588 G loss: 2.32550 (0.037 sec/batch, 1721.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:28,616] [train step29781] D loss: 0.32594 G loss: 2.33883 (0.038 sec/batch, 1682.106 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:29,002] [train step29790] D loss: 0.32586 G loss: 2.31285 (0.039 sec/batch, 1645.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:29,385] [train step29801] D loss: 0.32581 G loss: 2.34211 (0.038 sec/batch, 1685.263 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:29,778] [train step29811] D loss: 0.32622 G loss: 2.24157 (0.036 sec/batch, 1763.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:30,174] [train step29820] D loss: 0.32714 G loss: 2.46136 (0.045 sec/batch, 1420.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:30,561] [train step29831] D loss: 0.32715 G loss: 2.45643 (0.042 sec/batch, 1541.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:30,950] [train step29840] D loss: 0.32586 G loss: 2.32534 (0.037 sec/batch, 1751.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:31,327] [train step29850] D loss: 0.32573 G loss: 2.32673 (0.035 sec/batch, 1849.404 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:31,742] [train step29860] D loss: 0.32618 G loss: 2.24960 (0.040 sec/batch, 1588.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:32,119] [train step29870] D loss: 0.32575 G loss: 2.28270 (0.035 sec/batch, 1852.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:32,506] [train step29880] D loss: 0.32589 G loss: 2.32382 (0.037 sec/batch, 1735.806 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:32,892] [train step29890] D loss: 0.32578 G loss: 2.30250 (0.038 sec/batch, 1694.904 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:33,275] [train step29901] D loss: 0.32579 G loss: 2.34934 (0.039 sec/batch, 1631.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:33,660] [train step29910] D loss: 0.32584 G loss: 2.31299 (0.042 sec/batch, 1525.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:34,048] [train step29920] D loss: 0.32591 G loss: 2.35044 (0.038 sec/batch, 1687.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:34,443] [train step29931] D loss: 0.32610 G loss: 2.39155 (0.036 sec/batch, 1782.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:34,830] [train step29940] D loss: 0.32649 G loss: 2.18649 (0.033 sec/batch, 1920.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:35,229] [train step29950] D loss: 0.32694 G loss: 2.18695 (0.036 sec/batch, 1775.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:35,613] [train step29960] D loss: 0.32579 G loss: 2.32635 (0.036 sec/batch, 1793.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:36,010] [train step29970] D loss: 0.32625 G loss: 2.21569 (0.037 sec/batch, 1708.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:36,395] [train step29981] D loss: 0.32582 G loss: 2.29952 (0.039 sec/batch, 1642.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:36,783] [train step29990] D loss: 0.32570 G loss: 2.32359 (0.050 sec/batch, 1275.234 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:37,173] [train step30000] D loss: 0.32619 G loss: 2.40734 (0.038 sec/batch, 1683.804 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:28:37,173] Saved checkpoint at 30000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:37,760] [train step30010] D loss: 0.32683 G loss: 2.41444 (0.037 sec/batch, 1719.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:38,154] [train step30021] D loss: 0.32621 G loss: 2.36719 (0.037 sec/batch, 1729.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:38,542] [train step30030] D loss: 0.32622 G loss: 2.24067 (0.041 sec/batch, 1579.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:38,935] [train step30041] D loss: 0.32572 G loss: 2.33323 (0.035 sec/batch, 1821.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:39,317] [train step30050] D loss: 0.32648 G loss: 2.41156 (0.037 sec/batch, 1752.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:39,698] [train step30060] D loss: 0.32576 G loss: 2.26929 (0.038 sec/batch, 1663.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:40,089] [train step30071] D loss: 0.32662 G loss: 2.42843 (0.038 sec/batch, 1702.288 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:40,470] [train step30081] D loss: 0.32581 G loss: 2.28462 (0.045 sec/batch, 1433.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:40,862] [train step30090] D loss: 0.32589 G loss: 2.29323 (0.044 sec/batch, 1458.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:41,250] [train step30101] D loss: 0.32597 G loss: 2.32971 (0.036 sec/batch, 1769.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:41,628] [train step30111] D loss: 0.32620 G loss: 2.39681 (0.033 sec/batch, 1938.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:42,016] [train step30120] D loss: 0.32597 G loss: 2.27256 (0.032 sec/batch, 2016.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:42,405] [train step30130] D loss: 0.32603 G loss: 2.36387 (0.038 sec/batch, 1705.782 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:42,787] [train step30140] D loss: 0.32589 G loss: 2.30950 (0.039 sec/batch, 1628.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:43,173] [train step30150] D loss: 0.32568 G loss: 2.30324 (0.039 sec/batch, 1627.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:43,551] [train step30160] D loss: 0.32565 G loss: 2.34163 (0.042 sec/batch, 1523.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:43,944] [train step30171] D loss: 0.32594 G loss: 2.25308 (0.043 sec/batch, 1500.587 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:44,329] [train step30180] D loss: 0.32596 G loss: 2.38021 (0.034 sec/batch, 1894.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:44,735] [train step30191] D loss: 0.32635 G loss: 2.22074 (0.035 sec/batch, 1826.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:45,123] [train step30201] D loss: 0.32645 G loss: 2.19739 (0.036 sec/batch, 1777.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:45,515] [train step30210] D loss: 0.32581 G loss: 2.30324 (0.040 sec/batch, 1611.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:45,910] [train step30221] D loss: 0.32692 G loss: 2.16073 (0.037 sec/batch, 1746.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:46,306] [train step30230] D loss: 0.32605 G loss: 2.25730 (0.037 sec/batch, 1716.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:46,685] [train step30240] D loss: 0.32578 G loss: 2.30046 (0.036 sec/batch, 1800.734 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:47,074] [train step30250] D loss: 0.32605 G loss: 2.24296 (0.037 sec/batch, 1749.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:47,459] [train step30260] D loss: 0.32580 G loss: 2.32955 (0.038 sec/batch, 1673.413 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:47,840] [train step30270] D loss: 0.32559 G loss: 2.31602 (0.033 sec/batch, 1911.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:48,234] [train step30281] D loss: 0.32596 G loss: 2.34732 (0.037 sec/batch, 1745.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:48,614] [train step30291] D loss: 0.32644 G loss: 2.21757 (0.040 sec/batch, 1591.351 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:48,995] [train step30300] D loss: 0.32589 G loss: 2.33057 (0.035 sec/batch, 1842.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:49,395] [train step30310] D loss: 0.32607 G loss: 2.24767 (0.046 sec/batch, 1404.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:49,771] [train step30321] D loss: 0.32669 G loss: 2.18620 (0.035 sec/batch, 1850.296 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:50,165] [train step30330] D loss: 0.32635 G loss: 2.42255 (0.038 sec/batch, 1695.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:50,549] [train step30340] D loss: 0.32643 G loss: 2.39109 (0.038 sec/batch, 1696.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:50,928] [train step30350] D loss: 0.32725 G loss: 2.48611 (0.037 sec/batch, 1722.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:51,313] [train step30360] D loss: 0.32816 G loss: 2.10450 (0.041 sec/batch, 1574.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:51,691] [train step30370] D loss: 0.32621 G loss: 2.21528 (0.036 sec/batch, 1802.536 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:52,079] [train step30380] D loss: 0.32583 G loss: 2.29947 (0.041 sec/batch, 1575.205 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:52,468] [train step30390] D loss: 0.32595 G loss: 2.26440 (0.041 sec/batch, 1565.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:52,855] [train step30400] D loss: 0.32587 G loss: 2.27249 (0.039 sec/batch, 1651.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:53,249] [train step30410] D loss: 0.32590 G loss: 2.26160 (0.044 sec/batch, 1471.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:53,631] [train step30420] D loss: 0.32697 G loss: 2.45408 (0.033 sec/batch, 1956.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:54,022] [train step30431] D loss: 0.32606 G loss: 2.36124 (0.038 sec/batch, 1691.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:54,412] [train step30441] D loss: 0.32605 G loss: 2.25062 (0.039 sec/batch, 1647.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:54,777] [train step30450] D loss: 0.32765 G loss: 2.50695 (0.038 sec/batch, 1691.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:55,155] [train step30461] D loss: 0.32632 G loss: 2.41342 (0.035 sec/batch, 1811.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:55,536] [train step30471] D loss: 0.32593 G loss: 2.27862 (0.034 sec/batch, 1866.897 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:55,913] [train step30480] D loss: 0.32562 G loss: 2.32735 (0.039 sec/batch, 1636.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:56,309] [train step30491] D loss: 0.32566 G loss: 2.29548 (0.037 sec/batch, 1714.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:56,689] [train step30500] D loss: 0.32579 G loss: 2.32670 (0.043 sec/batch, 1500.176 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:57,060] [train step30510] D loss: 0.32581 G loss: 2.29255 (0.032 sec/batch, 1989.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:57,453] [train step30521] D loss: 0.32587 G loss: 2.25528 (0.035 sec/batch, 1834.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:57,829] [train step30531] D loss: 0.32582 G loss: 2.26669 (0.036 sec/batch, 1781.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:58,203] [train step30540] D loss: 0.32573 G loss: 2.33041 (0.040 sec/batch, 1580.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:58,586] [train step30550] D loss: 0.32596 G loss: 2.38163 (0.040 sec/batch, 1590.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:58,969] [train step30560] D loss: 0.32602 G loss: 2.37326 (0.034 sec/batch, 1872.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:59,361] [train step30570] D loss: 0.32570 G loss: 2.29145 (0.047 sec/batch, 1368.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:28:59,750] [train step30580] D loss: 0.32551 G loss: 2.31726 (0.038 sec/batch, 1692.574 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:00,123] [train step30591] D loss: 0.32627 G loss: 2.40951 (0.037 sec/batch, 1746.126 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:00,497] [train step30600] D loss: 0.32583 G loss: 2.26517 (0.036 sec/batch, 1756.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:00,881] [train step30611] D loss: 0.32561 G loss: 2.34189 (0.038 sec/batch, 1683.076 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:01,255] [train step30621] D loss: 0.32651 G loss: 2.43531 (0.036 sec/batch, 1779.639 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:01,638] [train step30630] D loss: 0.32599 G loss: 2.23160 (0.031 sec/batch, 2092.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:02,014] [train step30640] D loss: 0.32580 G loss: 2.27848 (0.036 sec/batch, 1774.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:02,393] [train step30651] D loss: 0.32634 G loss: 2.42436 (0.040 sec/batch, 1604.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:02,769] [train step30660] D loss: 0.32615 G loss: 2.21841 (0.039 sec/batch, 1627.463 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:03,142] [train step30671] D loss: 0.32570 G loss: 2.27139 (0.037 sec/batch, 1726.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:03,530] [train step30681] D loss: 0.32590 G loss: 2.30432 (0.041 sec/batch, 1568.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:03,904] [train step30690] D loss: 0.32577 G loss: 2.30014 (0.038 sec/batch, 1675.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:04,291] [train step30700] D loss: 0.32570 G loss: 2.33579 (0.046 sec/batch, 1403.166 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:04,670] [train step30711] D loss: 0.32567 G loss: 2.28928 (0.040 sec/batch, 1603.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:05,040] [train step30720] D loss: 0.32681 G loss: 2.45802 (0.032 sec/batch, 1976.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:05,426] [train step30731] D loss: 0.32654 G loss: 2.43511 (0.038 sec/batch, 1677.156 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:05,818] [train step30741] D loss: 0.32738 G loss: 2.48183 (0.038 sec/batch, 1691.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:06,196] [train step30750] D loss: 0.32653 G loss: 2.17871 (0.038 sec/batch, 1696.639 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:06,588] [train step30761] D loss: 0.32583 G loss: 2.23754 (0.036 sec/batch, 1762.208 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:06,986] [train step30771] D loss: 0.32575 G loss: 2.28200 (0.043 sec/batch, 1498.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:07,364] [train step30780] D loss: 0.32588 G loss: 2.30266 (0.034 sec/batch, 1870.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:07,761] [train step30790] D loss: 0.32557 G loss: 2.29550 (0.040 sec/batch, 1597.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:08,140] [train step30801] D loss: 0.32590 G loss: 2.36160 (0.039 sec/batch, 1636.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:08,527] [train step30810] D loss: 0.32610 G loss: 2.26549 (0.036 sec/batch, 1792.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:08,916] [train step30820] D loss: 0.32567 G loss: 2.32110 (0.037 sec/batch, 1750.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:09,296] [train step30830] D loss: 0.32581 G loss: 2.33647 (0.035 sec/batch, 1831.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:09,684] [train step30840] D loss: 0.32616 G loss: 2.22441 (0.039 sec/batch, 1620.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:10,071] [train step30850] D loss: 0.32582 G loss: 2.30658 (0.036 sec/batch, 1766.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:10,457] [train step30860] D loss: 0.32581 G loss: 2.30373 (0.039 sec/batch, 1620.263 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:10,851] [train step30870] D loss: 0.32563 G loss: 2.32302 (0.041 sec/batch, 1547.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:11,242] [train step30880] D loss: 0.32577 G loss: 2.36587 (0.038 sec/batch, 1663.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:11,625] [train step30891] D loss: 0.32615 G loss: 2.40647 (0.038 sec/batch, 1666.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:12,016] [train step30900] D loss: 0.32567 G loss: 2.34521 (0.037 sec/batch, 1723.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:12,390] [train step30910] D loss: 0.32559 G loss: 2.34766 (0.029 sec/batch, 2217.137 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:12,786] [train step30921] D loss: 0.32567 G loss: 2.29489 (0.041 sec/batch, 1549.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:13,163] [train step30930] D loss: 0.32599 G loss: 2.27225 (0.033 sec/batch, 1953.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:13,558] [train step30940] D loss: 0.32627 G loss: 2.20331 (0.038 sec/batch, 1669.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:13,947] [train step30950] D loss: 0.32661 G loss: 2.19453 (0.037 sec/batch, 1731.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:14,322] [train step30960] D loss: 0.32812 G loss: 2.52874 (0.035 sec/batch, 1817.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:14,716] [train step30970] D loss: 0.32898 G loss: 2.55739 (0.038 sec/batch, 1665.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:15,104] [train step30981] D loss: 0.32645 G loss: 2.41205 (0.038 sec/batch, 1682.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:15,483] [train step30990] D loss: 0.32585 G loss: 2.27901 (0.036 sec/batch, 1759.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:15,860] [train step31000] D loss: 0.32595 G loss: 2.35549 (0.028 sec/batch, 2281.509 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:29:15,860] Saved checkpoint at 31000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:16,462] [train step31010] D loss: 0.32660 G loss: 2.42645 (0.046 sec/batch, 1379.719 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:16,869] [train step31020] D loss: 0.32690 G loss: 2.15707 (0.045 sec/batch, 1420.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:17,264] [train step31031] D loss: 0.32652 G loss: 2.18308 (0.040 sec/batch, 1615.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:17,644] [train step31041] D loss: 0.32568 G loss: 2.28882 (0.036 sec/batch, 1778.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:18,034] [train step31050] D loss: 0.32602 G loss: 2.36622 (0.042 sec/batch, 1532.472 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:18,411] [train step31061] D loss: 0.32600 G loss: 2.37419 (0.037 sec/batch, 1729.231 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:18,798] [train step31071] D loss: 0.32632 G loss: 2.42249 (0.035 sec/batch, 1853.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:19,192] [train step31080] D loss: 0.32672 G loss: 2.18800 (0.036 sec/batch, 1768.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:19,576] [train step31091] D loss: 0.32681 G loss: 2.17025 (0.041 sec/batch, 1561.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:19,958] [train step31100] D loss: 0.32620 G loss: 2.22263 (0.041 sec/batch, 1563.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:20,350] [train step31110] D loss: 0.32569 G loss: 2.35091 (0.047 sec/batch, 1359.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:20,736] [train step31121] D loss: 0.32565 G loss: 2.32405 (0.039 sec/batch, 1660.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:21,125] [train step31130] D loss: 0.32632 G loss: 2.41672 (0.038 sec/batch, 1689.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:21,513] [train step31140] D loss: 0.32576 G loss: 2.23898 (0.046 sec/batch, 1401.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:21,907] [train step31151] D loss: 0.32577 G loss: 2.28215 (0.041 sec/batch, 1552.817 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:22,295] [train step31161] D loss: 0.32558 G loss: 2.31517 (0.031 sec/batch, 2037.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:22,675] [train step31170] D loss: 0.32579 G loss: 2.35559 (0.035 sec/batch, 1822.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:23,060] [train step31181] D loss: 0.32603 G loss: 2.38752 (0.036 sec/batch, 1779.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:23,441] [train step31191] D loss: 0.32564 G loss: 2.32159 (0.037 sec/batch, 1716.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:23,827] [train step31200] D loss: 0.32588 G loss: 2.27574 (0.037 sec/batch, 1714.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:24,218] [train step31210] D loss: 0.32577 G loss: 2.26256 (0.041 sec/batch, 1554.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:24,599] [train step31220] D loss: 0.32584 G loss: 2.25411 (0.035 sec/batch, 1835.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:24,971] [train step31230] D loss: 0.32573 G loss: 2.28850 (0.040 sec/batch, 1610.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:25,355] [train step31241] D loss: 0.32559 G loss: 2.30730 (0.039 sec/batch, 1658.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:25,729] [train step31251] D loss: 0.32560 G loss: 2.33581 (0.035 sec/batch, 1811.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:26,101] [train step31260] D loss: 0.32574 G loss: 2.33272 (0.039 sec/batch, 1622.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:26,483] [train step31271] D loss: 0.32560 G loss: 2.29000 (0.040 sec/batch, 1599.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:26,858] [train step31281] D loss: 0.32607 G loss: 2.22162 (0.036 sec/batch, 1789.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:27,253] [train step31290] D loss: 0.32600 G loss: 2.39466 (0.037 sec/batch, 1751.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:27,621] [train step31301] D loss: 0.32574 G loss: 2.35569 (0.037 sec/batch, 1719.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:28,009] [train step31310] D loss: 0.32653 G loss: 2.18542 (0.038 sec/batch, 1700.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:28,379] [train step31320] D loss: 0.32699 G loss: 2.46652 (0.026 sec/batch, 2421.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:28,760] [train step31331] D loss: 0.32589 G loss: 2.35403 (0.034 sec/batch, 1870.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:29,127] [train step31340] D loss: 0.32597 G loss: 2.30618 (0.033 sec/batch, 1959.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:29,513] [train step31350] D loss: 0.32582 G loss: 2.34798 (0.037 sec/batch, 1717.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:29,881] [train step31361] D loss: 0.32574 G loss: 2.35175 (0.038 sec/batch, 1705.923 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:30,265] [train step31371] D loss: 0.32797 G loss: 2.52161 (0.040 sec/batch, 1614.513 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:30,653] [train step31380] D loss: 0.32668 G loss: 2.17524 (0.040 sec/batch, 1612.574 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:31,022] [train step31390] D loss: 0.32654 G loss: 2.17747 (0.034 sec/batch, 1891.549 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:31,409] [train step31401] D loss: 0.32700 G loss: 2.14875 (0.045 sec/batch, 1433.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:31,788] [train step31410] D loss: 0.32698 G loss: 2.46438 (0.035 sec/batch, 1833.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:32,176] [train step31421] D loss: 0.32565 G loss: 2.33800 (0.043 sec/batch, 1483.782 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:32,563] [train step31430] D loss: 0.32586 G loss: 2.29547 (0.037 sec/batch, 1717.185 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:32,935] [train step31440] D loss: 0.32554 G loss: 2.30735 (0.037 sec/batch, 1738.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:33,322] [train step31451] D loss: 0.32563 G loss: 2.29181 (0.040 sec/batch, 1613.368 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:33,701] [train step31460] D loss: 0.32577 G loss: 2.32380 (0.036 sec/batch, 1758.169 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:34,094] [train step31470] D loss: 0.32568 G loss: 2.27546 (0.047 sec/batch, 1348.861 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:34,480] [train step31481] D loss: 0.32595 G loss: 2.22648 (0.036 sec/batch, 1763.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:34,864] [train step31490] D loss: 0.32571 G loss: 2.26162 (0.039 sec/batch, 1642.169 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:35,236] [train step31500] D loss: 0.32573 G loss: 2.34827 (0.036 sec/batch, 1764.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:35,631] [train step31511] D loss: 0.32604 G loss: 2.38912 (0.039 sec/batch, 1625.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:36,013] [train step31521] D loss: 0.32736 G loss: 2.48695 (0.036 sec/batch, 1764.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:36,399] [train step31530] D loss: 0.32676 G loss: 2.16999 (0.043 sec/batch, 1496.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:36,779] [train step31540] D loss: 0.32567 G loss: 2.31701 (0.039 sec/batch, 1649.018 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:37,161] [train step31550] D loss: 0.32624 G loss: 2.22581 (0.036 sec/batch, 1774.404 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:37,551] [train step31560] D loss: 0.32669 G loss: 2.44926 (0.039 sec/batch, 1661.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:37,942] [train step31571] D loss: 0.32606 G loss: 2.38805 (0.036 sec/batch, 1757.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:38,334] [train step31581] D loss: 0.32591 G loss: 2.38120 (0.047 sec/batch, 1347.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:38,733] [train step31590] D loss: 0.32569 G loss: 2.26192 (0.035 sec/batch, 1824.875 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:39,108] [train step31600] D loss: 0.32555 G loss: 2.26587 (0.036 sec/batch, 1787.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:39,501] [train step31610] D loss: 0.32582 G loss: 2.24185 (0.037 sec/batch, 1716.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:39,886] [train step31620] D loss: 0.32574 G loss: 2.33587 (0.038 sec/batch, 1696.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:40,268] [train step31631] D loss: 0.32573 G loss: 2.24302 (0.037 sec/batch, 1717.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:40,664] [train step31641] D loss: 0.32589 G loss: 2.24332 (0.038 sec/batch, 1665.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:41,062] [train step31650] D loss: 0.32570 G loss: 2.36860 (0.040 sec/batch, 1585.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:41,437] [train step31660] D loss: 0.32573 G loss: 2.31945 (0.036 sec/batch, 1768.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:41,840] [train step31671] D loss: 0.32564 G loss: 2.30978 (0.039 sec/batch, 1642.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:42,220] [train step31680] D loss: 0.32561 G loss: 2.31104 (0.035 sec/batch, 1829.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:42,609] [train step31691] D loss: 0.32567 G loss: 2.26024 (0.037 sec/batch, 1752.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:42,990] [train step31700] D loss: 0.32588 G loss: 2.26411 (0.037 sec/batch, 1714.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:43,373] [train step31710] D loss: 0.32594 G loss: 2.37546 (0.040 sec/batch, 1618.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:43,774] [train step31720] D loss: 0.32554 G loss: 2.30340 (0.043 sec/batch, 1494.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:44,161] [train step31731] D loss: 0.32573 G loss: 2.33482 (0.039 sec/batch, 1632.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:44,545] [train step31740] D loss: 0.32564 G loss: 2.24511 (0.037 sec/batch, 1731.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:44,933] [train step31751] D loss: 0.32609 G loss: 2.23654 (0.032 sec/batch, 1972.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:45,329] [train step31760] D loss: 0.32594 G loss: 2.23167 (0.038 sec/batch, 1664.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:45,718] [train step31770] D loss: 0.32644 G loss: 2.42103 (0.041 sec/batch, 1568.633 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:46,106] [train step31780] D loss: 0.32657 G loss: 2.42930 (0.040 sec/batch, 1616.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:46,487] [train step31790] D loss: 0.32597 G loss: 2.39011 (0.037 sec/batch, 1707.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:46,876] [train step31800] D loss: 0.32565 G loss: 2.30567 (0.038 sec/batch, 1694.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:47,257] [train step31811] D loss: 0.32580 G loss: 2.38368 (0.040 sec/batch, 1616.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:47,639] [train step31821] D loss: 0.32559 G loss: 2.33515 (0.034 sec/batch, 1866.429 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:48,032] [train step31830] D loss: 0.32573 G loss: 2.25667 (0.041 sec/batch, 1559.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:48,416] [train step31841] D loss: 0.32585 G loss: 2.23336 (0.039 sec/batch, 1640.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:48,805] [train step31850] D loss: 0.32618 G loss: 2.20643 (0.049 sec/batch, 1295.357 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:49,199] [train step31860] D loss: 0.32699 G loss: 2.46088 (0.040 sec/batch, 1608.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:49,580] [train step31870] D loss: 0.32671 G loss: 2.45667 (0.035 sec/batch, 1843.169 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:49,980] [train step31881] D loss: 0.32671 G loss: 2.44491 (0.037 sec/batch, 1744.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:50,381] [train step31890] D loss: 0.32607 G loss: 2.21459 (0.040 sec/batch, 1596.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:50,769] [train step31901] D loss: 0.32583 G loss: 2.26057 (0.045 sec/batch, 1420.677 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:51,159] [train step31911] D loss: 0.32579 G loss: 2.33622 (0.039 sec/batch, 1655.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:51,544] [train step31920] D loss: 0.32621 G loss: 2.19992 (0.040 sec/batch, 1608.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:51,945] [train step31931] D loss: 0.32651 G loss: 2.18042 (0.045 sec/batch, 1406.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:52,330] [train step31941] D loss: 0.32637 G loss: 2.17884 (0.035 sec/batch, 1835.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:52,714] [train step31950] D loss: 0.32669 G loss: 2.44697 (0.041 sec/batch, 1575.492 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:53,121] [train step31960] D loss: 0.32621 G loss: 2.41485 (0.037 sec/batch, 1734.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:53,507] [train step31971] D loss: 0.32585 G loss: 2.29150 (0.044 sec/batch, 1442.038 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:53,900] [train step31980] D loss: 0.32624 G loss: 2.40812 (0.037 sec/batch, 1750.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:54,289] [train step31991] D loss: 0.32698 G loss: 2.46533 (0.039 sec/batch, 1657.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:54,681] [train step32000] D loss: 0.32755 G loss: 2.49998 (0.038 sec/batch, 1680.358 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:29:54,681] Saved checkpoint at 32000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:55,274] [train step32010] D loss: 0.32631 G loss: 2.19853 (0.040 sec/batch, 1606.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:55,647] [train step32021] D loss: 0.32572 G loss: 2.24312 (0.036 sec/batch, 1763.888 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:56,037] [train step32030] D loss: 0.32570 G loss: 2.31564 (0.032 sec/batch, 1976.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:56,420] [train step32040] D loss: 0.32580 G loss: 2.35203 (0.030 sec/batch, 2113.482 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:56,805] [train step32050] D loss: 0.32574 G loss: 2.36810 (0.040 sec/batch, 1602.217 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:57,193] [train step32060] D loss: 0.32617 G loss: 2.40916 (0.039 sec/batch, 1647.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:57,574] [train step32070] D loss: 0.32587 G loss: 2.23912 (0.037 sec/batch, 1743.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:57,962] [train step32080] D loss: 0.32572 G loss: 2.26502 (0.045 sec/batch, 1410.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:58,339] [train step32091] D loss: 0.32575 G loss: 2.24537 (0.039 sec/batch, 1652.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:58,712] [train step32100] D loss: 0.32602 G loss: 2.38265 (0.037 sec/batch, 1722.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:59,094] [train step32111] D loss: 0.32574 G loss: 2.25625 (0.036 sec/batch, 1782.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:59,471] [train step32120] D loss: 0.32543 G loss: 2.28791 (0.038 sec/batch, 1697.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:29:59,868] [train step32130] D loss: 0.32572 G loss: 2.31893 (0.049 sec/batch, 1316.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:00,261] [train step32141] D loss: 0.32630 G loss: 2.20205 (0.037 sec/batch, 1752.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:00,635] [train step32151] D loss: 0.32638 G loss: 2.19934 (0.039 sec/batch, 1634.858 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:01,025] [train step32160] D loss: 0.32647 G loss: 2.42985 (0.041 sec/batch, 1558.117 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:01,403] [train step32171] D loss: 0.32550 G loss: 2.30269 (0.038 sec/batch, 1686.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:01,775] [train step32181] D loss: 0.32693 G loss: 2.15579 (0.041 sec/batch, 1551.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:02,158] [train step32190] D loss: 0.32768 G loss: 2.50170 (0.036 sec/batch, 1794.775 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:02,534] [train step32200] D loss: 0.32645 G loss: 2.43120 (0.037 sec/batch, 1746.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:02,913] [train step32211] D loss: 0.32702 G loss: 2.46079 (0.036 sec/batch, 1765.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:03,299] [train step32220] D loss: 0.32679 G loss: 2.16775 (0.036 sec/batch, 1770.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:03,679] [train step32230] D loss: 0.32765 G loss: 2.12276 (0.034 sec/batch, 1898.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:04,069] [train step32241] D loss: 0.32676 G loss: 2.16887 (0.039 sec/batch, 1651.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:04,455] [train step32250] D loss: 0.32607 G loss: 2.40830 (0.037 sec/batch, 1714.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:04,840] [train step32260] D loss: 0.32606 G loss: 2.39580 (0.039 sec/batch, 1637.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:05,222] [train step32270] D loss: 0.32563 G loss: 2.27563 (0.034 sec/batch, 1861.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:05,605] [train step32280] D loss: 0.32611 G loss: 2.39335 (0.035 sec/batch, 1804.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:05,971] [train step32290] D loss: 0.32991 G loss: 2.60281 (0.026 sec/batch, 2467.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:06,384] [train step32301] D loss: 0.33048 G loss: 2.61620 (0.049 sec/batch, 1295.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:06,762] [train step32310] D loss: 0.32858 G loss: 2.09094 (0.040 sec/batch, 1594.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:07,142] [train step32321] D loss: 0.32757 G loss: 2.12122 (0.037 sec/batch, 1735.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:07,526] [train step32331] D loss: 0.32699 G loss: 2.15865 (0.036 sec/batch, 1775.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:07,907] [train step32340] D loss: 0.32775 G loss: 2.51305 (0.042 sec/batch, 1538.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:08,304] [train step32350] D loss: 0.32929 G loss: 2.57372 (0.044 sec/batch, 1438.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:08,676] [train step32361] D loss: 0.32625 G loss: 2.41408 (0.030 sec/batch, 2143.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:09,061] [train step32370] D loss: 0.32564 G loss: 2.25366 (0.036 sec/batch, 1787.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:09,455] [train step32380] D loss: 0.32571 G loss: 2.24843 (0.039 sec/batch, 1635.984 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:09,836] [train step32391] D loss: 0.32555 G loss: 2.28014 (0.035 sec/batch, 1845.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:10,231] [train step32400] D loss: 0.32625 G loss: 2.41106 (0.040 sec/batch, 1606.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:10,619] [train step32411] D loss: 0.32797 G loss: 2.50256 (0.037 sec/batch, 1711.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:11,014] [train step32420] D loss: 0.32868 G loss: 2.54600 (0.038 sec/batch, 1689.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:11,411] [train step32430] D loss: 0.32848 G loss: 2.10858 (0.041 sec/batch, 1555.156 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:11,800] [train step32440] D loss: 0.33491 G loss: 1.93740 (0.039 sec/batch, 1645.319 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:12,180] [train step32450] D loss: 0.33480 G loss: 1.93106 (0.032 sec/batch, 1977.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:12,573] [train step32460] D loss: 0.34047 G loss: 2.87854 (0.030 sec/batch, 2144.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:12,972] [train step32470] D loss: 0.32886 G loss: 2.55641 (0.039 sec/batch, 1644.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:13,353] [train step32480] D loss: 0.32596 G loss: 2.38576 (0.041 sec/batch, 1567.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:13,746] [train step32490] D loss: 0.32567 G loss: 2.32670 (0.034 sec/batch, 1886.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:14,139] [train step32501] D loss: 0.32640 G loss: 2.42181 (0.041 sec/batch, 1578.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:14,530] [train step32511] D loss: 0.32587 G loss: 2.37526 (0.037 sec/batch, 1747.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:14,922] [train step32520] D loss: 0.32628 G loss: 2.21827 (0.038 sec/batch, 1680.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:15,312] [train step32531] D loss: 0.32636 G loss: 2.19652 (0.045 sec/batch, 1410.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:15,704] [train step32541] D loss: 0.32746 G loss: 2.14004 (0.034 sec/batch, 1865.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:16,083] [train step32550] D loss: 0.32721 G loss: 2.47449 (0.039 sec/batch, 1650.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:16,467] [train step32561] D loss: 0.32789 G loss: 2.50708 (0.045 sec/batch, 1424.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:16,849] [train step32571] D loss: 0.33321 G loss: 2.69977 (0.037 sec/batch, 1723.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:17,228] [train step32580] D loss: 0.32992 G loss: 2.04432 (0.039 sec/batch, 1658.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:17,614] [train step32590] D loss: 0.32694 G loss: 2.15790 (0.033 sec/batch, 1910.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:18,003] [train step32601] D loss: 0.32624 G loss: 2.19819 (0.039 sec/batch, 1635.286 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:18,386] [train step32610] D loss: 0.32665 G loss: 2.44695 (0.037 sec/batch, 1728.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:18,783] [train step32621] D loss: 0.32731 G loss: 2.48659 (0.037 sec/batch, 1717.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:19,163] [train step32630] D loss: 0.32634 G loss: 2.42304 (0.039 sec/batch, 1649.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:19,557] [train step32640] D loss: 0.32596 G loss: 2.24228 (0.048 sec/batch, 1327.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:19,943] [train step32651] D loss: 0.32552 G loss: 2.30447 (0.036 sec/batch, 1763.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:20,332] [train step32661] D loss: 0.32617 G loss: 2.39056 (0.037 sec/batch, 1710.597 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:20,744] [train step32670] D loss: 0.32647 G loss: 2.20052 (0.036 sec/batch, 1785.143 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:21,134] [train step32680] D loss: 0.32780 G loss: 2.12620 (0.038 sec/batch, 1695.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:21,515] [train step32691] D loss: 0.32765 G loss: 2.12784 (0.037 sec/batch, 1731.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:21,904] [train step32700] D loss: 0.32767 G loss: 2.50409 (0.039 sec/batch, 1645.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:22,289] [train step32711] D loss: 0.32642 G loss: 2.41987 (0.039 sec/batch, 1659.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:22,678] [train step32720] D loss: 0.32582 G loss: 2.37109 (0.041 sec/batch, 1574.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:23,054] [train step32730] D loss: 0.32581 G loss: 2.24368 (0.038 sec/batch, 1687.806 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:23,436] [train step32740] D loss: 0.32685 G loss: 2.17101 (0.034 sec/batch, 1872.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:23,826] [train step32751] D loss: 0.32693 G loss: 2.15098 (0.041 sec/batch, 1576.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:24,213] [train step32760] D loss: 0.32774 G loss: 2.50732 (0.039 sec/batch, 1648.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:24,593] [train step32770] D loss: 0.32790 G loss: 2.50987 (0.037 sec/batch, 1720.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:24,983] [train step32781] D loss: 0.32693 G loss: 2.45871 (0.037 sec/batch, 1709.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:25,358] [train step32790] D loss: 0.32691 G loss: 2.15131 (0.036 sec/batch, 1760.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:25,739] [train step32800] D loss: 0.32629 G loss: 2.19663 (0.048 sec/batch, 1323.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:26,115] [train step32810] D loss: 0.32698 G loss: 2.15666 (0.040 sec/batch, 1595.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:26,487] [train step32820] D loss: 0.32617 G loss: 2.36147 (0.040 sec/batch, 1608.959 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:26,869] [train step32830] D loss: 0.32680 G loss: 2.16261 (0.034 sec/batch, 1890.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:27,243] [train step32840] D loss: 0.32568 G loss: 2.26141 (0.035 sec/batch, 1835.983 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:27,631] [train step32850] D loss: 0.32577 G loss: 2.24838 (0.038 sec/batch, 1705.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:28,021] [train step32860] D loss: 0.32622 G loss: 2.20251 (0.036 sec/batch, 1778.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:28,384] [train step32871] D loss: 0.32634 G loss: 2.23106 (0.035 sec/batch, 1819.716 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:28,768] [train step32880] D loss: 0.32617 G loss: 2.21812 (0.043 sec/batch, 1494.962 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:29,143] [train step32891] D loss: 0.32674 G loss: 2.41684 (0.036 sec/batch, 1792.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:29,525] [train step32901] D loss: 0.32741 G loss: 2.39858 (0.036 sec/batch, 1763.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:29,915] [train step32910] D loss: 0.32559 G loss: 2.27600 (0.044 sec/batch, 1452.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:30,293] [train step32920] D loss: 0.32597 G loss: 2.28070 (0.044 sec/batch, 1443.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:30,676] [train step32931] D loss: 0.32581 G loss: 2.24696 (0.036 sec/batch, 1793.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:31,058] [train step32940] D loss: 0.32627 G loss: 2.39370 (0.037 sec/batch, 1724.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:31,457] [train step32951] D loss: 0.32589 G loss: 2.23028 (0.038 sec/batch, 1700.078 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:31,830] [train step32960] D loss: 0.32579 G loss: 2.26416 (0.037 sec/batch, 1713.108 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:32,210] [train step32970] D loss: 0.32702 G loss: 2.31503 (0.036 sec/batch, 1801.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:32,589] [train step32980] D loss: 0.32823 G loss: 2.42968 (0.034 sec/batch, 1903.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:32,991] [train step32991] D loss: 0.32583 G loss: 2.27823 (0.042 sec/batch, 1510.143 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:33,370] [train step33000] D loss: 0.32665 G loss: 2.28891 (0.038 sec/batch, 1671.714 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:30:33,370] Saved checkpoint at 33000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:33,971] [train step33011] D loss: 0.32715 G loss: 2.18307 (0.046 sec/batch, 1404.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:34,353] [train step33021] D loss: 0.32565 G loss: 2.31537 (0.040 sec/batch, 1590.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:34,736] [train step33030] D loss: 0.32789 G loss: 2.16236 (0.036 sec/batch, 1780.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:35,122] [train step33040] D loss: 0.32632 G loss: 2.36820 (0.039 sec/batch, 1647.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:35,500] [train step33051] D loss: 0.32674 G loss: 2.32701 (0.036 sec/batch, 1759.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:35,878] [train step33060] D loss: 0.32665 G loss: 2.24940 (0.038 sec/batch, 1690.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:36,260] [train step33070] D loss: 0.33472 G loss: 2.64345 (0.034 sec/batch, 1874.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:36,636] [train step33081] D loss: 0.33025 G loss: 2.23424 (0.037 sec/batch, 1753.265 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:37,009] [train step33090] D loss: 0.32839 G loss: 2.44677 (0.040 sec/batch, 1610.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:37,400] [train step33101] D loss: 0.34436 G loss: 2.93693 (0.037 sec/batch, 1716.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:37,787] [train step33110] D loss: 0.32673 G loss: 2.34606 (0.039 sec/batch, 1625.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:38,170] [train step33120] D loss: 0.32600 G loss: 2.27854 (0.038 sec/batch, 1698.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:38,546] [train step33130] D loss: 0.33142 G loss: 2.46228 (0.038 sec/batch, 1696.296 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:38,930] [train step33140] D loss: 0.33966 G loss: 2.11740 (0.038 sec/batch, 1679.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:39,317] [train step33150] D loss: 0.33824 G loss: 2.50750 (0.039 sec/batch, 1650.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:39,697] [train step33161] D loss: 0.33273 G loss: 2.09358 (0.037 sec/batch, 1710.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:40,079] [train step33170] D loss: 0.34851 G loss: 2.32259 (0.042 sec/batch, 1522.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:40,466] [train step33180] D loss: 0.34212 G loss: 2.70494 (0.038 sec/batch, 1686.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:40,844] [train step33190] D loss: 0.32648 G loss: 2.23105 (0.038 sec/batch, 1679.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:41,233] [train step33201] D loss: 0.32832 G loss: 2.43086 (0.035 sec/batch, 1853.631 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:41,620] [train step33210] D loss: 0.33181 G loss: 2.22453 (0.045 sec/batch, 1421.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:42,021] [train step33220] D loss: 0.32696 G loss: 2.39423 (0.036 sec/batch, 1799.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:42,415] [train step33230] D loss: 0.33742 G loss: 2.74106 (0.046 sec/batch, 1389.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:42,791] [train step33240] D loss: 0.33230 G loss: 2.48910 (0.038 sec/batch, 1676.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:43,174] [train step33251] D loss: 0.33936 G loss: 2.58050 (0.039 sec/batch, 1637.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:43,562] [train step33261] D loss: 0.33377 G loss: 2.14529 (0.036 sec/batch, 1770.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:43,947] [train step33270] D loss: 0.34851 G loss: 1.80754 (0.034 sec/batch, 1879.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:44,349] [train step33281] D loss: 0.34832 G loss: 2.67035 (0.039 sec/batch, 1645.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:44,738] [train step33290] D loss: 0.32857 G loss: 2.16302 (0.037 sec/batch, 1718.053 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:45,122] [train step33300] D loss: 0.35056 G loss: 2.19368 (0.040 sec/batch, 1593.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:45,513] [train step33310] D loss: 0.37339 G loss: 2.91951 (0.038 sec/batch, 1665.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:45,900] [train step33321] D loss: 0.33550 G loss: 2.25053 (0.036 sec/batch, 1786.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:46,288] [train step33330] D loss: 0.33475 G loss: 2.17236 (0.040 sec/batch, 1590.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:46,673] [train step33340] D loss: 0.33834 G loss: 2.65931 (0.041 sec/batch, 1558.578 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:47,060] [train step33350] D loss: 0.32688 G loss: 2.29148 (0.040 sec/batch, 1606.494 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:47,455] [train step33360] D loss: 0.33415 G loss: 2.11186 (0.038 sec/batch, 1699.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:47,841] [train step33371] D loss: 0.32844 G loss: 2.35165 (0.039 sec/batch, 1637.101 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:48,223] [train step33380] D loss: 0.32800 G loss: 2.34498 (0.036 sec/batch, 1801.205 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:48,621] [train step33390] D loss: 0.32843 G loss: 2.31177 (0.038 sec/batch, 1676.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:49,008] [train step33400] D loss: 0.32697 G loss: 2.29228 (0.035 sec/batch, 1832.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:49,398] [train step33411] D loss: 0.33829 G loss: 2.53565 (0.044 sec/batch, 1471.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:49,780] [train step33420] D loss: 0.32642 G loss: 2.33086 (0.036 sec/batch, 1756.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:50,156] [train step33430] D loss: 0.33167 G loss: 2.18922 (0.037 sec/batch, 1723.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:50,536] [train step33441] D loss: 0.32968 G loss: 2.49002 (0.036 sec/batch, 1801.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:50,914] [train step33450] D loss: 0.32741 G loss: 2.34915 (0.039 sec/batch, 1635.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:51,290] [train step33461] D loss: 0.32765 G loss: 2.36206 (0.040 sec/batch, 1601.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:51,692] [train step33471] D loss: 0.34269 G loss: 2.43269 (0.036 sec/batch, 1760.775 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:52,081] [train step33480] D loss: 0.33023 G loss: 2.24052 (0.037 sec/batch, 1727.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:52,484] [train step33490] D loss: 0.34730 G loss: 2.75974 (0.048 sec/batch, 1324.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:52,877] [train step33500] D loss: 0.32619 G loss: 2.27402 (0.042 sec/batch, 1538.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:53,254] [train step33510] D loss: 0.32693 G loss: 2.29305 (0.037 sec/batch, 1741.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:53,653] [train step33521] D loss: 0.32821 G loss: 2.19680 (0.035 sec/batch, 1814.440 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:54,034] [train step33531] D loss: 0.32784 G loss: 2.28574 (0.038 sec/batch, 1668.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:54,414] [train step33540] D loss: 0.32721 G loss: 2.22755 (0.038 sec/batch, 1699.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:54,810] [train step33550] D loss: 0.33201 G loss: 2.06521 (0.039 sec/batch, 1623.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:55,197] [train step33561] D loss: 0.39784 G loss: 3.62457 (0.041 sec/batch, 1566.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:55,568] [train step33570] D loss: 0.39384 G loss: 3.51793 (0.040 sec/batch, 1597.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:55,948] [train step33580] D loss: 0.36505 G loss: 2.34320 (0.039 sec/batch, 1632.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:56,321] [train step33590] D loss: 0.34099 G loss: 2.35554 (0.032 sec/batch, 2017.356 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:56,710] [train step33600] D loss: 0.34189 G loss: 2.19357 (0.035 sec/batch, 1803.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:57,079] [train step33611] D loss: 0.45608 G loss: 4.33661 (0.034 sec/batch, 1857.505 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:57,449] [train step33621] D loss: 0.33934 G loss: 2.50978 (0.031 sec/batch, 2062.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:57,846] [train step33630] D loss: 0.34498 G loss: 2.18698 (0.035 sec/batch, 1827.185 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:58,227] [train step33641] D loss: 0.36306 G loss: 2.23218 (0.039 sec/batch, 1637.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:58,602] [train step33650] D loss: 0.34098 G loss: 2.52171 (0.038 sec/batch, 1692.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:58,978] [train step33660] D loss: 0.33316 G loss: 2.29620 (0.036 sec/batch, 1761.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:59,347] [train step33671] D loss: 0.37454 G loss: 1.65711 (0.037 sec/batch, 1750.156 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:30:59,729] [train step33681] D loss: 0.41115 G loss: 3.88874 (0.045 sec/batch, 1428.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:00,099] [train step33690] D loss: 0.75570 G loss: 0.73041 (0.034 sec/batch, 1897.030 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:00,479] [train step33700] D loss: 1.21051 G loss: 12.08142 (0.036 sec/batch, 1769.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:00,865] [train step33711] D loss: 0.59898 G loss: 5.89888 (0.040 sec/batch, 1602.246 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:01,233] [train step33720] D loss: 0.35829 G loss: 2.71391 (0.037 sec/batch, 1708.224 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:01,611] [train step33731] D loss: 0.47981 G loss: 4.54446 (0.036 sec/batch, 1778.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:01,994] [train step33740] D loss: 0.42253 G loss: 3.94338 (0.041 sec/batch, 1544.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:02,373] [train step33750] D loss: 0.34451 G loss: 2.41073 (0.040 sec/batch, 1592.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:02,773] [train step33760] D loss: 0.33635 G loss: 2.57383 (0.052 sec/batch, 1230.565 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:03,160] [train step33771] D loss: 0.33233 G loss: 2.24640 (0.033 sec/batch, 1926.699 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:03,535] [train step33780] D loss: 0.33092 G loss: 2.24905 (0.037 sec/batch, 1739.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:03,919] [train step33791] D loss: 0.33484 G loss: 2.62480 (0.033 sec/batch, 1911.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:04,297] [train step33801] D loss: 0.32959 G loss: 2.26857 (0.042 sec/batch, 1541.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:04,678] [train step33810] D loss: 0.32836 G loss: 2.30007 (0.042 sec/batch, 1541.014 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:05,070] [train step33821] D loss: 0.32858 G loss: 2.24816 (0.037 sec/batch, 1730.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:05,444] [train step33831] D loss: 0.33195 G loss: 2.54046 (0.033 sec/batch, 1945.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:05,824] [train step33840] D loss: 0.32843 G loss: 2.44377 (0.038 sec/batch, 1679.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:06,202] [train step33850] D loss: 0.32939 G loss: 2.46192 (0.031 sec/batch, 2031.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:06,587] [train step33861] D loss: 0.32903 G loss: 2.24109 (0.037 sec/batch, 1711.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:06,991] [train step33870] D loss: 0.33148 G loss: 2.58452 (0.037 sec/batch, 1723.347 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:07,373] [train step33880] D loss: 0.32908 G loss: 2.24252 (0.040 sec/batch, 1594.205 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:07,759] [train step33890] D loss: 0.33037 G loss: 2.26288 (0.046 sec/batch, 1396.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:08,145] [train step33900] D loss: 0.32818 G loss: 2.34462 (0.036 sec/batch, 1755.754 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:08,533] [train step33910] D loss: 0.33300 G loss: 2.20393 (0.038 sec/batch, 1666.287 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:08,913] [train step33920] D loss: 0.32914 G loss: 2.33761 (0.040 sec/batch, 1594.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:09,301] [train step33930] D loss: 0.32934 G loss: 2.38326 (0.038 sec/batch, 1685.157 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:09,676] [train step33941] D loss: 0.33421 G loss: 2.61936 (0.035 sec/batch, 1827.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:10,069] [train step33950] D loss: 0.32882 G loss: 2.34902 (0.038 sec/batch, 1684.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:10,457] [train step33960] D loss: 0.32970 G loss: 2.38009 (0.038 sec/batch, 1691.188 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:10,834] [train step33970] D loss: 0.33565 G loss: 2.72330 (0.032 sec/batch, 2016.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:11,238] [train step33980] D loss: 0.34042 G loss: 2.82435 (0.046 sec/batch, 1397.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:11,626] [train step33990] D loss: 0.32979 G loss: 2.16308 (0.035 sec/batch, 1837.114 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:12,018] [train step34000] D loss: 0.32758 G loss: 2.31784 (0.042 sec/batch, 1512.636 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:31:12,018] Saved checkpoint at 34000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:12,617] [train step34010] D loss: 0.33317 G loss: 2.06828 (0.030 sec/batch, 2117.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:13,016] [train step34020] D loss: 0.33680 G loss: 2.75445 (0.039 sec/batch, 1652.246 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:13,423] [train step34031] D loss: 0.33650 G loss: 2.73425 (0.037 sec/batch, 1724.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:13,809] [train step34040] D loss: 0.32877 G loss: 2.30985 (0.036 sec/batch, 1794.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:14,214] [train step34050] D loss: 0.32954 G loss: 2.42432 (0.038 sec/batch, 1696.886 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:14,593] [train step34061] D loss: 0.32806 G loss: 2.39291 (0.035 sec/batch, 1814.268 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:14,975] [train step34071] D loss: 0.32896 G loss: 2.34104 (0.040 sec/batch, 1610.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:15,377] [train step34080] D loss: 0.32748 G loss: 2.32356 (0.038 sec/batch, 1667.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:15,761] [train step34091] D loss: 0.32865 G loss: 2.37085 (0.034 sec/batch, 1869.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:16,151] [train step34100] D loss: 0.32966 G loss: 2.20455 (0.040 sec/batch, 1611.296 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:16,540] [train step34110] D loss: 0.32760 G loss: 2.36005 (0.037 sec/batch, 1731.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:16,920] [train step34121] D loss: 0.32877 G loss: 2.29574 (0.044 sec/batch, 1452.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:17,315] [train step34131] D loss: 0.32837 G loss: 2.25186 (0.040 sec/batch, 1593.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:17,699] [train step34140] D loss: 0.32944 G loss: 2.19655 (0.037 sec/batch, 1708.387 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:18,091] [train step34150] D loss: 0.33080 G loss: 2.12475 (0.041 sec/batch, 1568.230 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:18,483] [train step34161] D loss: 0.33003 G loss: 2.20832 (0.038 sec/batch, 1700.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:18,870] [train step34170] D loss: 0.33203 G loss: 2.60989 (0.037 sec/batch, 1747.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:19,273] [train step34180] D loss: 0.32794 G loss: 2.33474 (0.038 sec/batch, 1666.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:19,666] [train step34190] D loss: 0.32838 G loss: 2.24078 (0.033 sec/batch, 1922.215 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:20,059] [train step34200] D loss: 0.32989 G loss: 2.55269 (0.041 sec/batch, 1557.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:20,453] [train step34211] D loss: 0.33099 G loss: 2.59829 (0.043 sec/batch, 1505.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:20,846] [train step34221] D loss: 0.32804 G loss: 2.35181 (0.040 sec/batch, 1615.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:21,224] [train step34230] D loss: 0.32819 G loss: 2.42996 (0.035 sec/batch, 1810.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:21,610] [train step34241] D loss: 0.32774 G loss: 2.37673 (0.033 sec/batch, 1958.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:22,001] [train step34250] D loss: 0.32792 G loss: 2.32958 (0.037 sec/batch, 1719.440 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:22,399] [train step34260] D loss: 0.32728 G loss: 2.31247 (0.038 sec/batch, 1679.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:22,780] [train step34271] D loss: 0.33031 G loss: 2.11419 (0.043 sec/batch, 1504.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:23,170] [train step34280] D loss: 0.32833 G loss: 2.20639 (0.039 sec/batch, 1657.449 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:23,555] [train step34290] D loss: 0.32882 G loss: 2.45470 (0.029 sec/batch, 2226.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:23,961] [train step34301] D loss: 0.32776 G loss: 2.28774 (0.040 sec/batch, 1606.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:24,346] [train step34311] D loss: 0.32707 G loss: 2.35440 (0.036 sec/batch, 1797.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:24,742] [train step34320] D loss: 0.32823 G loss: 2.30820 (0.035 sec/batch, 1810.108 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:25,132] [train step34331] D loss: 0.32788 G loss: 2.30902 (0.042 sec/batch, 1518.558 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:25,533] [train step34341] D loss: 0.32747 G loss: 2.25868 (0.041 sec/batch, 1551.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:25,915] [train step34350] D loss: 0.32720 G loss: 2.27443 (0.047 sec/batch, 1351.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:26,297] [train step34361] D loss: 0.32844 G loss: 2.20236 (0.042 sec/batch, 1509.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:26,683] [train step34371] D loss: 0.33041 G loss: 2.12509 (0.040 sec/batch, 1592.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:27,061] [train step34380] D loss: 0.32936 G loss: 2.51965 (0.036 sec/batch, 1754.709 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:27,433] [train step34390] D loss: 0.32744 G loss: 2.35979 (0.034 sec/batch, 1871.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:27,838] [train step34401] D loss: 0.32742 G loss: 2.28042 (0.038 sec/batch, 1703.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:28,238] [train step34410] D loss: 0.32867 G loss: 2.19639 (0.046 sec/batch, 1401.788 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:28,627] [train step34421] D loss: 0.32918 G loss: 2.16345 (0.038 sec/batch, 1691.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:29,015] [train step34430] D loss: 0.33081 G loss: 2.10309 (0.040 sec/batch, 1605.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:29,397] [train step34440] D loss: 0.33017 G loss: 2.56258 (0.038 sec/batch, 1664.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:29,781] [train step34451] D loss: 0.32884 G loss: 2.12949 (0.030 sec/batch, 2156.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:30,163] [train step34461] D loss: 0.32833 G loss: 2.17065 (0.037 sec/batch, 1743.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:30,542] [train step34470] D loss: 0.32732 G loss: 2.42078 (0.037 sec/batch, 1733.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:30,919] [train step34480] D loss: 0.32742 G loss: 2.35508 (0.029 sec/batch, 2192.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:31,290] [train step34490] D loss: 0.32745 G loss: 2.22287 (0.034 sec/batch, 1901.950 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:31,683] [train step34500] D loss: 0.33038 G loss: 2.55620 (0.036 sec/batch, 1765.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:32,062] [train step34511] D loss: 0.32793 G loss: 2.23506 (0.042 sec/batch, 1531.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:32,444] [train step34520] D loss: 0.32783 G loss: 2.16978 (0.040 sec/batch, 1604.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:32,840] [train step34530] D loss: 0.32859 G loss: 2.48493 (0.041 sec/batch, 1552.547 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:33,223] [train step34541] D loss: 0.32762 G loss: 2.24922 (0.043 sec/batch, 1482.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:33,603] [train step34550] D loss: 0.32716 G loss: 2.39407 (0.039 sec/batch, 1647.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:33,989] [train step34560] D loss: 0.32770 G loss: 2.22437 (0.039 sec/batch, 1660.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:34,373] [train step34570] D loss: 0.32710 G loss: 2.27188 (0.042 sec/batch, 1540.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:34,785] [train step34581] D loss: 0.32857 G loss: 2.46529 (0.038 sec/batch, 1691.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:35,169] [train step34590] D loss: 0.32742 G loss: 2.23146 (0.038 sec/batch, 1702.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:35,550] [train step34600] D loss: 0.32650 G loss: 2.30148 (0.040 sec/batch, 1601.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:35,948] [train step34610] D loss: 0.32668 G loss: 2.31732 (0.039 sec/batch, 1647.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:36,335] [train step34620] D loss: 0.32748 G loss: 2.22782 (0.037 sec/batch, 1749.210 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:36,728] [train step34630] D loss: 0.32703 G loss: 2.31847 (0.045 sec/batch, 1407.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:37,111] [train step34641] D loss: 0.32779 G loss: 2.44303 (0.036 sec/batch, 1753.781 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:37,499] [train step34650] D loss: 0.32680 G loss: 2.28493 (0.040 sec/batch, 1618.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:37,899] [train step34661] D loss: 0.32698 G loss: 2.41640 (0.041 sec/batch, 1572.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:38,279] [train step34671] D loss: 0.32699 G loss: 2.38673 (0.036 sec/batch, 1767.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:38,673] [train step34680] D loss: 0.32650 G loss: 2.30990 (0.043 sec/batch, 1498.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:39,067] [train step34691] D loss: 0.32661 G loss: 2.35351 (0.041 sec/batch, 1564.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:39,442] [train step34701] D loss: 0.32672 G loss: 2.37914 (0.034 sec/batch, 1880.366 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:39,835] [train step34710] D loss: 0.32675 G loss: 2.24105 (0.051 sec/batch, 1254.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:40,232] [train step34720] D loss: 0.32683 G loss: 2.34035 (0.039 sec/batch, 1625.709 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:40,610] [train step34731] D loss: 0.32697 G loss: 2.39812 (0.036 sec/batch, 1766.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:41,007] [train step34740] D loss: 0.32639 G loss: 2.30094 (0.045 sec/batch, 1415.269 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:41,397] [train step34751] D loss: 0.32700 G loss: 2.39427 (0.042 sec/batch, 1533.041 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:41,784] [train step34761] D loss: 0.32731 G loss: 2.21244 (0.037 sec/batch, 1706.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:42,173] [train step34770] D loss: 0.32741 G loss: 2.40631 (0.034 sec/batch, 1855.848 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:42,559] [train step34780] D loss: 0.32651 G loss: 2.35325 (0.041 sec/batch, 1569.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:42,951] [train step34790] D loss: 0.32676 G loss: 2.34700 (0.036 sec/batch, 1784.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:43,351] [train step34800] D loss: 0.32670 G loss: 2.35774 (0.040 sec/batch, 1604.832 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:43,760] [train step34811] D loss: 0.32640 G loss: 2.34683 (0.038 sec/batch, 1668.234 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:44,169] [train step34820] D loss: 0.32633 G loss: 2.30864 (0.039 sec/batch, 1627.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:44,560] [train step34830] D loss: 0.32629 G loss: 2.30694 (0.032 sec/batch, 2014.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:44,989] [train step34840] D loss: 0.32719 G loss: 2.41768 (0.048 sec/batch, 1325.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:45,378] [train step34850] D loss: 0.32681 G loss: 2.32942 (0.040 sec/batch, 1600.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:45,773] [train step34860] D loss: 0.32672 G loss: 2.23857 (0.042 sec/batch, 1515.463 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:46,172] [train step34871] D loss: 0.32687 G loss: 2.34916 (0.037 sec/batch, 1734.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:46,565] [train step34881] D loss: 0.32741 G loss: 2.46293 (0.036 sec/batch, 1768.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:47,004] [train step34890] D loss: 0.32643 G loss: 2.34571 (0.049 sec/batch, 1313.054 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:47,415] [train step34901] D loss: 0.32684 G loss: 2.40609 (0.041 sec/batch, 1580.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:47,808] [train step34911] D loss: 0.32699 G loss: 2.24893 (0.038 sec/batch, 1690.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:48,208] [train step34920] D loss: 0.32651 G loss: 2.40015 (0.038 sec/batch, 1705.327 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:48,609] [train step34930] D loss: 0.32624 G loss: 2.28020 (0.042 sec/batch, 1525.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:49,004] [train step34941] D loss: 0.32701 G loss: 2.21154 (0.034 sec/batch, 1889.074 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:49,422] [train step34950] D loss: 0.32655 G loss: 2.38845 (0.037 sec/batch, 1740.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:49,825] [train step34960] D loss: 0.32657 G loss: 2.30535 (0.039 sec/batch, 1657.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:50,231] [train step34970] D loss: 0.32650 G loss: 2.23013 (0.041 sec/batch, 1556.734 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:50,630] [train step34980] D loss: 0.32638 G loss: 2.41623 (0.037 sec/batch, 1736.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:51,034] [train step34990] D loss: 0.32626 G loss: 2.31140 (0.037 sec/batch, 1748.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:51,440] [train step35001] D loss: 0.32646 G loss: 2.27387 (0.038 sec/batch, 1686.905 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:31:51,440] Saved checkpoint at 35000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:52,070] [train step35010] D loss: 0.32752 G loss: 2.18914 (0.036 sec/batch, 1798.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:52,475] [train step35020] D loss: 0.32661 G loss: 2.24010 (0.037 sec/batch, 1711.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:52,865] [train step35031] D loss: 0.32710 G loss: 2.41512 (0.037 sec/batch, 1752.418 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:53,267] [train step35040] D loss: 0.32633 G loss: 2.29456 (0.036 sec/batch, 1767.185 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:53,662] [train step35050] D loss: 0.32901 G loss: 2.53376 (0.038 sec/batch, 1703.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:54,059] [train step35061] D loss: 0.32673 G loss: 2.31526 (0.036 sec/batch, 1758.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:54,468] [train step35070] D loss: 0.32657 G loss: 2.36377 (0.040 sec/batch, 1602.944 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:54,857] [train step35081] D loss: 0.32672 G loss: 2.27298 (0.042 sec/batch, 1508.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:55,266] [train step35091] D loss: 0.32656 G loss: 2.23314 (0.047 sec/batch, 1357.558 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:55,687] [train step35100] D loss: 0.32676 G loss: 2.20280 (0.045 sec/batch, 1428.122 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:56,093] [train step35111] D loss: 0.32623 G loss: 2.30954 (0.039 sec/batch, 1625.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:56,498] [train step35121] D loss: 0.32621 G loss: 2.35509 (0.033 sec/batch, 1965.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:56,897] [train step35130] D loss: 0.32663 G loss: 2.25164 (0.035 sec/batch, 1846.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:57,308] [train step35141] D loss: 0.32613 G loss: 2.30227 (0.041 sec/batch, 1578.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:57,693] [train step35151] D loss: 0.32602 G loss: 2.31324 (0.039 sec/batch, 1645.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:58,077] [train step35160] D loss: 0.32612 G loss: 2.34770 (0.038 sec/batch, 1692.126 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:58,463] [train step35171] D loss: 0.32628 G loss: 2.25280 (0.045 sec/batch, 1430.420 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:58,842] [train step35181] D loss: 0.32645 G loss: 2.40169 (0.037 sec/batch, 1720.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:59,219] [train step35190] D loss: 0.32622 G loss: 2.34411 (0.039 sec/batch, 1635.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:59,605] [train step35200] D loss: 0.32629 G loss: 2.27539 (0.047 sec/batch, 1362.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:31:59,992] [train step35210] D loss: 0.32695 G loss: 2.18655 (0.037 sec/batch, 1731.440 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:00,368] [train step35220] D loss: 0.32690 G loss: 2.39361 (0.041 sec/batch, 1572.133 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:00,752] [train step35230] D loss: 0.32658 G loss: 2.36677 (0.031 sec/batch, 2058.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:01,131] [train step35241] D loss: 0.32684 G loss: 2.29064 (0.030 sec/batch, 2146.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:01,528] [train step35250] D loss: 0.32646 G loss: 2.37274 (0.038 sec/batch, 1668.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:01,907] [train step35260] D loss: 0.32623 G loss: 2.29938 (0.038 sec/batch, 1684.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:02,277] [train step35270] D loss: 0.32645 G loss: 2.35161 (0.034 sec/batch, 1895.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:02,665] [train step35280] D loss: 0.32604 G loss: 2.24408 (0.037 sec/batch, 1730.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:03,046] [train step35291] D loss: 0.32627 G loss: 2.29706 (0.035 sec/batch, 1835.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:03,432] [train step35300] D loss: 0.32581 G loss: 2.29122 (0.040 sec/batch, 1609.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:03,826] [train step35310] D loss: 0.32619 G loss: 2.24812 (0.040 sec/batch, 1605.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:04,215] [train step35320] D loss: 0.32610 G loss: 2.33094 (0.039 sec/batch, 1650.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:04,604] [train step35331] D loss: 0.32634 G loss: 2.24906 (0.041 sec/batch, 1543.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:04,984] [train step35340] D loss: 0.32661 G loss: 2.32836 (0.038 sec/batch, 1678.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:05,360] [train step35351] D loss: 0.32643 G loss: 2.23756 (0.037 sec/batch, 1707.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:05,751] [train step35360] D loss: 0.32674 G loss: 2.22049 (0.038 sec/batch, 1674.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:06,148] [train step35370] D loss: 0.32687 G loss: 2.26875 (0.047 sec/batch, 1374.351 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:06,534] [train step35380] D loss: 0.32639 G loss: 2.30915 (0.044 sec/batch, 1443.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:06,917] [train step35391] D loss: 0.32684 G loss: 2.38246 (0.040 sec/batch, 1619.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:07,309] [train step35400] D loss: 0.32668 G loss: 2.40320 (0.038 sec/batch, 1693.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:07,694] [train step35411] D loss: 0.32599 G loss: 2.30502 (0.035 sec/batch, 1847.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:08,076] [train step35421] D loss: 0.32635 G loss: 2.28969 (0.036 sec/batch, 1768.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:08,453] [train step35430] D loss: 0.32635 G loss: 2.26084 (0.036 sec/batch, 1775.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:08,844] [train step35441] D loss: 0.32658 G loss: 2.31896 (0.039 sec/batch, 1629.657 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:09,227] [train step35450] D loss: 0.32652 G loss: 2.28437 (0.042 sec/batch, 1528.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:09,607] [train step35460] D loss: 0.32618 G loss: 2.29044 (0.040 sec/batch, 1602.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:10,008] [train step35470] D loss: 0.32633 G loss: 2.35390 (0.039 sec/batch, 1628.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:10,390] [train step35480] D loss: 0.32650 G loss: 2.41039 (0.038 sec/batch, 1702.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:10,782] [train step35490] D loss: 0.32610 G loss: 2.29702 (0.036 sec/batch, 1797.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:11,172] [train step35500] D loss: 0.32608 G loss: 2.33843 (0.038 sec/batch, 1686.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:11,554] [train step35510] D loss: 0.32657 G loss: 2.28877 (0.040 sec/batch, 1593.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:11,942] [train step35520] D loss: 0.32598 G loss: 2.33149 (0.037 sec/batch, 1711.732 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:12,324] [train step35530] D loss: 0.32651 G loss: 2.34206 (0.038 sec/batch, 1693.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:12,701] [train step35541] D loss: 0.32597 G loss: 2.24248 (0.035 sec/batch, 1848.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:13,101] [train step35550] D loss: 0.32637 G loss: 2.27719 (0.038 sec/batch, 1701.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:13,490] [train step35561] D loss: 0.32636 G loss: 2.35504 (0.038 sec/batch, 1699.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:13,880] [train step35571] D loss: 0.32568 G loss: 2.31126 (0.038 sec/batch, 1705.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:14,264] [train step35580] D loss: 0.32673 G loss: 2.40002 (0.039 sec/batch, 1660.874 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:14,671] [train step35591] D loss: 0.32576 G loss: 2.29617 (0.038 sec/batch, 1666.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:15,071] [train step35601] D loss: 0.32615 G loss: 2.33659 (0.035 sec/batch, 1831.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:15,464] [train step35610] D loss: 0.32627 G loss: 2.28127 (0.039 sec/batch, 1650.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:15,856] [train step35620] D loss: 0.32627 G loss: 2.30811 (0.051 sec/batch, 1261.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:16,237] [train step35630] D loss: 0.32588 G loss: 2.33216 (0.039 sec/batch, 1628.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:16,621] [train step35640] D loss: 0.32633 G loss: 2.26255 (0.035 sec/batch, 1848.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:17,033] [train step35651] D loss: 0.32702 G loss: 2.41610 (0.038 sec/batch, 1670.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:17,417] [train step35660] D loss: 0.32642 G loss: 2.22590 (0.040 sec/batch, 1594.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:17,799] [train step35670] D loss: 0.32751 G loss: 2.41305 (0.034 sec/batch, 1893.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:18,197] [train step35681] D loss: 0.32678 G loss: 2.19941 (0.040 sec/batch, 1610.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:18,582] [train step35690] D loss: 0.32618 G loss: 2.32555 (0.037 sec/batch, 1719.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:18,964] [train step35700] D loss: 0.32727 G loss: 2.20969 (0.045 sec/batch, 1426.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:19,352] [train step35711] D loss: 0.32614 G loss: 2.29645 (0.039 sec/batch, 1625.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:19,734] [train step35720] D loss: 0.32586 G loss: 2.33043 (0.037 sec/batch, 1707.409 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:20,127] [train step35730] D loss: 0.32610 G loss: 2.31196 (0.032 sec/batch, 2005.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:20,520] [train step35740] D loss: 0.32605 G loss: 2.34399 (0.040 sec/batch, 1606.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:20,904] [train step35751] D loss: 0.32618 G loss: 2.31729 (0.040 sec/batch, 1605.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:21,289] [train step35760] D loss: 0.32681 G loss: 2.35950 (0.034 sec/batch, 1889.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:21,685] [train step35770] D loss: 0.32670 G loss: 2.32030 (0.038 sec/batch, 1706.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:22,077] [train step35781] D loss: 0.32669 G loss: 2.21816 (0.038 sec/batch, 1694.615 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:22,475] [train step35790] D loss: 0.32663 G loss: 2.35999 (0.040 sec/batch, 1588.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:22,862] [train step35801] D loss: 0.32672 G loss: 2.19095 (0.035 sec/batch, 1849.137 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:23,253] [train step35810] D loss: 0.32616 G loss: 2.30452 (0.036 sec/batch, 1779.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:23,641] [train step35820] D loss: 0.32631 G loss: 2.30905 (0.037 sec/batch, 1713.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:24,020] [train step35831] D loss: 0.32618 G loss: 2.28262 (0.039 sec/batch, 1631.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:24,407] [train step35841] D loss: 0.32829 G loss: 2.46360 (0.038 sec/batch, 1706.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:24,805] [train step35850] D loss: 0.32729 G loss: 2.36581 (0.040 sec/batch, 1602.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:25,203] [train step35860] D loss: 0.32605 G loss: 2.27722 (0.036 sec/batch, 1781.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:25,586] [train step35870] D loss: 0.32594 G loss: 2.26181 (0.037 sec/batch, 1717.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:25,959] [train step35880] D loss: 0.32619 G loss: 2.24352 (0.036 sec/batch, 1788.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:26,355] [train step35890] D loss: 0.32563 G loss: 2.28170 (0.038 sec/batch, 1663.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:26,733] [train step35900] D loss: 0.32614 G loss: 2.37279 (0.041 sec/batch, 1567.268 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:27,113] [train step35910] D loss: 0.32698 G loss: 2.40205 (0.040 sec/batch, 1597.849 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:27,492] [train step35920] D loss: 0.32574 G loss: 2.25730 (0.037 sec/batch, 1724.886 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:27,881] [train step35930] D loss: 0.32606 G loss: 2.31100 (0.038 sec/batch, 1685.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:28,278] [train step35940] D loss: 0.32693 G loss: 2.25067 (0.037 sec/batch, 1714.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:28,665] [train step35951] D loss: 0.32648 G loss: 2.39637 (0.036 sec/batch, 1760.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:29,053] [train step35960] D loss: 0.32559 G loss: 2.29985 (0.037 sec/batch, 1745.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:29,435] [train step35970] D loss: 0.32617 G loss: 2.25956 (0.037 sec/batch, 1730.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:29,822] [train step35980] D loss: 0.32600 G loss: 2.27783 (0.041 sec/batch, 1551.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:30,201] [train step35991] D loss: 0.32608 G loss: 2.33533 (0.032 sec/batch, 1992.085 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:30,594] [train step36000] D loss: 0.32570 G loss: 2.32766 (0.035 sec/batch, 1816.097 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:32:30,594] Saved checkpoint at 36000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:31,185] [train step36011] D loss: 0.32632 G loss: 2.30677 (0.036 sec/batch, 1777.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:31,566] [train step36021] D loss: 0.32614 G loss: 2.24970 (0.035 sec/batch, 1815.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:31,950] [train step36030] D loss: 0.32658 G loss: 2.39985 (0.036 sec/batch, 1790.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:32,341] [train step36040] D loss: 0.32595 G loss: 2.28341 (0.049 sec/batch, 1317.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:32,727] [train step36050] D loss: 0.32575 G loss: 2.26796 (0.046 sec/batch, 1379.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:33,104] [train step36060] D loss: 0.32651 G loss: 2.25940 (0.041 sec/batch, 1576.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:33,499] [train step36070] D loss: 0.32686 G loss: 2.36658 (0.037 sec/batch, 1738.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:33,889] [train step36080] D loss: 0.32595 G loss: 2.30167 (0.040 sec/batch, 1597.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:34,265] [train step36090] D loss: 0.32643 G loss: 2.36752 (0.039 sec/batch, 1644.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:34,661] [train step36101] D loss: 0.32602 G loss: 2.33669 (0.036 sec/batch, 1786.794 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:35,040] [train step36110] D loss: 0.32606 G loss: 2.25249 (0.036 sec/batch, 1780.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:35,429] [train step36120] D loss: 0.32628 G loss: 2.33261 (0.047 sec/batch, 1368.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:35,811] [train step36131] D loss: 0.32586 G loss: 2.33854 (0.036 sec/batch, 1778.319 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:36,189] [train step36141] D loss: 0.32641 G loss: 2.32616 (0.036 sec/batch, 1777.283 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:36,585] [train step36150] D loss: 0.32594 G loss: 2.27694 (0.036 sec/batch, 1774.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:37,007] [train step36161] D loss: 0.32581 G loss: 2.31350 (0.041 sec/batch, 1545.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:37,391] [train step36171] D loss: 0.32631 G loss: 2.32984 (0.038 sec/batch, 1663.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:37,768] [train step36180] D loss: 0.32577 G loss: 2.28598 (0.030 sec/batch, 2118.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:38,174] [train step36190] D loss: 0.32623 G loss: 2.30879 (0.045 sec/batch, 1407.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:38,571] [train step36201] D loss: 0.32565 G loss: 2.27057 (0.036 sec/batch, 1764.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:38,952] [train step36210] D loss: 0.32610 G loss: 2.26603 (0.035 sec/batch, 1821.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:39,335] [train step36220] D loss: 0.32583 G loss: 2.32746 (0.039 sec/batch, 1621.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:39,718] [train step36230] D loss: 0.32692 G loss: 2.40816 (0.037 sec/batch, 1710.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:40,094] [train step36240] D loss: 0.32626 G loss: 2.33659 (0.037 sec/batch, 1752.384 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:40,473] [train step36251] D loss: 0.32629 G loss: 2.36656 (0.041 sec/batch, 1556.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:40,870] [train step36261] D loss: 0.32710 G loss: 2.25542 (0.038 sec/batch, 1690.240 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:41,247] [train step36270] D loss: 0.32582 G loss: 2.31979 (0.038 sec/batch, 1682.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:41,639] [train step36281] D loss: 0.32627 G loss: 2.27078 (0.036 sec/batch, 1796.661 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:42,022] [train step36290] D loss: 0.32549 G loss: 2.29309 (0.039 sec/batch, 1622.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:42,403] [train step36300] D loss: 0.32594 G loss: 2.29238 (0.038 sec/batch, 1692.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:42,788] [train step36311] D loss: 0.32577 G loss: 2.28355 (0.037 sec/batch, 1747.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:43,178] [train step36320] D loss: 0.32604 G loss: 2.30301 (0.039 sec/batch, 1657.971 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:43,560] [train step36330] D loss: 0.32631 G loss: 2.37322 (0.040 sec/batch, 1600.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:43,952] [train step36341] D loss: 0.32822 G loss: 2.23205 (0.037 sec/batch, 1751.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:44,327] [train step36350] D loss: 0.32621 G loss: 2.35859 (0.040 sec/batch, 1610.407 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:44,708] [train step36360] D loss: 0.32643 G loss: 2.20410 (0.036 sec/batch, 1764.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:45,089] [train step36371] D loss: 0.32619 G loss: 2.36401 (0.038 sec/batch, 1705.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:45,468] [train step36381] D loss: 0.32606 G loss: 2.28688 (0.037 sec/batch, 1717.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:45,853] [train step36390] D loss: 0.34207 G loss: 1.91972 (0.035 sec/batch, 1816.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:46,234] [train step36400] D loss: 0.33766 G loss: 2.56782 (0.037 sec/batch, 1728.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:46,615] [train step36411] D loss: 0.33337 G loss: 2.12827 (0.040 sec/batch, 1616.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:47,017] [train step36420] D loss: 0.33422 G loss: 2.58240 (0.037 sec/batch, 1724.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:47,399] [train step36431] D loss: 0.32847 G loss: 2.15961 (0.038 sec/batch, 1668.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:47,788] [train step36441] D loss: 0.32631 G loss: 2.35118 (0.044 sec/batch, 1452.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:48,175] [train step36450] D loss: 0.32693 G loss: 2.41579 (0.037 sec/batch, 1727.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:48,559] [train step36460] D loss: 0.32693 G loss: 2.37977 (0.038 sec/batch, 1676.621 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:48,957] [train step36470] D loss: 0.32664 G loss: 2.20832 (0.041 sec/batch, 1548.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:49,333] [train step36480] D loss: 0.32646 G loss: 2.22300 (0.040 sec/batch, 1601.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:49,705] [train step36491] D loss: 0.32650 G loss: 2.28989 (0.037 sec/batch, 1738.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:50,096] [train step36500] D loss: 0.32642 G loss: 2.33997 (0.035 sec/batch, 1831.824 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:50,488] [train step36510] D loss: 0.32588 G loss: 2.22844 (0.039 sec/batch, 1624.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:50,872] [train step36521] D loss: 0.32602 G loss: 2.33498 (0.044 sec/batch, 1444.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:51,248] [train step36530] D loss: 0.32703 G loss: 2.26327 (0.041 sec/batch, 1570.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:51,635] [train step36540] D loss: 0.32586 G loss: 2.33343 (0.035 sec/batch, 1812.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:52,029] [train step36551] D loss: 0.32620 G loss: 2.25183 (0.037 sec/batch, 1734.606 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:52,413] [train step36561] D loss: 0.32631 G loss: 2.24658 (0.037 sec/batch, 1711.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:52,797] [train step36570] D loss: 0.32966 G loss: 2.14611 (0.035 sec/batch, 1825.173 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:53,190] [train step36581] D loss: 0.32685 G loss: 2.34879 (0.041 sec/batch, 1572.778 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:53,572] [train step36590] D loss: 0.32637 G loss: 2.30417 (0.039 sec/batch, 1641.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:53,976] [train step36600] D loss: 0.32583 G loss: 2.31053 (0.040 sec/batch, 1586.320 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:54,361] [train step36610] D loss: 0.32590 G loss: 2.31206 (0.041 sec/batch, 1547.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:54,741] [train step36620] D loss: 0.32556 G loss: 2.29818 (0.038 sec/batch, 1686.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:55,127] [train step36630] D loss: 0.32617 G loss: 2.33903 (0.028 sec/batch, 2272.816 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:55,518] [train step36641] D loss: 0.32840 G loss: 2.25934 (0.039 sec/batch, 1641.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:55,898] [train step36650] D loss: 0.32776 G loss: 2.41453 (0.036 sec/batch, 1780.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:56,287] [train step36660] D loss: 0.32583 G loss: 2.24385 (0.038 sec/batch, 1673.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:56,667] [train step36671] D loss: 0.32590 G loss: 2.32485 (0.035 sec/batch, 1814.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:57,061] [train step36681] D loss: 0.32564 G loss: 2.30888 (0.044 sec/batch, 1441.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:57,450] [train step36690] D loss: 0.32766 G loss: 2.43616 (0.039 sec/batch, 1644.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:57,816] [train step36701] D loss: 0.32670 G loss: 2.28804 (0.032 sec/batch, 2024.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:58,197] [train step36711] D loss: 0.32582 G loss: 2.30907 (0.035 sec/batch, 1848.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:58,579] [train step36720] D loss: 0.32596 G loss: 2.28715 (0.034 sec/batch, 1862.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:58,953] [train step36730] D loss: 0.32595 G loss: 2.38534 (0.032 sec/batch, 2007.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:59,350] [train step36741] D loss: 0.32589 G loss: 2.27013 (0.035 sec/batch, 1822.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:32:59,718] [train step36750] D loss: 0.32620 G loss: 2.33252 (0.036 sec/batch, 1787.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:00,100] [train step36760] D loss: 0.32638 G loss: 2.29802 (0.043 sec/batch, 1475.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:00,482] [train step36770] D loss: 0.32616 G loss: 2.36914 (0.034 sec/batch, 1864.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:00,857] [train step36780] D loss: 0.32801 G loss: 2.49449 (0.035 sec/batch, 1811.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:01,246] [train step36791] D loss: 0.32680 G loss: 2.18606 (0.034 sec/batch, 1891.696 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:01,630] [train step36800] D loss: 0.32599 G loss: 2.35250 (0.042 sec/batch, 1535.040 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:02,004] [train step36810] D loss: 0.32607 G loss: 2.34808 (0.036 sec/batch, 1766.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:02,394] [train step36821] D loss: 0.32691 G loss: 2.26262 (0.038 sec/batch, 1668.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:02,764] [train step36831] D loss: 0.32605 G loss: 2.35655 (0.042 sec/batch, 1530.489 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:03,138] [train step36840] D loss: 0.32615 G loss: 2.23366 (0.039 sec/batch, 1643.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:03,527] [train step36850] D loss: 0.32683 G loss: 2.38768 (0.034 sec/batch, 1864.549 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:03,906] [train step36861] D loss: 0.32572 G loss: 2.30921 (0.039 sec/batch, 1636.881 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:04,279] [train step36870] D loss: 0.32607 G loss: 2.30090 (0.029 sec/batch, 2207.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:04,669] [train step36881] D loss: 0.32590 G loss: 2.30259 (0.038 sec/batch, 1667.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:05,043] [train step36891] D loss: 0.32679 G loss: 2.25505 (0.038 sec/batch, 1678.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:05,428] [train step36900] D loss: 0.32637 G loss: 2.32728 (0.036 sec/batch, 1765.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:05,806] [train step36910] D loss: 0.32575 G loss: 2.29844 (0.037 sec/batch, 1713.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:06,188] [train step36921] D loss: 0.32696 G loss: 2.19866 (0.038 sec/batch, 1700.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:06,577] [train step36930] D loss: 0.32786 G loss: 2.46530 (0.036 sec/batch, 1775.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:06,959] [train step36941] D loss: 0.32741 G loss: 2.24763 (0.041 sec/batch, 1546.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:07,352] [train step36950] D loss: 0.32797 G loss: 2.36079 (0.038 sec/batch, 1672.505 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:07,730] [train step36960] D loss: 0.32695 G loss: 2.18539 (0.036 sec/batch, 1769.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:08,107] [train step36971] D loss: 0.32631 G loss: 2.34023 (0.037 sec/batch, 1727.729 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:08,493] [train step36980] D loss: 0.32599 G loss: 2.35236 (0.037 sec/batch, 1726.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:08,868] [train step36990] D loss: 0.32748 G loss: 2.22918 (0.038 sec/batch, 1704.244 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:09,254] [train step37000] D loss: 0.32709 G loss: 2.31703 (0.041 sec/batch, 1551.363 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:33:09,255] Saved checkpoint at 37000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:09,867] [train step37010] D loss: 0.32672 G loss: 2.17272 (0.037 sec/batch, 1739.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:10,258] [train step37020] D loss: 0.32664 G loss: 2.39133 (0.037 sec/batch, 1734.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:10,651] [train step37031] D loss: 0.32559 G loss: 2.26319 (0.034 sec/batch, 1874.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:11,044] [train step37040] D loss: 0.32620 G loss: 2.24739 (0.038 sec/batch, 1665.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:11,427] [train step37050] D loss: 0.33045 G loss: 2.06754 (0.043 sec/batch, 1487.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:11,815] [train step37061] D loss: 0.32839 G loss: 2.52201 (0.036 sec/batch, 1759.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:12,195] [train step37071] D loss: 0.32585 G loss: 2.27536 (0.032 sec/batch, 2013.784 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:12,593] [train step37080] D loss: 0.32780 G loss: 2.47042 (0.036 sec/batch, 1758.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:12,962] [train step37090] D loss: 0.32831 G loss: 2.17447 (0.038 sec/batch, 1696.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:13,356] [train step37100] D loss: 0.32601 G loss: 2.24077 (0.043 sec/batch, 1471.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:13,742] [train step37110] D loss: 0.32857 G loss: 2.12378 (0.038 sec/batch, 1691.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:14,125] [train step37121] D loss: 0.32592 G loss: 2.29479 (0.034 sec/batch, 1870.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:14,528] [train step37131] D loss: 0.32618 G loss: 2.38443 (0.042 sec/batch, 1507.074 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:14,902] [train step37140] D loss: 0.32586 G loss: 2.33883 (0.030 sec/batch, 2139.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:15,299] [train step37150] D loss: 0.32588 G loss: 2.31162 (0.038 sec/batch, 1673.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:15,683] [train step37161] D loss: 0.32674 G loss: 2.34960 (0.036 sec/batch, 1788.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:16,059] [train step37170] D loss: 0.32693 G loss: 2.25188 (0.037 sec/batch, 1736.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:16,442] [train step37181] D loss: 0.33496 G loss: 2.71924 (0.039 sec/batch, 1628.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:16,836] [train step37191] D loss: 0.32572 G loss: 2.35425 (0.039 sec/batch, 1620.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:17,212] [train step37200] D loss: 0.32969 G loss: 2.42152 (0.037 sec/batch, 1721.126 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:17,593] [train step37210] D loss: 0.32890 G loss: 2.21194 (0.035 sec/batch, 1829.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:17,992] [train step37221] D loss: 0.33225 G loss: 2.48454 (0.037 sec/batch, 1728.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:18,374] [train step37230] D loss: 0.32929 G loss: 2.08468 (0.043 sec/batch, 1472.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:18,765] [train step37241] D loss: 0.32801 G loss: 2.30245 (0.037 sec/batch, 1727.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:19,150] [train step37251] D loss: 0.32944 G loss: 2.46050 (0.038 sec/batch, 1695.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:19,531] [train step37260] D loss: 0.32607 G loss: 2.23272 (0.038 sec/batch, 1700.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:19,918] [train step37270] D loss: 0.32639 G loss: 2.37104 (0.036 sec/batch, 1771.080 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:20,332] [train step37281] D loss: 0.32773 G loss: 2.26929 (0.040 sec/batch, 1604.151 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:20,729] [train step37290] D loss: 0.32705 G loss: 2.37244 (0.038 sec/batch, 1705.153 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:21,118] [train step37301] D loss: 0.32719 G loss: 2.34637 (0.038 sec/batch, 1704.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:21,503] [train step37311] D loss: 0.32650 G loss: 2.28183 (0.037 sec/batch, 1734.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:21,889] [train step37320] D loss: 0.32641 G loss: 2.24511 (0.036 sec/batch, 1779.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:22,282] [train step37331] D loss: 0.32666 G loss: 2.36623 (0.036 sec/batch, 1756.420 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:22,670] [train step37340] D loss: 0.32711 G loss: 2.38374 (0.038 sec/batch, 1697.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:23,063] [train step37350] D loss: 0.32617 G loss: 2.28665 (0.041 sec/batch, 1549.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:23,440] [train step37361] D loss: 0.32572 G loss: 2.27330 (0.038 sec/batch, 1668.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:23,831] [train step37371] D loss: 0.32649 G loss: 2.27021 (0.035 sec/batch, 1849.404 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:24,210] [train step37380] D loss: 0.32619 G loss: 2.37224 (0.039 sec/batch, 1623.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:24,589] [train step37390] D loss: 0.32617 G loss: 2.35316 (0.038 sec/batch, 1666.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:24,980] [train step37401] D loss: 0.32574 G loss: 2.29678 (0.041 sec/batch, 1549.035 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:25,357] [train step37410] D loss: 0.33132 G loss: 2.24318 (0.037 sec/batch, 1714.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:25,746] [train step37421] D loss: 0.32871 G loss: 2.36348 (0.036 sec/batch, 1773.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:26,132] [train step37431] D loss: 0.33100 G loss: 2.20851 (0.033 sec/batch, 1950.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:26,517] [train step37440] D loss: 0.33063 G loss: 2.51046 (0.040 sec/batch, 1587.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:26,903] [train step37451] D loss: 0.32802 G loss: 2.19331 (0.037 sec/batch, 1750.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:27,285] [train step37460] D loss: 0.32604 G loss: 2.33138 (0.044 sec/batch, 1451.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:27,659] [train step37470] D loss: 0.32641 G loss: 2.23859 (0.038 sec/batch, 1670.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:28,043] [train step37480] D loss: 0.32838 G loss: 2.41671 (0.037 sec/batch, 1748.014 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:28,412] [train step37491] D loss: 0.32782 G loss: 2.24777 (0.037 sec/batch, 1734.886 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:28,790] [train step37500] D loss: 0.32656 G loss: 2.35729 (0.036 sec/batch, 1791.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:29,171] [train step37510] D loss: 0.32652 G loss: 2.29227 (0.035 sec/batch, 1837.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:29,559] [train step37521] D loss: 0.32613 G loss: 2.37878 (0.035 sec/batch, 1835.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:29,947] [train step37530] D loss: 0.32652 G loss: 2.35148 (0.038 sec/batch, 1689.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:30,314] [train step37540] D loss: 0.32729 G loss: 2.24572 (0.027 sec/batch, 2379.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:30,692] [train step37551] D loss: 0.32645 G loss: 2.24312 (0.034 sec/batch, 1861.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:31,095] [train step37560] D loss: 0.32702 G loss: 2.19909 (0.036 sec/batch, 1768.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:31,472] [train step37571] D loss: 0.32726 G loss: 2.40931 (0.036 sec/batch, 1763.667 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:31,874] [train step37581] D loss: 0.32759 G loss: 2.21863 (0.040 sec/batch, 1599.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:32,265] [train step37590] D loss: 0.32616 G loss: 2.26925 (0.039 sec/batch, 1629.815 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:32,664] [train step37601] D loss: 0.32577 G loss: 2.30538 (0.044 sec/batch, 1439.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:33,056] [train step37611] D loss: 0.32834 G loss: 2.45529 (0.041 sec/batch, 1543.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:33,437] [train step37620] D loss: 0.32579 G loss: 2.30746 (0.035 sec/batch, 1806.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:33,824] [train step37631] D loss: 0.32785 G loss: 2.23808 (0.040 sec/batch, 1604.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:34,229] [train step37640] D loss: 0.33660 G loss: 2.59387 (0.042 sec/batch, 1510.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:34,615] [train step37650] D loss: 0.32746 G loss: 2.13098 (0.035 sec/batch, 1815.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:35,014] [train step37660] D loss: 0.33224 G loss: 2.56387 (0.044 sec/batch, 1465.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:35,416] [train step37671] D loss: 0.32757 G loss: 2.17297 (0.039 sec/batch, 1643.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:35,807] [train step37680] D loss: 0.32834 G loss: 2.21528 (0.045 sec/batch, 1430.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:36,204] [train step37691] D loss: 0.32609 G loss: 2.36438 (0.038 sec/batch, 1705.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:36,590] [train step37701] D loss: 0.32815 G loss: 2.37820 (0.041 sec/batch, 1555.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:36,988] [train step37710] D loss: 0.32667 G loss: 2.40350 (0.038 sec/batch, 1666.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:37,382] [train step37721] D loss: 0.32636 G loss: 2.30307 (0.036 sec/batch, 1761.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:37,767] [train step37731] D loss: 0.32752 G loss: 2.22742 (0.046 sec/batch, 1392.582 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:38,161] [train step37740] D loss: 0.32774 G loss: 2.42048 (0.043 sec/batch, 1505.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:38,542] [train step37750] D loss: 0.32817 G loss: 2.19752 (0.037 sec/batch, 1737.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:38,921] [train step37761] D loss: 0.32676 G loss: 2.40889 (0.047 sec/batch, 1361.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:39,304] [train step37770] D loss: 0.32577 G loss: 2.26298 (0.032 sec/batch, 2029.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:39,691] [train step37780] D loss: 0.32602 G loss: 2.34840 (0.038 sec/batch, 1666.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:40,067] [train step37791] D loss: 0.32822 G loss: 2.43653 (0.039 sec/batch, 1641.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:40,465] [train step37800] D loss: 0.32598 G loss: 2.23844 (0.042 sec/batch, 1524.621 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:40,848] [train step37810] D loss: 0.32594 G loss: 2.35177 (0.040 sec/batch, 1611.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:41,247] [train step37820] D loss: 0.32577 G loss: 2.31451 (0.053 sec/batch, 1202.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:41,644] [train step37830] D loss: 0.32588 G loss: 2.28773 (0.036 sec/batch, 1761.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:42,021] [train step37840] D loss: 0.32940 G loss: 2.49171 (0.037 sec/batch, 1751.378 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:42,416] [train step37850] D loss: 0.32644 G loss: 2.26681 (0.040 sec/batch, 1586.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:42,790] [train step37860] D loss: 0.32686 G loss: 2.22307 (0.033 sec/batch, 1965.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:43,180] [train step37870] D loss: 0.32754 G loss: 2.42364 (0.038 sec/batch, 1704.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:43,567] [train step37881] D loss: 0.32698 G loss: 2.40342 (0.040 sec/batch, 1589.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:43,943] [train step37890] D loss: 0.32592 G loss: 2.33145 (0.042 sec/batch, 1533.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:44,340] [train step37901] D loss: 0.32795 G loss: 2.26856 (0.047 sec/batch, 1349.166 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:44,720] [train step37910] D loss: 0.32664 G loss: 2.20911 (0.040 sec/batch, 1594.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:45,100] [train step37920] D loss: 0.32785 G loss: 2.19034 (0.034 sec/batch, 1877.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:45,502] [train step37930] D loss: 0.32665 G loss: 2.36927 (0.033 sec/batch, 1966.114 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:45,896] [train step37941] D loss: 0.32752 G loss: 2.27709 (0.036 sec/batch, 1757.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:46,269] [train step37950] D loss: 0.32575 G loss: 2.31698 (0.036 sec/batch, 1761.434 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:46,668] [train step37961] D loss: 0.32676 G loss: 2.22560 (0.037 sec/batch, 1731.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:47,055] [train step37971] D loss: 0.32729 G loss: 2.20862 (0.035 sec/batch, 1822.199 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:47,444] [train step37980] D loss: 0.32557 G loss: 2.30806 (0.036 sec/batch, 1775.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:47,828] [train step37990] D loss: 0.32573 G loss: 2.30932 (0.037 sec/batch, 1723.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:48,213] [train step38000] D loss: 0.32583 G loss: 2.26118 (0.040 sec/batch, 1583.335 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:33:48,213] Saved checkpoint at 38000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:48,806] [train step38010] D loss: 0.32644 G loss: 2.43377 (0.033 sec/batch, 1945.128 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:49,192] [train step38021] D loss: 0.32583 G loss: 2.26360 (0.035 sec/batch, 1829.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:49,580] [train step38031] D loss: 0.32698 G loss: 2.28330 (0.040 sec/batch, 1614.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:49,958] [train step38040] D loss: 0.32608 G loss: 2.33645 (0.038 sec/batch, 1692.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:50,334] [train step38051] D loss: 0.32803 G loss: 2.42462 (0.035 sec/batch, 1822.954 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:50,732] [train step38061] D loss: 0.32841 G loss: 2.23499 (0.042 sec/batch, 1538.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:51,103] [train step38070] D loss: 0.32714 G loss: 2.33155 (0.033 sec/batch, 1918.328 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:51,495] [train step38080] D loss: 0.32579 G loss: 2.34481 (0.039 sec/batch, 1625.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:51,899] [train step38090] D loss: 0.32693 G loss: 2.36533 (0.038 sec/batch, 1682.074 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:52,294] [train step38100] D loss: 0.32567 G loss: 2.33383 (0.038 sec/batch, 1689.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:52,682] [train step38110] D loss: 0.32614 G loss: 2.41342 (0.041 sec/batch, 1565.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:53,064] [train step38121] D loss: 0.32667 G loss: 2.25860 (0.038 sec/batch, 1663.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:53,446] [train step38130] D loss: 0.33248 G loss: 2.03637 (0.046 sec/batch, 1401.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:53,840] [train step38140] D loss: 0.32951 G loss: 2.45748 (0.040 sec/batch, 1584.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:54,230] [train step38150] D loss: 0.32674 G loss: 2.25340 (0.039 sec/batch, 1639.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:54,618] [train step38160] D loss: 0.33218 G loss: 2.51231 (0.045 sec/batch, 1407.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:55,004] [train step38171] D loss: 0.33102 G loss: 2.13613 (0.037 sec/batch, 1709.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:55,384] [train step38180] D loss: 0.32676 G loss: 2.37206 (0.033 sec/batch, 1933.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:55,777] [train step38190] D loss: 0.32611 G loss: 2.33645 (0.029 sec/batch, 2232.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:56,174] [train step38200] D loss: 0.32863 G loss: 2.43799 (0.041 sec/batch, 1570.680 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:56,555] [train step38210] D loss: 0.32862 G loss: 2.13995 (0.038 sec/batch, 1690.666 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:56,959] [train step38220] D loss: 0.32668 G loss: 2.40499 (0.040 sec/batch, 1615.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:57,329] [train step38230] D loss: 0.32600 G loss: 2.29194 (0.029 sec/batch, 2217.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:57,731] [train step38241] D loss: 0.32598 G loss: 2.24523 (0.042 sec/batch, 1511.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:58,121] [train step38250] D loss: 0.32743 G loss: 2.21209 (0.039 sec/batch, 1640.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:58,499] [train step38260] D loss: 0.32585 G loss: 2.31022 (0.038 sec/batch, 1678.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:58,889] [train step38271] D loss: 0.32644 G loss: 2.37089 (0.035 sec/batch, 1805.106 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:59,263] [train step38280] D loss: 0.32587 G loss: 2.30866 (0.036 sec/batch, 1799.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:33:59,646] [train step38291] D loss: 0.32643 G loss: 2.34156 (0.043 sec/batch, 1505.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:00,026] [train step38301] D loss: 0.32589 G loss: 2.26536 (0.034 sec/batch, 1874.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:00,404] [train step38310] D loss: 0.32647 G loss: 2.36812 (0.037 sec/batch, 1735.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:00,788] [train step38321] D loss: 0.32553 G loss: 2.32049 (0.042 sec/batch, 1520.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:01,173] [train step38331] D loss: 0.32652 G loss: 2.34724 (0.036 sec/batch, 1795.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:01,557] [train step38340] D loss: 0.32699 G loss: 2.22726 (0.039 sec/batch, 1625.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:01,961] [train step38351] D loss: 0.32816 G loss: 2.43887 (0.032 sec/batch, 1987.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:02,337] [train step38360] D loss: 0.32710 G loss: 2.40007 (0.032 sec/batch, 2017.356 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:02,734] [train step38370] D loss: 0.32738 G loss: 2.41387 (0.045 sec/batch, 1431.587 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:03,114] [train step38381] D loss: 0.32615 G loss: 2.34999 (0.039 sec/batch, 1626.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:03,500] [train step38391] D loss: 0.32607 G loss: 2.21383 (0.038 sec/batch, 1705.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:03,886] [train step38400] D loss: 0.32834 G loss: 2.17996 (0.046 sec/batch, 1390.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:04,269] [train step38410] D loss: 0.32667 G loss: 2.37317 (0.029 sec/batch, 2199.876 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:04,649] [train step38420] D loss: 0.32651 G loss: 2.31987 (0.038 sec/batch, 1667.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:05,035] [train step38430] D loss: 0.32784 G loss: 2.39469 (0.033 sec/batch, 1927.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:05,424] [train step38441] D loss: 0.32595 G loss: 2.30708 (0.037 sec/batch, 1722.429 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:05,809] [train step38451] D loss: 0.32620 G loss: 2.31426 (0.036 sec/batch, 1763.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:06,206] [train step38460] D loss: 0.32621 G loss: 2.22490 (0.040 sec/batch, 1601.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:06,589] [train step38471] D loss: 0.32620 G loss: 2.34232 (0.036 sec/batch, 1783.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:06,975] [train step38480] D loss: 0.32567 G loss: 2.34013 (0.048 sec/batch, 1346.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:07,360] [train step38490] D loss: 0.32860 G loss: 2.29434 (0.040 sec/batch, 1613.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:07,734] [train step38500] D loss: 0.32788 G loss: 2.31817 (0.039 sec/batch, 1628.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:08,127] [train step38510] D loss: 0.32663 G loss: 2.31735 (0.041 sec/batch, 1550.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:08,514] [train step38520] D loss: 0.32594 G loss: 2.32650 (0.037 sec/batch, 1738.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:08,900] [train step38530] D loss: 0.32585 G loss: 2.30268 (0.040 sec/batch, 1585.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:09,292] [train step38540] D loss: 0.32630 G loss: 2.23574 (0.038 sec/batch, 1671.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:09,668] [train step38550] D loss: 0.32639 G loss: 2.39864 (0.036 sec/batch, 1761.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:10,057] [train step38561] D loss: 0.32885 G loss: 2.20900 (0.046 sec/batch, 1386.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:10,442] [train step38571] D loss: 0.32758 G loss: 2.43426 (0.037 sec/batch, 1734.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:10,823] [train step38580] D loss: 0.32604 G loss: 2.25232 (0.039 sec/batch, 1657.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:11,207] [train step38590] D loss: 0.32634 G loss: 2.22414 (0.037 sec/batch, 1734.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:11,600] [train step38600] D loss: 0.32597 G loss: 2.33184 (0.040 sec/batch, 1594.849 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:11,987] [train step38610] D loss: 0.32660 G loss: 2.43297 (0.036 sec/batch, 1785.487 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:12,383] [train step38621] D loss: 0.32655 G loss: 2.19814 (0.036 sec/batch, 1783.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:12,761] [train step38630] D loss: 0.32615 G loss: 2.29068 (0.042 sec/batch, 1524.881 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:13,172] [train step38640] D loss: 0.32554 G loss: 2.33761 (0.040 sec/batch, 1588.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:13,578] [train step38651] D loss: 0.32634 G loss: 2.26652 (0.040 sec/batch, 1595.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:13,961] [train step38660] D loss: 0.32571 G loss: 2.25794 (0.040 sec/batch, 1590.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:14,360] [train step38670] D loss: 0.32595 G loss: 2.27458 (0.037 sec/batch, 1741.640 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:14,738] [train step38681] D loss: 0.32599 G loss: 2.30062 (0.034 sec/batch, 1868.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:15,121] [train step38690] D loss: 0.32568 G loss: 2.31814 (0.040 sec/batch, 1599.897 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:15,513] [train step38700] D loss: 0.32620 G loss: 2.27830 (0.036 sec/batch, 1793.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:15,893] [train step38711] D loss: 0.32584 G loss: 2.34067 (0.035 sec/batch, 1823.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:16,286] [train step38720] D loss: 0.32630 G loss: 2.28065 (0.039 sec/batch, 1631.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:16,666] [train step38730] D loss: 0.32586 G loss: 2.37582 (0.040 sec/batch, 1591.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:17,048] [train step38741] D loss: 0.32701 G loss: 2.38184 (0.039 sec/batch, 1641.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:17,437] [train step38750] D loss: 0.32650 G loss: 2.28183 (0.037 sec/batch, 1742.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:17,826] [train step38760] D loss: 0.32585 G loss: 2.31838 (0.037 sec/batch, 1727.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:18,209] [train step38770] D loss: 0.32658 G loss: 2.31727 (0.040 sec/batch, 1594.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:18,604] [train step38780] D loss: 0.32603 G loss: 2.32586 (0.038 sec/batch, 1673.225 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:18,990] [train step38790] D loss: 0.32599 G loss: 2.35863 (0.039 sec/batch, 1643.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:19,380] [train step38801] D loss: 0.32587 G loss: 2.27280 (0.036 sec/batch, 1767.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:19,757] [train step38811] D loss: 0.32680 G loss: 2.36040 (0.035 sec/batch, 1814.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:20,146] [train step38820] D loss: 0.32668 G loss: 2.32390 (0.043 sec/batch, 1493.640 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:20,534] [train step38830] D loss: 0.32639 G loss: 2.28608 (0.037 sec/batch, 1723.546 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:20,910] [train step38841] D loss: 0.32702 G loss: 2.35110 (0.035 sec/batch, 1830.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:21,292] [train step38850] D loss: 0.32884 G loss: 2.12383 (0.035 sec/batch, 1835.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:21,682] [train step38861] D loss: 0.32689 G loss: 2.36192 (0.039 sec/batch, 1622.751 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:22,066] [train step38870] D loss: 0.65711 G loss: 6.55055 (0.038 sec/batch, 1695.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:22,447] [train step38880] D loss: 0.35975 G loss: 1.80552 (0.040 sec/batch, 1588.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:22,826] [train step38891] D loss: 0.34129 G loss: 2.19329 (0.042 sec/batch, 1514.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:23,199] [train step38900] D loss: 0.34492 G loss: 2.93955 (0.036 sec/batch, 1756.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:23,601] [train step38910] D loss: 0.33430 G loss: 2.00262 (0.038 sec/batch, 1670.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:23,975] [train step38920] D loss: 0.32879 G loss: 2.17345 (0.036 sec/batch, 1778.319 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:24,356] [train step38931] D loss: 0.32796 G loss: 2.21871 (0.037 sec/batch, 1713.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:24,741] [train step38940] D loss: 0.32862 G loss: 2.47565 (0.040 sec/batch, 1604.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:25,118] [train step38950] D loss: 0.32851 G loss: 2.46886 (0.030 sec/batch, 2154.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:25,512] [train step38960] D loss: 0.33196 G loss: 2.09689 (0.045 sec/batch, 1419.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:25,905] [train step38970] D loss: 0.34112 G loss: 2.86904 (0.038 sec/batch, 1687.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:26,286] [train step38981] D loss: 0.45622 G loss: 4.44270 (0.039 sec/batch, 1644.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:26,676] [train step38991] D loss: 0.46989 G loss: 1.17064 (0.035 sec/batch, 1803.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:27,056] [train step39000] D loss: 0.58713 G loss: 5.81901 (0.037 sec/batch, 1724.986 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:34:27,056] Saved checkpoint at 39000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:27,655] [train step39010] D loss: 0.53189 G loss: 1.62224 (0.041 sec/batch, 1552.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:28,038] [train step39020] D loss: 0.54879 G loss: 5.44288 (0.036 sec/batch, 1796.228 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:28,425] [train step39030] D loss: 0.37861 G loss: 2.79551 (0.039 sec/batch, 1636.133 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:28,813] [train step39041] D loss: 0.76994 G loss: 7.69346 (0.034 sec/batch, 1898.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:29,187] [train step39050] D loss: 0.36491 G loss: 2.75435 (0.040 sec/batch, 1597.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:29,566] [train step39060] D loss: 0.65373 G loss: 0.79704 (0.039 sec/batch, 1631.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:29,957] [train step39070] D loss: 0.68463 G loss: 6.83393 (0.037 sec/batch, 1751.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:30,333] [train step39081] D loss: 0.38178 G loss: 1.61350 (0.035 sec/batch, 1849.430 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:30,719] [train step39090] D loss: 0.53090 G loss: 5.24589 (0.041 sec/batch, 1577.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:31,092] [train step39101] D loss: 0.36020 G loss: 1.73337 (0.046 sec/batch, 1382.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:31,474] [train step39111] D loss: 0.34217 G loss: 2.80994 (0.039 sec/batch, 1635.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:31,866] [train step39120] D loss: 0.33583 G loss: 2.14589 (0.037 sec/batch, 1735.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:32,250] [train step39130] D loss: 0.33041 G loss: 2.35250 (0.041 sec/batch, 1546.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:32,620] [train step39140] D loss: 0.33352 G loss: 2.63655 (0.038 sec/batch, 1698.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:33,007] [train step39150] D loss: 0.33130 G loss: 2.31674 (0.038 sec/batch, 1687.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:33,385] [train step39160] D loss: 0.32898 G loss: 2.44220 (0.037 sec/batch, 1717.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:33,775] [train step39170] D loss: 0.34149 G loss: 2.84501 (0.044 sec/batch, 1450.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:34,167] [train step39180] D loss: 0.33110 G loss: 2.22305 (0.046 sec/batch, 1380.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:34,556] [train step39191] D loss: 0.33099 G loss: 2.36713 (0.037 sec/batch, 1743.597 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:34,946] [train step39200] D loss: 0.33011 G loss: 2.40001 (0.038 sec/batch, 1676.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:35,329] [train step39210] D loss: 0.32982 G loss: 2.44217 (0.037 sec/batch, 1712.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:35,706] [train step39221] D loss: 0.33324 G loss: 2.63889 (0.037 sec/batch, 1750.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:36,090] [train step39230] D loss: 0.32835 G loss: 2.34305 (0.038 sec/batch, 1684.967 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:36,473] [train step39240] D loss: 0.32936 G loss: 2.40516 (0.037 sec/batch, 1720.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:36,868] [train step39251] D loss: 0.33294 G loss: 2.13396 (0.038 sec/batch, 1704.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:37,252] [train step39261] D loss: 0.33158 G loss: 2.59289 (0.041 sec/batch, 1544.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:37,637] [train step39270] D loss: 0.32884 G loss: 2.21797 (0.036 sec/batch, 1795.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:38,021] [train step39280] D loss: 0.32926 G loss: 2.23563 (0.025 sec/batch, 2583.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:38,417] [train step39290] D loss: 0.32879 G loss: 2.25078 (0.040 sec/batch, 1598.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:38,795] [train step39300] D loss: 0.32821 G loss: 2.32058 (0.036 sec/batch, 1790.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:39,188] [train step39311] D loss: 0.32860 G loss: 2.23478 (0.036 sec/batch, 1762.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:39,569] [train step39321] D loss: 0.33138 G loss: 2.20846 (0.035 sec/batch, 1840.238 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:39,963] [train step39330] D loss: 0.32912 G loss: 2.44217 (0.040 sec/batch, 1595.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:40,355] [train step39340] D loss: 0.32983 G loss: 2.26114 (0.038 sec/batch, 1671.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:40,745] [train step39350] D loss: 0.32843 G loss: 2.26743 (0.039 sec/batch, 1639.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:41,143] [train step39360] D loss: 0.32947 G loss: 2.24125 (0.037 sec/batch, 1736.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:41,526] [train step39371] D loss: 0.33016 G loss: 2.21192 (0.036 sec/batch, 1787.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:41,903] [train step39381] D loss: 0.33059 G loss: 2.17152 (0.034 sec/batch, 1858.817 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:42,297] [train step39390] D loss: 0.32905 G loss: 2.42168 (0.030 sec/batch, 2126.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:42,678] [train step39401] D loss: 0.33032 G loss: 2.29340 (0.039 sec/batch, 1660.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:43,072] [train step39410] D loss: 0.32842 G loss: 2.26952 (0.039 sec/batch, 1658.411 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:43,454] [train step39420] D loss: 0.32747 G loss: 2.28321 (0.039 sec/batch, 1654.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:43,840] [train step39430] D loss: 0.32856 G loss: 2.22022 (0.041 sec/batch, 1577.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:44,238] [train step39440] D loss: 0.32902 G loss: 2.26166 (0.040 sec/batch, 1596.908 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:44,623] [train step39450] D loss: 0.32795 G loss: 2.23981 (0.037 sec/batch, 1720.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:45,018] [train step39460] D loss: 0.32865 G loss: 2.40095 (0.028 sec/batch, 2295.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:45,427] [train step39471] D loss: 0.32892 G loss: 2.37853 (0.039 sec/batch, 1637.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:45,813] [train step39480] D loss: 0.32878 G loss: 2.35042 (0.036 sec/batch, 1773.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:46,203] [train step39490] D loss: 0.32911 G loss: 2.43064 (0.038 sec/batch, 1696.618 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:46,585] [train step39500] D loss: 0.32820 G loss: 2.42514 (0.038 sec/batch, 1695.578 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:46,969] [train step39510] D loss: 0.33044 G loss: 2.14030 (0.041 sec/batch, 1570.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:47,352] [train step39521] D loss: 0.32863 G loss: 2.29980 (0.037 sec/batch, 1749.096 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:47,729] [train step39531] D loss: 0.33315 G loss: 2.64177 (0.038 sec/batch, 1687.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:48,117] [train step39540] D loss: 0.32950 G loss: 2.13079 (0.037 sec/batch, 1731.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:48,505] [train step39550] D loss: 0.32814 G loss: 2.43399 (0.036 sec/batch, 1781.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:48,885] [train step39561] D loss: 0.32967 G loss: 2.49692 (0.040 sec/batch, 1613.427 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:49,276] [train step39570] D loss: 0.32723 G loss: 2.34444 (0.040 sec/batch, 1584.494 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:49,664] [train step39581] D loss: 0.32789 G loss: 2.33229 (0.037 sec/batch, 1716.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:50,047] [train step39590] D loss: 0.32738 G loss: 2.30163 (0.036 sec/batch, 1775.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:50,428] [train step39600] D loss: 0.32816 G loss: 2.42930 (0.027 sec/batch, 2334.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:50,833] [train step39610] D loss: 0.32855 G loss: 2.39718 (0.040 sec/batch, 1600.211 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:51,222] [train step39620] D loss: 0.32862 G loss: 2.32583 (0.036 sec/batch, 1758.583 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:51,620] [train step39630] D loss: 0.32824 G loss: 2.33467 (0.041 sec/batch, 1579.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:52,009] [train step39640] D loss: 0.32814 G loss: 2.33270 (0.039 sec/batch, 1644.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:52,407] [train step39650] D loss: 0.32829 G loss: 2.36414 (0.044 sec/batch, 1461.597 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:52,790] [train step39660] D loss: 0.33157 G loss: 2.13613 (0.038 sec/batch, 1679.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:53,187] [train step39671] D loss: 0.33245 G loss: 2.03907 (0.035 sec/batch, 1833.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:53,585] [train step39681] D loss: 0.32861 G loss: 2.20870 (0.036 sec/batch, 1761.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:53,969] [train step39690] D loss: 0.32837 G loss: 2.39241 (0.039 sec/batch, 1652.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:54,360] [train step39700] D loss: 0.32885 G loss: 2.18113 (0.039 sec/batch, 1645.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:54,743] [train step39711] D loss: 0.32855 G loss: 2.21068 (0.034 sec/batch, 1909.336 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:55,140] [train step39720] D loss: 0.32766 G loss: 2.28115 (0.039 sec/batch, 1635.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:55,549] [train step39730] D loss: 0.32927 G loss: 2.21019 (0.041 sec/batch, 1575.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:55,946] [train step39741] D loss: 0.33020 G loss: 2.55685 (0.037 sec/batch, 1722.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:56,328] [train step39750] D loss: 0.32981 G loss: 2.12739 (0.037 sec/batch, 1733.049 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:56,719] [train step39761] D loss: 0.32753 G loss: 2.31014 (0.035 sec/batch, 1807.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:57,095] [train step39771] D loss: 0.32924 G loss: 2.49031 (0.034 sec/batch, 1864.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:57,487] [train step39780] D loss: 0.33378 G loss: 2.03636 (0.034 sec/batch, 1870.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:57,875] [train step39791] D loss: 0.33104 G loss: 2.10359 (0.034 sec/batch, 1871.765 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:58,264] [train step39800] D loss: 0.32728 G loss: 2.30256 (0.039 sec/batch, 1635.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:58,649] [train step39810] D loss: 0.32764 G loss: 2.32308 (0.026 sec/batch, 2451.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:59,032] [train step39820] D loss: 0.32731 G loss: 2.37626 (0.034 sec/batch, 1888.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:59,406] [train step39831] D loss: 0.32777 G loss: 2.43191 (0.034 sec/batch, 1869.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:34:59,796] [train step39840] D loss: 0.32849 G loss: 2.20669 (0.043 sec/batch, 1500.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:00,162] [train step39851] D loss: 0.32771 G loss: 2.28485 (0.034 sec/batch, 1856.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:00,540] [train step39861] D loss: 0.32850 G loss: 2.17452 (0.034 sec/batch, 1873.568 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:00,918] [train step39870] D loss: 0.32949 G loss: 2.51796 (0.036 sec/batch, 1798.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:01,287] [train step39881] D loss: 0.32827 G loss: 2.47486 (0.036 sec/batch, 1782.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:01,678] [train step39891] D loss: 0.32919 G loss: 2.15563 (0.041 sec/batch, 1578.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:02,059] [train step39900] D loss: 0.33003 G loss: 2.53700 (0.038 sec/batch, 1692.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:02,445] [train step39910] D loss: 0.32700 G loss: 2.32443 (0.036 sec/batch, 1758.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:02,830] [train step39920] D loss: 0.32808 G loss: 2.25439 (0.037 sec/batch, 1726.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:03,209] [train step39930] D loss: 0.32832 G loss: 2.47676 (0.037 sec/batch, 1726.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:03,599] [train step39940] D loss: 0.32922 G loss: 2.54029 (0.049 sec/batch, 1316.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:03,974] [train step39950] D loss: 0.33243 G loss: 2.64454 (0.042 sec/batch, 1523.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:04,351] [train step39960] D loss: 0.32848 G loss: 2.19856 (0.037 sec/batch, 1721.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:04,738] [train step39971] D loss: 0.32770 G loss: 2.32842 (0.037 sec/batch, 1748.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:05,122] [train step39980] D loss: 0.32877 G loss: 2.36351 (0.045 sec/batch, 1430.618 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:05,513] [train step39990] D loss: 0.32745 G loss: 2.21999 (0.039 sec/batch, 1621.163 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:05,914] [train step40001] D loss: 0.32798 G loss: 2.20257 (0.046 sec/batch, 1379.521 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:35:05,914] Saved checkpoint at 40000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:06,530] [train step40010] D loss: 0.32734 G loss: 2.26481 (0.039 sec/batch, 1626.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:06,931] [train step40020] D loss: 0.32782 G loss: 2.35877 (0.040 sec/batch, 1595.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:07,317] [train step40030] D loss: 0.32761 G loss: 2.26247 (0.034 sec/batch, 1885.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:07,700] [train step40041] D loss: 0.32705 G loss: 2.23609 (0.039 sec/batch, 1641.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:08,083] [train step40050] D loss: 0.32714 G loss: 2.39220 (0.039 sec/batch, 1635.515 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:08,462] [train step40061] D loss: 0.32851 G loss: 2.16673 (0.033 sec/batch, 1957.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:08,854] [train step40071] D loss: 0.32953 G loss: 2.13906 (0.038 sec/batch, 1702.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:09,230] [train step40080] D loss: 0.32876 G loss: 2.49675 (0.039 sec/batch, 1660.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:09,621] [train step40090] D loss: 0.32908 G loss: 2.50897 (0.039 sec/batch, 1622.702 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:10,010] [train step40101] D loss: 0.32729 G loss: 2.37685 (0.044 sec/batch, 1454.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:10,398] [train step40110] D loss: 0.32730 G loss: 2.38335 (0.035 sec/batch, 1817.302 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:10,789] [train step40121] D loss: 0.33095 G loss: 2.59190 (0.039 sec/batch, 1632.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:11,180] [train step40131] D loss: 0.32822 G loss: 2.35911 (0.037 sec/batch, 1738.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:11,565] [train step40140] D loss: 0.32740 G loss: 2.38424 (0.034 sec/batch, 1896.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:11,948] [train step40150] D loss: 0.32741 G loss: 2.25889 (0.028 sec/batch, 2256.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:12,337] [train step40160] D loss: 0.32747 G loss: 2.22627 (0.033 sec/batch, 1920.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:12,734] [train step40170] D loss: 0.32744 G loss: 2.25928 (0.046 sec/batch, 1396.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:13,134] [train step40181] D loss: 0.33035 G loss: 2.10555 (0.042 sec/batch, 1535.725 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:13,521] [train step40191] D loss: 0.32881 G loss: 2.20692 (0.047 sec/batch, 1357.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:13,910] [train step40200] D loss: 0.32807 G loss: 2.47288 (0.038 sec/batch, 1691.923 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:14,297] [train step40211] D loss: 0.32723 G loss: 2.38489 (0.033 sec/batch, 1959.640 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:14,685] [train step40221] D loss: 0.32762 G loss: 2.43464 (0.041 sec/batch, 1579.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:15,088] [train step40230] D loss: 0.32672 G loss: 2.24891 (0.040 sec/batch, 1584.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:15,470] [train step40241] D loss: 0.32720 G loss: 2.29290 (0.038 sec/batch, 1686.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:15,852] [train step40250] D loss: 0.32730 G loss: 2.27207 (0.031 sec/batch, 2085.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:16,257] [train step40260] D loss: 0.32732 G loss: 2.33810 (0.037 sec/batch, 1741.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:16,660] [train step40270] D loss: 0.32697 G loss: 2.31079 (0.039 sec/batch, 1642.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:17,062] [train step40281] D loss: 0.33010 G loss: 2.08767 (0.044 sec/batch, 1446.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:17,440] [train step40290] D loss: 0.33244 G loss: 2.64518 (0.036 sec/batch, 1782.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:17,836] [train step40301] D loss: 0.32787 G loss: 2.42927 (0.038 sec/batch, 1692.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:18,228] [train step40310] D loss: 0.32680 G loss: 2.36177 (0.038 sec/batch, 1680.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:18,620] [train step40320] D loss: 0.32859 G loss: 2.16131 (0.034 sec/batch, 1872.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:19,009] [train step40331] D loss: 0.32696 G loss: 2.25036 (0.042 sec/batch, 1536.199 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:19,400] [train step40341] D loss: 0.32727 G loss: 2.37985 (0.039 sec/batch, 1640.543 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:19,786] [train step40350] D loss: 0.32731 G loss: 2.24941 (0.029 sec/batch, 2235.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:20,197] [train step40361] D loss: 0.32674 G loss: 2.26167 (0.034 sec/batch, 1905.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:20,584] [train step40371] D loss: 0.32782 G loss: 2.44712 (0.037 sec/batch, 1744.005 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:20,966] [train step40380] D loss: 0.32680 G loss: 2.28328 (0.042 sec/batch, 1507.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:21,359] [train step40390] D loss: 0.32648 G loss: 2.32990 (0.041 sec/batch, 1565.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:21,747] [train step40400] D loss: 0.32789 G loss: 2.20060 (0.039 sec/batch, 1639.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:22,139] [train step40410] D loss: 0.32734 G loss: 2.44160 (0.046 sec/batch, 1384.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:22,532] [train step40420] D loss: 0.32868 G loss: 2.50885 (0.039 sec/batch, 1656.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:22,917] [train step40430] D loss: 0.32795 G loss: 2.48387 (0.042 sec/batch, 1540.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:23,307] [train step40440] D loss: 0.32708 G loss: 2.23401 (0.035 sec/batch, 1821.284 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:23,703] [train step40450] D loss: 0.32713 G loss: 2.24493 (0.041 sec/batch, 1560.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:24,083] [train step40461] D loss: 0.32691 G loss: 2.26486 (0.038 sec/batch, 1698.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:24,483] [train step40470] D loss: 0.32732 G loss: 2.27914 (0.038 sec/batch, 1690.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:24,875] [train step40481] D loss: 0.32679 G loss: 2.26880 (0.037 sec/batch, 1711.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:25,275] [train step40490] D loss: 0.32730 G loss: 2.42367 (0.039 sec/batch, 1625.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:25,659] [train step40500] D loss: 0.32722 G loss: 2.34956 (0.037 sec/batch, 1721.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:26,038] [train step40511] D loss: 0.33077 G loss: 2.59696 (0.038 sec/batch, 1686.608 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:26,443] [train step40520] D loss: 0.32800 G loss: 2.46042 (0.037 sec/batch, 1751.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:26,826] [train step40530] D loss: 0.32735 G loss: 2.27198 (0.040 sec/batch, 1600.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:27,216] [train step40540] D loss: 0.32709 G loss: 2.36050 (0.032 sec/batch, 1981.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:27,618] [train step40551] D loss: 0.32911 G loss: 2.50600 (0.036 sec/batch, 1757.639 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:28,001] [train step40560] D loss: 0.32697 G loss: 2.22147 (0.035 sec/batch, 1828.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:28,396] [train step40571] D loss: 0.32701 G loss: 2.33195 (0.042 sec/batch, 1532.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:28,778] [train step40580] D loss: 0.32678 G loss: 2.32145 (0.037 sec/batch, 1725.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:29,170] [train step40590] D loss: 0.32681 G loss: 2.30935 (0.045 sec/batch, 1435.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:29,560] [train step40600] D loss: 0.32721 G loss: 2.42506 (0.036 sec/batch, 1783.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:29,937] [train step40611] D loss: 0.32729 G loss: 2.28565 (0.036 sec/batch, 1800.263 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:30,310] [train step40620] D loss: 0.32727 G loss: 2.29082 (0.040 sec/batch, 1582.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:30,702] [train step40631] D loss: 0.32758 G loss: 2.20229 (0.037 sec/batch, 1747.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:31,077] [train step40641] D loss: 0.32680 G loss: 2.22106 (0.040 sec/batch, 1602.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:31,464] [train step40650] D loss: 0.32694 G loss: 2.31953 (0.040 sec/batch, 1586.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:31,839] [train step40661] D loss: 0.32833 G loss: 2.16227 (0.036 sec/batch, 1783.553 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:32,228] [train step40671] D loss: 0.32867 G loss: 2.12640 (0.035 sec/batch, 1828.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:32,619] [train step40680] D loss: 0.32676 G loss: 2.39398 (0.036 sec/batch, 1753.678 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:32,992] [train step40690] D loss: 0.32731 G loss: 2.21362 (0.034 sec/batch, 1859.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:33,381] [train step40701] D loss: 0.32910 G loss: 2.08085 (0.042 sec/batch, 1510.228 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:33,760] [train step40710] D loss: 0.32871 G loss: 2.52077 (0.036 sec/batch, 1801.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:34,139] [train step40720] D loss: 0.32758 G loss: 2.40225 (0.036 sec/batch, 1774.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:34,529] [train step40731] D loss: 0.32694 G loss: 2.36222 (0.038 sec/batch, 1699.948 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:34,906] [train step40740] D loss: 0.32724 G loss: 2.30621 (0.038 sec/batch, 1692.371 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:35,284] [train step40751] D loss: 0.32750 G loss: 2.30229 (0.038 sec/batch, 1695.460 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:35,670] [train step40761] D loss: 0.32733 G loss: 2.26314 (0.038 sec/batch, 1705.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:36,044] [train step40770] D loss: 0.32767 G loss: 2.43103 (0.038 sec/batch, 1688.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:36,425] [train step40780] D loss: 0.32897 G loss: 2.51975 (0.037 sec/batch, 1716.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:36,815] [train step40791] D loss: 0.32939 G loss: 2.54875 (0.037 sec/batch, 1737.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:37,192] [train step40800] D loss: 0.32890 G loss: 2.12652 (0.036 sec/batch, 1789.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:37,606] [train step40811] D loss: 0.32669 G loss: 2.25933 (0.053 sec/batch, 1213.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:37,982] [train step40820] D loss: 0.32770 G loss: 2.40948 (0.030 sec/batch, 2138.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:38,375] [train step40830] D loss: 0.32732 G loss: 2.19166 (0.042 sec/batch, 1517.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:38,762] [train step40840] D loss: 0.32745 G loss: 2.23041 (0.039 sec/batch, 1623.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:39,142] [train step40850] D loss: 0.32666 G loss: 2.26799 (0.039 sec/batch, 1647.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:39,536] [train step40860] D loss: 0.32636 G loss: 2.37679 (0.036 sec/batch, 1787.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:39,925] [train step40871] D loss: 0.32666 G loss: 2.39774 (0.037 sec/batch, 1717.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:40,303] [train step40880] D loss: 0.32664 G loss: 2.31957 (0.037 sec/batch, 1708.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:40,696] [train step40890] D loss: 0.32675 G loss: 2.38875 (0.035 sec/batch, 1849.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:41,078] [train step40901] D loss: 0.32742 G loss: 2.44508 (0.036 sec/batch, 1775.472 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:41,465] [train step40910] D loss: 0.32783 G loss: 2.48768 (0.038 sec/batch, 1668.285 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:41,869] [train step40920] D loss: 0.32744 G loss: 2.18509 (0.039 sec/batch, 1651.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:42,249] [train step40931] D loss: 0.32669 G loss: 2.26311 (0.036 sec/batch, 1761.515 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:42,630] [train step40940] D loss: 0.32832 G loss: 2.13907 (0.038 sec/batch, 1692.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:43,029] [train step40950] D loss: 0.32685 G loss: 2.42023 (0.041 sec/batch, 1560.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:43,409] [train step40961] D loss: 0.32673 G loss: 2.40087 (0.038 sec/batch, 1704.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:43,802] [train step40971] D loss: 0.32643 G loss: 2.34176 (0.036 sec/batch, 1764.143 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:44,194] [train step40980] D loss: 0.32715 G loss: 2.38693 (0.040 sec/batch, 1604.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:44,575] [train step40990] D loss: 0.32668 G loss: 2.40393 (0.033 sec/batch, 1932.344 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:44,974] [train step41000] D loss: 0.32654 G loss: 2.30382 (0.039 sec/batch, 1645.995 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:35:44,975] Saved checkpoint at 41000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:45,566] [train step41010] D loss: 0.32675 G loss: 2.28932 (0.031 sec/batch, 2033.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:45,973] [train step41021] D loss: 0.32678 G loss: 2.22688 (0.041 sec/batch, 1550.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:46,355] [train step41030] D loss: 0.32654 G loss: 2.20756 (0.040 sec/batch, 1599.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:46,744] [train step41040] D loss: 0.32710 G loss: 2.36185 (0.039 sec/batch, 1634.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:47,137] [train step41051] D loss: 0.32718 G loss: 2.18904 (0.041 sec/batch, 1548.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:47,520] [train step41060] D loss: 0.32722 G loss: 2.23340 (0.037 sec/batch, 1724.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:47,923] [train step41070] D loss: 0.32683 G loss: 2.37446 (0.038 sec/batch, 1705.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:48,324] [train step41080] D loss: 0.32658 G loss: 2.37567 (0.039 sec/batch, 1624.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:48,708] [train step41090] D loss: 0.32905 G loss: 2.54534 (0.047 sec/batch, 1373.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:49,108] [train step41100] D loss: 0.32853 G loss: 2.14440 (0.037 sec/batch, 1743.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:49,498] [train step41110] D loss: 0.32894 G loss: 2.11485 (0.037 sec/batch, 1732.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:49,890] [train step41121] D loss: 0.32898 G loss: 2.10911 (0.042 sec/batch, 1511.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:50,274] [train step41130] D loss: 0.32628 G loss: 2.38450 (0.037 sec/batch, 1748.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:50,662] [train step41140] D loss: 0.32784 G loss: 2.14371 (0.036 sec/batch, 1785.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:51,063] [train step41150] D loss: 0.32925 G loss: 2.09397 (0.040 sec/batch, 1594.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:51,448] [train step41160] D loss: 0.32803 G loss: 2.48999 (0.038 sec/batch, 1690.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:51,837] [train step41171] D loss: 0.32717 G loss: 2.43627 (0.039 sec/batch, 1642.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:52,225] [train step41180] D loss: 0.32616 G loss: 2.30305 (0.036 sec/batch, 1782.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:52,614] [train step41190] D loss: 0.32712 G loss: 2.39145 (0.036 sec/batch, 1765.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:53,014] [train step41200] D loss: 0.32784 G loss: 2.47710 (0.041 sec/batch, 1576.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:53,405] [train step41210] D loss: 0.32681 G loss: 2.30173 (0.038 sec/batch, 1671.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:53,792] [train step41220] D loss: 0.32706 G loss: 2.42573 (0.041 sec/batch, 1556.003 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:54,191] [train step41231] D loss: 0.32833 G loss: 2.50740 (0.038 sec/batch, 1696.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:54,576] [train step41241] D loss: 0.33085 G loss: 2.60717 (0.035 sec/batch, 1847.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:54,966] [train step41250] D loss: 0.32948 G loss: 2.08741 (0.039 sec/batch, 1650.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:55,357] [train step41261] D loss: 0.32700 G loss: 2.19563 (0.037 sec/batch, 1713.688 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:55,749] [train step41271] D loss: 0.32684 G loss: 2.37985 (0.040 sec/batch, 1582.038 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:56,144] [train step41280] D loss: 0.32626 G loss: 2.26235 (0.037 sec/batch, 1709.508 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:56,530] [train step41291] D loss: 0.32685 G loss: 2.39734 (0.037 sec/batch, 1741.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:56,919] [train step41300] D loss: 0.32799 G loss: 2.47505 (0.039 sec/batch, 1646.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:57,308] [train step41310] D loss: 0.32753 G loss: 2.16937 (0.040 sec/batch, 1588.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:57,691] [train step41320] D loss: 0.32642 G loss: 2.27112 (0.040 sec/batch, 1611.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:58,070] [train step41331] D loss: 0.32656 G loss: 2.26839 (0.037 sec/batch, 1750.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:58,462] [train step41340] D loss: 0.32716 G loss: 2.43105 (0.038 sec/batch, 1692.329 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:58,846] [train step41351] D loss: 0.32712 G loss: 2.43491 (0.035 sec/batch, 1844.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:59,253] [train step41361] D loss: 0.32687 G loss: 2.30148 (0.038 sec/batch, 1695.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:35:59,634] [train step41370] D loss: 0.32632 G loss: 2.29140 (0.030 sec/batch, 2142.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:00,026] [train step41381] D loss: 0.32723 G loss: 2.20171 (0.038 sec/batch, 1674.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:00,409] [train step41390] D loss: 0.32627 G loss: 2.31300 (0.034 sec/batch, 1898.036 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:00,789] [train step41400] D loss: 0.32651 G loss: 2.24001 (0.040 sec/batch, 1588.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:01,160] [train step41411] D loss: 0.32630 G loss: 2.24815 (0.033 sec/batch, 1924.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:01,547] [train step41420] D loss: 0.32648 G loss: 2.33190 (0.039 sec/batch, 1650.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:01,927] [train step41430] D loss: 0.32630 G loss: 2.29860 (0.036 sec/batch, 1792.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:02,316] [train step41440] D loss: 0.32682 G loss: 2.20883 (0.038 sec/batch, 1689.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:02,691] [train step41450] D loss: 0.32938 G loss: 2.08207 (0.043 sec/batch, 1483.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:03,067] [train step41460] D loss: 0.32923 G loss: 2.55912 (0.038 sec/batch, 1681.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:03,452] [train step41470] D loss: 0.32630 G loss: 2.32758 (0.037 sec/batch, 1724.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:03,825] [train step41481] D loss: 0.32680 G loss: 2.39572 (0.036 sec/batch, 1778.590 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:04,206] [train step41490] D loss: 0.32631 G loss: 2.27682 (0.040 sec/batch, 1604.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:04,588] [train step41500] D loss: 0.32611 G loss: 2.33890 (0.041 sec/batch, 1566.656 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:04,965] [train step41510] D loss: 0.32631 G loss: 2.37137 (0.030 sec/batch, 2107.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:05,359] [train step41520] D loss: 0.32655 G loss: 2.20908 (0.035 sec/batch, 1829.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:05,744] [train step41530] D loss: 0.32655 G loss: 2.21594 (0.043 sec/batch, 1498.810 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:06,129] [train step41540] D loss: 0.32621 G loss: 2.29832 (0.040 sec/batch, 1619.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:06,516] [train step41550] D loss: 0.32676 G loss: 2.36636 (0.038 sec/batch, 1675.313 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:06,901] [train step41561] D loss: 0.32651 G loss: 2.36891 (0.037 sec/batch, 1716.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:07,287] [train step41571] D loss: 0.32653 G loss: 2.35663 (0.036 sec/batch, 1768.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:07,690] [train step41580] D loss: 0.32623 G loss: 2.31716 (0.036 sec/batch, 1754.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:08,064] [train step41590] D loss: 0.32734 G loss: 2.46096 (0.037 sec/batch, 1725.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:08,454] [train step41600] D loss: 0.32631 G loss: 2.37076 (0.039 sec/batch, 1644.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:08,844] [train step41610] D loss: 0.32625 G loss: 2.36866 (0.041 sec/batch, 1556.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:09,235] [train step41621] D loss: 0.32690 G loss: 2.43886 (0.040 sec/batch, 1611.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:09,617] [train step41631] D loss: 0.32629 G loss: 2.33685 (0.037 sec/batch, 1716.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:09,991] [train step41640] D loss: 0.32643 G loss: 2.39199 (0.034 sec/batch, 1859.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:10,379] [train step41651] D loss: 0.32594 G loss: 2.34244 (0.040 sec/batch, 1591.700 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:10,768] [train step41660] D loss: 0.32656 G loss: 2.24922 (0.038 sec/batch, 1706.096 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:11,155] [train step41670] D loss: 0.32735 G loss: 2.45218 (0.036 sec/batch, 1783.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:11,544] [train step41681] D loss: 0.32786 G loss: 2.47276 (0.037 sec/batch, 1728.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:11,936] [train step41690] D loss: 0.32625 G loss: 2.39056 (0.040 sec/batch, 1584.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:12,325] [train step41700] D loss: 0.32619 G loss: 2.27234 (0.040 sec/batch, 1587.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:12,709] [train step41710] D loss: 0.32747 G loss: 2.48175 (0.038 sec/batch, 1688.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:13,090] [train step41720] D loss: 0.33076 G loss: 2.60683 (0.036 sec/batch, 1794.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:13,474] [train step41730] D loss: 0.32889 G loss: 2.10012 (0.038 sec/batch, 1703.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:13,865] [train step41741] D loss: 0.32672 G loss: 2.22485 (0.038 sec/batch, 1676.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:14,247] [train step41750] D loss: 0.32621 G loss: 2.23068 (0.039 sec/batch, 1645.208 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:14,643] [train step41760] D loss: 0.32666 G loss: 2.39782 (0.035 sec/batch, 1850.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:15,041] [train step41770] D loss: 0.32639 G loss: 2.40037 (0.050 sec/batch, 1276.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:15,435] [train step41781] D loss: 0.32640 G loss: 2.28367 (0.040 sec/batch, 1611.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:15,832] [train step41790] D loss: 0.32629 G loss: 2.39169 (0.035 sec/batch, 1853.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:16,217] [train step41800] D loss: 0.32616 G loss: 2.34540 (0.039 sec/batch, 1640.433 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:16,606] [train step41811] D loss: 0.32636 G loss: 2.24614 (0.040 sec/batch, 1619.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:17,004] [train step41820] D loss: 0.32675 G loss: 2.38963 (0.040 sec/batch, 1604.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:17,394] [train step41830] D loss: 0.32582 G loss: 2.27210 (0.041 sec/batch, 1547.419 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:17,782] [train step41841] D loss: 0.32624 G loss: 2.30919 (0.032 sec/batch, 1969.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:18,181] [train step41850] D loss: 0.32633 G loss: 2.35233 (0.038 sec/batch, 1696.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:18,562] [train step41861] D loss: 0.32635 G loss: 2.24361 (0.035 sec/batch, 1831.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:18,956] [train step41870] D loss: 0.32611 G loss: 2.28438 (0.042 sec/batch, 1541.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:19,341] [train step41880] D loss: 0.32616 G loss: 2.28744 (0.039 sec/batch, 1646.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:19,754] [train step41891] D loss: 0.32835 G loss: 2.10740 (0.044 sec/batch, 1450.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:20,144] [train step41901] D loss: 0.32619 G loss: 2.26210 (0.038 sec/batch, 1673.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:20,529] [train step41910] D loss: 0.32619 G loss: 2.30110 (0.032 sec/batch, 1994.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:20,927] [train step41921] D loss: 0.32701 G loss: 2.17860 (0.037 sec/batch, 1735.391 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:21,317] [train step41931] D loss: 0.32602 G loss: 2.23675 (0.038 sec/batch, 1704.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:21,697] [train step41940] D loss: 0.32610 G loss: 2.35636 (0.038 sec/batch, 1666.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:22,094] [train step41950] D loss: 0.32616 G loss: 2.35087 (0.039 sec/batch, 1623.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:22,475] [train step41960] D loss: 0.32643 G loss: 2.22404 (0.040 sec/batch, 1608.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:22,862] [train step41970] D loss: 0.32621 G loss: 2.38499 (0.036 sec/batch, 1802.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:23,261] [train step41981] D loss: 0.32633 G loss: 2.37093 (0.039 sec/batch, 1633.733 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:23,654] [train step41990] D loss: 0.32624 G loss: 2.37315 (0.036 sec/batch, 1773.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:24,059] [train step42000] D loss: 0.32634 G loss: 2.24459 (0.041 sec/batch, 1574.660 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:36:24,059] Saved checkpoint at 42000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:24,647] [train step42011] D loss: 0.32584 G loss: 2.28048 (0.038 sec/batch, 1704.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:25,028] [train step42020] D loss: 0.32586 G loss: 2.26611 (0.032 sec/batch, 1990.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:25,421] [train step42030] D loss: 0.32604 G loss: 2.31065 (0.037 sec/batch, 1747.058 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:25,802] [train step42041] D loss: 0.32596 G loss: 2.28575 (0.038 sec/batch, 1684.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:26,186] [train step42051] D loss: 0.32631 G loss: 2.22266 (0.038 sec/batch, 1681.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:26,565] [train step42060] D loss: 0.32679 G loss: 2.44306 (0.033 sec/batch, 1945.100 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:26,966] [train step42071] D loss: 0.32668 G loss: 2.43321 (0.045 sec/batch, 1408.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:27,356] [train step42080] D loss: 0.32601 G loss: 2.28944 (0.037 sec/batch, 1744.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:27,735] [train step42090] D loss: 0.32582 G loss: 2.34368 (0.040 sec/batch, 1580.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:28,127] [train step42100] D loss: 0.32687 G loss: 2.42794 (0.031 sec/batch, 2091.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:28,525] [train step42111] D loss: 0.32681 G loss: 2.44294 (0.034 sec/batch, 1904.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:28,907] [train step42120] D loss: 0.32673 G loss: 2.18543 (0.040 sec/batch, 1587.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:29,299] [train step42130] D loss: 0.32630 G loss: 2.24882 (0.036 sec/batch, 1766.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:29,693] [train step42140] D loss: 0.32640 G loss: 2.38186 (0.036 sec/batch, 1764.108 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:30,082] [train step42150] D loss: 0.32698 G loss: 2.18498 (0.040 sec/batch, 1599.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:30,490] [train step42161] D loss: 0.32632 G loss: 2.23775 (0.034 sec/batch, 1902.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:30,858] [train step42171] D loss: 0.32635 G loss: 2.23724 (0.035 sec/batch, 1816.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:31,253] [train step42180] D loss: 0.32735 G loss: 2.47754 (0.039 sec/batch, 1634.997 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:31,618] [train step42190] D loss: 0.32810 G loss: 2.50566 (0.030 sec/batch, 2119.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:31,995] [train step42201] D loss: 0.32598 G loss: 2.35290 (0.038 sec/batch, 1673.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:32,380] [train step42210] D loss: 0.32595 G loss: 2.32438 (0.040 sec/batch, 1609.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:32,756] [train step42220] D loss: 0.32622 G loss: 2.35984 (0.035 sec/batch, 1807.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:33,148] [train step42231] D loss: 0.32604 G loss: 2.26267 (0.039 sec/batch, 1630.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:33,523] [train step42240] D loss: 0.32615 G loss: 2.36409 (0.037 sec/batch, 1736.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:33,902] [train step42250] D loss: 0.32648 G loss: 2.40533 (0.039 sec/batch, 1658.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:34,289] [train step42260] D loss: 0.32638 G loss: 2.36165 (0.037 sec/batch, 1713.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:34,672] [train step42270] D loss: 0.32627 G loss: 2.25208 (0.038 sec/batch, 1663.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:35,050] [train step42281] D loss: 0.32642 G loss: 2.21603 (0.033 sec/batch, 1930.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:35,436] [train step42291] D loss: 0.32596 G loss: 2.32053 (0.038 sec/batch, 1664.871 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:35,814] [train step42300] D loss: 0.32603 G loss: 2.35512 (0.036 sec/batch, 1776.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:36,194] [train step42311] D loss: 0.32619 G loss: 2.39045 (0.042 sec/batch, 1518.738 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:36,570] [train step42321] D loss: 0.32607 G loss: 2.35053 (0.041 sec/batch, 1555.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:36,944] [train step42330] D loss: 0.32612 G loss: 2.27444 (0.037 sec/batch, 1714.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:37,337] [train step42340] D loss: 0.32624 G loss: 2.24189 (0.035 sec/batch, 1813.153 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:37,712] [train step42351] D loss: 0.32608 G loss: 2.26783 (0.034 sec/batch, 1865.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:38,102] [train step42360] D loss: 0.32587 G loss: 2.34144 (0.040 sec/batch, 1595.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:38,493] [train step42370] D loss: 0.32601 G loss: 2.35408 (0.036 sec/batch, 1755.513 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:38,879] [train step42380] D loss: 0.32598 G loss: 2.31433 (0.041 sec/batch, 1566.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:39,253] [train step42390] D loss: 0.32606 G loss: 2.26847 (0.036 sec/batch, 1757.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:39,644] [train step42400] D loss: 0.32610 G loss: 2.27394 (0.039 sec/batch, 1633.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:40,025] [train step42411] D loss: 0.32586 G loss: 2.33057 (0.039 sec/batch, 1647.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:40,411] [train step42420] D loss: 0.32595 G loss: 2.30745 (0.035 sec/batch, 1813.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:40,815] [train step42431] D loss: 0.32587 G loss: 2.33775 (0.036 sec/batch, 1779.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:41,197] [train step42440] D loss: 0.32636 G loss: 2.36510 (0.040 sec/batch, 1608.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:41,583] [train step42450] D loss: 0.32587 G loss: 2.27389 (0.036 sec/batch, 1799.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:41,961] [train step42460] D loss: 0.32611 G loss: 2.31532 (0.043 sec/batch, 1484.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:42,364] [train step42471] D loss: 0.32592 G loss: 2.30115 (0.050 sec/batch, 1274.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:42,746] [train step42480] D loss: 0.32661 G loss: 2.21452 (0.037 sec/batch, 1720.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:43,136] [train step42490] D loss: 0.32741 G loss: 2.14342 (0.038 sec/batch, 1703.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:43,529] [train step42500] D loss: 0.32813 G loss: 2.11470 (0.036 sec/batch, 1765.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:43,916] [train step42510] D loss: 0.32656 G loss: 2.41764 (0.036 sec/batch, 1776.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:44,308] [train step42520] D loss: 0.32621 G loss: 2.23375 (0.040 sec/batch, 1597.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:44,705] [train step42530] D loss: 0.32668 G loss: 2.18023 (0.039 sec/batch, 1633.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:45,083] [train step42540] D loss: 0.32674 G loss: 2.44323 (0.036 sec/batch, 1759.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:45,477] [train step42551] D loss: 0.32577 G loss: 2.28000 (0.053 sec/batch, 1196.578 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:45,878] [train step42560] D loss: 0.32606 G loss: 2.35371 (0.041 sec/batch, 1549.661 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:46,262] [train step42570] D loss: 0.32587 G loss: 2.25990 (0.040 sec/batch, 1591.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:46,647] [train step42580] D loss: 0.32675 G loss: 2.17991 (0.036 sec/batch, 1779.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:47,035] [train step42591] D loss: 0.32658 G loss: 2.20316 (0.037 sec/batch, 1737.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:47,415] [train step42600] D loss: 0.32596 G loss: 2.37140 (0.037 sec/batch, 1727.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:47,801] [train step42610] D loss: 0.32631 G loss: 2.40419 (0.029 sec/batch, 2187.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:48,196] [train step42621] D loss: 0.32882 G loss: 2.55018 (0.037 sec/batch, 1710.368 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:48,588] [train step42630] D loss: 0.32883 G loss: 2.08852 (0.039 sec/batch, 1636.512 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:48,970] [train step42641] D loss: 0.32914 G loss: 2.07830 (0.040 sec/batch, 1604.487 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:49,353] [train step42650] D loss: 0.32750 G loss: 2.13925 (0.045 sec/batch, 1416.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:49,743] [train step42660] D loss: 0.32611 G loss: 2.40203 (0.040 sec/batch, 1605.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:50,122] [train step42671] D loss: 0.32577 G loss: 2.30717 (0.039 sec/batch, 1634.201 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:50,506] [train step42681] D loss: 0.32578 G loss: 2.28550 (0.040 sec/batch, 1594.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:50,916] [train step42690] D loss: 0.32608 G loss: 2.36145 (0.038 sec/batch, 1703.887 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:51,320] [train step42701] D loss: 0.32596 G loss: 2.27319 (0.042 sec/batch, 1524.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:51,706] [train step42710] D loss: 0.32614 G loss: 2.23811 (0.037 sec/batch, 1726.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:52,080] [train step42720] D loss: 0.32573 G loss: 2.34410 (0.037 sec/batch, 1734.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:52,480] [train step42731] D loss: 0.32580 G loss: 2.33009 (0.036 sec/batch, 1765.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:52,872] [train step42740] D loss: 0.32594 G loss: 2.27119 (0.038 sec/batch, 1682.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:53,256] [train step42750] D loss: 0.32568 G loss: 2.30315 (0.040 sec/batch, 1594.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:53,634] [train step42761] D loss: 0.32592 G loss: 2.30959 (0.038 sec/batch, 1701.284 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:54,029] [train step42771] D loss: 0.32688 G loss: 2.18235 (0.048 sec/batch, 1320.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:54,408] [train step42780] D loss: 0.32610 G loss: 2.38040 (0.036 sec/batch, 1758.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:54,798] [train step42791] D loss: 0.32580 G loss: 2.25590 (0.048 sec/batch, 1342.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:55,179] [train step42801] D loss: 0.32689 G loss: 2.17364 (0.037 sec/batch, 1716.427 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:55,558] [train step42810] D loss: 0.32663 G loss: 2.42866 (0.038 sec/batch, 1705.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:55,946] [train step42820] D loss: 0.32582 G loss: 2.29183 (0.041 sec/batch, 1559.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:56,327] [train step42831] D loss: 0.32818 G loss: 2.10595 (0.037 sec/batch, 1744.356 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:56,712] [train step42840] D loss: 0.33004 G loss: 2.60142 (0.033 sec/batch, 1933.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:57,108] [train step42850] D loss: 0.33157 G loss: 2.65398 (0.034 sec/batch, 1899.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:57,520] [train step42860] D loss: 0.33135 G loss: 2.64364 (0.043 sec/batch, 1494.838 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:57,946] [train step42870] D loss: 0.32869 G loss: 2.08360 (0.041 sec/batch, 1562.161 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:58,336] [train step42880] D loss: 0.32842 G loss: 2.10409 (0.043 sec/batch, 1503.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:58,729] [train step42891] D loss: 0.32838 G loss: 2.10878 (0.040 sec/batch, 1601.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:59,124] [train step42900] D loss: 0.32807 G loss: 2.51441 (0.039 sec/batch, 1631.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:59,525] [train step42911] D loss: 0.32591 G loss: 2.32545 (0.039 sec/batch, 1645.117 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:36:59,927] [train step42920] D loss: 0.32653 G loss: 2.20501 (0.030 sec/batch, 2156.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:00,330] [train step42930] D loss: 0.32683 G loss: 2.45490 (0.036 sec/batch, 1762.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:00,722] [train step42941] D loss: 0.32661 G loss: 2.42712 (0.035 sec/batch, 1825.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:01,120] [train step42951] D loss: 0.32582 G loss: 2.30662 (0.035 sec/batch, 1815.876 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:01,501] [train step42960] D loss: 0.32593 G loss: 2.34069 (0.037 sec/batch, 1748.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:01,880] [train step42971] D loss: 0.32581 G loss: 2.34179 (0.035 sec/batch, 1850.054 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:02,289] [train step42981] D loss: 0.32589 G loss: 2.25709 (0.036 sec/batch, 1756.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:02,672] [train step42990] D loss: 0.32604 G loss: 2.34345 (0.040 sec/batch, 1604.688 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:03,072] [train step43001] D loss: 0.32660 G loss: 2.20874 (0.036 sec/batch, 1798.382 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:37:03,072] Saved checkpoint at 43000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:03,666] [train step43010] D loss: 0.32579 G loss: 2.27426 (0.038 sec/batch, 1669.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:04,067] [train step43020] D loss: 0.32572 G loss: 2.30753 (0.039 sec/batch, 1638.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:04,454] [train step43031] D loss: 0.32580 G loss: 2.32567 (0.039 sec/batch, 1654.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:04,834] [train step43040] D loss: 0.32595 G loss: 2.34650 (0.037 sec/batch, 1741.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:05,232] [train step43050] D loss: 0.32619 G loss: 2.21667 (0.035 sec/batch, 1839.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:05,615] [train step43061] D loss: 0.32622 G loss: 2.20815 (0.038 sec/batch, 1678.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:06,011] [train step43071] D loss: 0.32589 G loss: 2.27445 (0.041 sec/batch, 1557.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:06,420] [train step43080] D loss: 0.32594 G loss: 2.24138 (0.036 sec/batch, 1756.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:06,806] [train step43090] D loss: 0.32599 G loss: 2.26762 (0.040 sec/batch, 1611.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:07,203] [train step43101] D loss: 0.32639 G loss: 2.21632 (0.043 sec/batch, 1490.265 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:07,595] [train step43110] D loss: 0.32720 G loss: 2.47689 (0.038 sec/batch, 1687.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:07,983] [train step43121] D loss: 0.32739 G loss: 2.48683 (0.037 sec/batch, 1747.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:08,369] [train step43131] D loss: 0.32783 G loss: 2.50779 (0.038 sec/batch, 1702.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:08,742] [train step43140] D loss: 0.32596 G loss: 2.24967 (0.036 sec/batch, 1775.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:09,121] [train step43150] D loss: 0.32585 G loss: 2.28243 (0.037 sec/batch, 1711.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:09,516] [train step43161] D loss: 0.32593 G loss: 2.27364 (0.039 sec/batch, 1629.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:09,893] [train step43170] D loss: 0.32585 G loss: 2.27450 (0.037 sec/batch, 1738.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:10,287] [train step43181] D loss: 0.32589 G loss: 2.29217 (0.040 sec/batch, 1620.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:10,664] [train step43191] D loss: 0.32574 G loss: 2.32210 (0.035 sec/batch, 1829.489 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:11,043] [train step43200] D loss: 0.32582 G loss: 2.29410 (0.039 sec/batch, 1659.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:11,425] [train step43211] D loss: 0.32575 G loss: 2.31644 (0.040 sec/batch, 1599.106 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:11,802] [train step43221] D loss: 0.32760 G loss: 2.12187 (0.037 sec/batch, 1706.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:12,174] [train step43230] D loss: 0.32946 G loss: 2.58251 (0.036 sec/batch, 1776.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:12,570] [train step43241] D loss: 0.32705 G loss: 2.46568 (0.040 sec/batch, 1602.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:12,957] [train step43251] D loss: 0.32588 G loss: 2.33213 (0.037 sec/batch, 1737.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:13,350] [train step43260] D loss: 0.32570 G loss: 2.28074 (0.038 sec/batch, 1664.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:13,732] [train step43270] D loss: 0.32586 G loss: 2.24541 (0.041 sec/batch, 1579.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:14,120] [train step43280] D loss: 0.32635 G loss: 2.20169 (0.040 sec/batch, 1589.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:14,508] [train step43290] D loss: 0.32620 G loss: 2.38599 (0.036 sec/batch, 1786.711 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:14,895] [train step43300] D loss: 0.32667 G loss: 2.43792 (0.043 sec/batch, 1496.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:15,268] [train step43310] D loss: 0.32596 G loss: 2.36931 (0.036 sec/batch, 1793.719 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:15,658] [train step43320] D loss: 0.32588 G loss: 2.24145 (0.039 sec/batch, 1634.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:16,037] [train step43330] D loss: 0.32573 G loss: 2.33609 (0.034 sec/batch, 1906.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:16,426] [train step43340] D loss: 0.32570 G loss: 2.29163 (0.034 sec/batch, 1900.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:16,811] [train step43350] D loss: 0.32587 G loss: 2.26223 (0.036 sec/batch, 1779.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:17,194] [train step43361] D loss: 0.32607 G loss: 2.22845 (0.038 sec/batch, 1699.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:17,585] [train step43371] D loss: 0.32573 G loss: 2.33486 (0.038 sec/batch, 1693.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:17,963] [train step43380] D loss: 0.32577 G loss: 2.29230 (0.038 sec/batch, 1678.897 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:18,342] [train step43391] D loss: 0.32581 G loss: 2.28671 (0.037 sec/batch, 1713.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:18,730] [train step43400] D loss: 0.32565 G loss: 2.28769 (0.039 sec/batch, 1655.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:19,122] [train step43410] D loss: 0.32583 G loss: 2.32845 (0.035 sec/batch, 1850.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:19,504] [train step43421] D loss: 0.32569 G loss: 2.30173 (0.032 sec/batch, 2027.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:19,889] [train step43430] D loss: 0.32569 G loss: 2.26222 (0.033 sec/batch, 1918.191 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:20,279] [train step43440] D loss: 0.32629 G loss: 2.41467 (0.043 sec/batch, 1480.166 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:20,661] [train step43450] D loss: 0.32725 G loss: 2.48777 (0.038 sec/batch, 1676.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:21,039] [train step43460] D loss: 0.32639 G loss: 2.41902 (0.038 sec/batch, 1702.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:21,416] [train step43470] D loss: 0.32743 G loss: 2.14804 (0.040 sec/batch, 1587.173 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:21,808] [train step43481] D loss: 0.32837 G loss: 2.09266 (0.036 sec/batch, 1765.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:22,192] [train step43491] D loss: 0.32816 G loss: 2.11202 (0.036 sec/batch, 1764.038 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:22,587] [train step43500] D loss: 0.32817 G loss: 2.51862 (0.037 sec/batch, 1732.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:22,970] [train step43510] D loss: 0.32616 G loss: 2.39143 (0.042 sec/batch, 1532.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:23,360] [train step43520] D loss: 0.32580 G loss: 2.30378 (0.043 sec/batch, 1491.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:23,749] [train step43530] D loss: 0.32603 G loss: 2.35769 (0.037 sec/batch, 1720.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:24,139] [train step43541] D loss: 0.32610 G loss: 2.38251 (0.038 sec/batch, 1668.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:24,520] [train step43550] D loss: 0.32618 G loss: 2.39850 (0.036 sec/batch, 1785.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:24,907] [train step43560] D loss: 0.32564 G loss: 2.28034 (0.041 sec/batch, 1547.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:25,281] [train step43571] D loss: 0.32596 G loss: 2.36888 (0.035 sec/batch, 1851.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:25,666] [train step43580] D loss: 0.32571 G loss: 2.30615 (0.035 sec/batch, 1845.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:26,042] [train step43590] D loss: 0.32598 G loss: 2.27816 (0.040 sec/batch, 1611.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:26,422] [train step43600] D loss: 0.32633 G loss: 2.20895 (0.036 sec/batch, 1767.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:26,809] [train step43611] D loss: 0.32605 G loss: 2.23090 (0.040 sec/batch, 1612.690 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:27,199] [train step43620] D loss: 0.32577 G loss: 2.33355 (0.040 sec/batch, 1591.861 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:27,577] [train step43630] D loss: 0.32577 G loss: 2.30013 (0.037 sec/batch, 1714.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:27,960] [train step43640] D loss: 0.32601 G loss: 2.23083 (0.039 sec/batch, 1662.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:28,341] [train step43650] D loss: 0.32586 G loss: 2.35907 (0.035 sec/batch, 1850.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:28,728] [train step43661] D loss: 0.32569 G loss: 2.28171 (0.037 sec/batch, 1722.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:29,108] [train step43671] D loss: 0.32568 G loss: 2.34389 (0.037 sec/batch, 1745.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:29,494] [train step43680] D loss: 0.32586 G loss: 2.26935 (0.039 sec/batch, 1642.772 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:29,895] [train step43691] D loss: 0.32628 G loss: 2.20578 (0.038 sec/batch, 1702.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:30,271] [train step43700] D loss: 0.32612 G loss: 2.23002 (0.033 sec/batch, 1962.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:30,657] [train step43710] D loss: 0.32588 G loss: 2.37639 (0.043 sec/batch, 1483.282 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:31,039] [train step43720] D loss: 0.32570 G loss: 2.33051 (0.034 sec/batch, 1890.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:31,409] [train step43731] D loss: 0.32603 G loss: 2.22693 (0.035 sec/batch, 1810.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:31,796] [train step43740] D loss: 0.32594 G loss: 2.36867 (0.038 sec/batch, 1670.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:32,171] [train step43750] D loss: 0.32615 G loss: 2.37336 (0.037 sec/batch, 1743.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:32,540] [train step43761] D loss: 0.32611 G loss: 2.21589 (0.037 sec/batch, 1707.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:32,914] [train step43770] D loss: 0.32598 G loss: 2.36095 (0.034 sec/batch, 1884.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:33,289] [train step43781] D loss: 0.32611 G loss: 2.23409 (0.035 sec/batch, 1809.877 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:33,660] [train step43791] D loss: 0.32571 G loss: 2.32908 (0.034 sec/batch, 1898.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:34,057] [train step43800] D loss: 0.32654 G loss: 2.18328 (0.051 sec/batch, 1261.848 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:34,430] [train step43811] D loss: 0.32786 G loss: 2.11804 (0.035 sec/batch, 1820.395 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:34,798] [train step43821] D loss: 0.32714 G loss: 2.14765 (0.034 sec/batch, 1892.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:35,171] [train step43830] D loss: 0.32595 G loss: 2.37638 (0.035 sec/batch, 1813.030 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:35,545] [train step43840] D loss: 0.32614 G loss: 2.23348 (0.038 sec/batch, 1683.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:35,927] [train step43851] D loss: 0.32618 G loss: 2.21886 (0.033 sec/batch, 1932.511 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:36,308] [train step43860] D loss: 0.32687 G loss: 2.44628 (0.036 sec/batch, 1765.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:36,680] [train step43870] D loss: 0.32570 G loss: 2.30871 (0.031 sec/batch, 2094.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:37,075] [train step43881] D loss: 0.32556 G loss: 2.29897 (0.040 sec/batch, 1612.981 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:37,451] [train step43890] D loss: 0.32639 G loss: 2.31341 (0.036 sec/batch, 1762.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:37,824] [train step43901] D loss: 0.32672 G loss: 2.40661 (0.032 sec/batch, 1971.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:38,216] [train step43910] D loss: 0.34866 G loss: 1.76470 (0.041 sec/batch, 1561.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:38,592] [train step43920] D loss: 0.51678 G loss: 5.11161 (0.035 sec/batch, 1804.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:38,966] [train step43930] D loss: 0.49337 G loss: 1.06572 (0.033 sec/batch, 1966.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:39,353] [train step43940] D loss: 0.65443 G loss: 6.53009 (0.045 sec/batch, 1426.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:39,725] [train step43950] D loss: 0.40453 G loss: 3.84744 (0.038 sec/batch, 1671.308 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:40,116] [train step43960] D loss: 0.42786 G loss: 4.12833 (0.042 sec/batch, 1512.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:40,492] [train step43971] D loss: 0.32759 G loss: 2.17204 (0.036 sec/batch, 1784.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:40,868] [train step43980] D loss: 0.35967 G loss: 1.64635 (0.035 sec/batch, 1805.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:41,253] [train step43991] D loss: 0.33707 G loss: 2.79293 (0.041 sec/batch, 1565.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:41,621] [train step44001] D loss: 0.32775 G loss: 2.46039 (0.030 sec/batch, 2098.431 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:37:41,621] Saved checkpoint at 44000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:42,223] [train step44010] D loss: 0.32735 G loss: 2.45369 (0.037 sec/batch, 1747.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:42,605] [train step44021] D loss: 0.32725 G loss: 2.17854 (0.033 sec/batch, 1962.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:42,986] [train step44031] D loss: 0.32603 G loss: 2.33428 (0.039 sec/batch, 1647.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:43,376] [train step44040] D loss: 0.32641 G loss: 2.27810 (0.038 sec/batch, 1665.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:43,758] [train step44050] D loss: 0.32631 G loss: 2.29957 (0.037 sec/batch, 1712.398 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:44,158] [train step44061] D loss: 0.32684 G loss: 2.24792 (0.038 sec/batch, 1666.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:44,553] [train step44070] D loss: 0.32633 G loss: 2.34389 (0.041 sec/batch, 1557.122 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:44,932] [train step44081] D loss: 0.32625 G loss: 2.28965 (0.041 sec/batch, 1574.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:45,320] [train step44091] D loss: 0.32649 G loss: 2.28448 (0.036 sec/batch, 1760.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:45,697] [train step44100] D loss: 0.32603 G loss: 2.34546 (0.034 sec/batch, 1871.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:46,069] [train step44111] D loss: 0.32613 G loss: 2.28552 (0.030 sec/batch, 2101.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:46,461] [train step44120] D loss: 0.32587 G loss: 2.32156 (0.038 sec/batch, 1692.862 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:46,839] [train step44130] D loss: 0.32686 G loss: 2.21594 (0.037 sec/batch, 1722.463 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:47,224] [train step44140] D loss: 0.32603 G loss: 2.26001 (0.039 sec/batch, 1632.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:47,606] [train step44151] D loss: 0.32687 G loss: 2.44283 (0.036 sec/batch, 1770.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:47,994] [train step44160] D loss: 0.32665 G loss: 2.23728 (0.040 sec/batch, 1592.862 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:48,381] [train step44170] D loss: 0.32630 G loss: 2.36117 (0.036 sec/batch, 1768.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:48,764] [train step44180] D loss: 0.32645 G loss: 2.37243 (0.039 sec/batch, 1647.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:49,142] [train step44190] D loss: 0.32601 G loss: 2.32269 (0.042 sec/batch, 1522.157 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:49,525] [train step44201] D loss: 0.32708 G loss: 2.40655 (0.032 sec/batch, 1981.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:49,918] [train step44211] D loss: 0.32738 G loss: 2.42984 (0.040 sec/batch, 1617.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:50,316] [train step44220] D loss: 0.32782 G loss: 2.13889 (0.045 sec/batch, 1434.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:50,695] [train step44230] D loss: 0.32633 G loss: 2.23322 (0.039 sec/batch, 1656.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:51,079] [train step44241] D loss: 0.32863 G loss: 2.24039 (0.035 sec/batch, 1823.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:51,471] [train step44250] D loss: 0.33137 G loss: 2.62571 (0.037 sec/batch, 1727.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:51,859] [train step44261] D loss: 0.32600 G loss: 2.34711 (0.039 sec/batch, 1651.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:52,236] [train step44271] D loss: 0.32690 G loss: 2.20360 (0.034 sec/batch, 1891.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:52,626] [train step44280] D loss: 0.33450 G loss: 2.11977 (0.041 sec/batch, 1573.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:52,999] [train step44291] D loss: 0.32751 G loss: 2.20653 (0.036 sec/batch, 1768.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:53,390] [train step44301] D loss: 0.69317 G loss: 2.69297 (0.047 sec/batch, 1363.454 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:53,767] [train step44310] D loss: 2.46164 G loss: 0.07909 (0.040 sec/batch, 1587.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:54,147] [train step44321] D loss: 0.44738 G loss: 4.00081 (0.036 sec/batch, 1755.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:54,540] [train step44331] D loss: 0.35976 G loss: 2.36315 (0.045 sec/batch, 1429.582 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:54,920] [train step44340] D loss: 0.35844 G loss: 1.78839 (0.041 sec/batch, 1574.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:55,327] [train step44351] D loss: 0.34666 G loss: 2.85866 (0.037 sec/batch, 1712.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:55,720] [train step44361] D loss: 0.33620 G loss: 1.99611 (0.039 sec/batch, 1632.293 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:56,113] [train step44370] D loss: 0.32955 G loss: 2.47040 (0.043 sec/batch, 1499.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:56,496] [train step44381] D loss: 0.32730 G loss: 2.24841 (0.037 sec/batch, 1718.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:56,875] [train step44391] D loss: 0.33175 G loss: 2.49038 (0.036 sec/batch, 1768.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:57,258] [train step44400] D loss: 0.32948 G loss: 2.24953 (0.035 sec/batch, 1809.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:57,645] [train step44411] D loss: 0.32880 G loss: 2.45884 (0.036 sec/batch, 1779.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:58,031] [train step44421] D loss: 0.32744 G loss: 2.37875 (0.038 sec/batch, 1671.860 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:58,416] [train step44430] D loss: 0.33101 G loss: 2.21314 (0.039 sec/batch, 1641.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:58,808] [train step44441] D loss: 0.33149 G loss: 2.24702 (0.038 sec/batch, 1704.590 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:59,182] [train step44450] D loss: 0.34121 G loss: 2.76784 (0.032 sec/batch, 2020.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:59,567] [train step44460] D loss: 0.33134 G loss: 2.42514 (0.043 sec/batch, 1480.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:37:59,971] [train step44471] D loss: 0.42554 G loss: 3.96040 (0.036 sec/batch, 1765.547 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:00,362] [train step44481] D loss: 0.36676 G loss: 2.71575 (0.039 sec/batch, 1639.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:00,757] [train step44490] D loss: 0.33089 G loss: 2.15378 (0.044 sec/batch, 1465.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:01,144] [train step44501] D loss: 0.33896 G loss: 2.54041 (0.034 sec/batch, 1857.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:01,513] [train step44511] D loss: 0.33330 G loss: 2.58801 (0.036 sec/batch, 1772.050 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:01,897] [train step44520] D loss: 0.33283 G loss: 2.17520 (0.035 sec/batch, 1811.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:02,274] [train step44530] D loss: 0.32849 G loss: 2.27044 (0.039 sec/batch, 1641.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:02,654] [train step44541] D loss: 0.32971 G loss: 2.34168 (0.039 sec/batch, 1641.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:03,031] [train step44550] D loss: 0.33061 G loss: 2.47633 (0.038 sec/batch, 1669.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:03,399] [train step44561] D loss: 0.33855 G loss: 2.23994 (0.033 sec/batch, 1911.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:03,776] [train step44570] D loss: 0.34131 G loss: 2.62037 (0.039 sec/batch, 1637.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:04,144] [train step44580] D loss: 0.32675 G loss: 2.32713 (0.035 sec/batch, 1830.113 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:04,521] [train step44590] D loss: 0.32924 G loss: 2.43568 (0.039 sec/batch, 1652.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:04,903] [train step44601] D loss: 0.32754 G loss: 2.26415 (0.035 sec/batch, 1854.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:05,279] [train step44610] D loss: 0.32781 G loss: 2.22493 (0.033 sec/batch, 1912.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:05,663] [train step44621] D loss: 0.33049 G loss: 2.47686 (0.032 sec/batch, 2007.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:06,049] [train step44631] D loss: 0.32753 G loss: 2.25542 (0.033 sec/batch, 1944.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:06,422] [train step44640] D loss: 0.32855 G loss: 2.39465 (0.044 sec/batch, 1454.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:06,803] [train step44650] D loss: 0.32700 G loss: 2.28510 (0.041 sec/batch, 1567.030 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:07,184] [train step44660] D loss: 0.32699 G loss: 2.29094 (0.032 sec/batch, 2017.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:07,563] [train step44670] D loss: 0.32736 G loss: 2.28368 (0.037 sec/batch, 1747.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:07,951] [train step44680] D loss: 0.32640 G loss: 2.28176 (0.036 sec/batch, 1756.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:08,326] [train step44691] D loss: 0.33550 G loss: 2.20180 (0.036 sec/batch, 1788.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:08,701] [train step44700] D loss: 0.32720 G loss: 2.34951 (0.036 sec/batch, 1775.238 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:09,083] [train step44711] D loss: 0.32704 G loss: 2.32878 (0.042 sec/batch, 1532.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:09,449] [train step44721] D loss: 0.32702 G loss: 2.40778 (0.032 sec/batch, 2015.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:09,825] [train step44730] D loss: 0.32619 G loss: 2.28358 (0.039 sec/batch, 1634.549 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:10,213] [train step44740] D loss: 0.33705 G loss: 2.48350 (0.040 sec/batch, 1611.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:10,600] [train step44750] D loss: 0.33086 G loss: 2.40210 (0.038 sec/batch, 1672.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:10,980] [train step44760] D loss: 0.32929 G loss: 2.18679 (0.043 sec/batch, 1489.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:11,363] [train step44771] D loss: 0.33057 G loss: 2.48761 (0.037 sec/batch, 1718.867 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:11,732] [train step44780] D loss: 0.32959 G loss: 2.22558 (0.035 sec/batch, 1807.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:12,116] [train step44790] D loss: 0.32758 G loss: 2.30835 (0.034 sec/batch, 1901.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:12,491] [train step44801] D loss: 0.32882 G loss: 2.30861 (0.034 sec/batch, 1863.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:12,893] [train step44810] D loss: 0.32885 G loss: 2.43130 (0.041 sec/batch, 1551.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:13,276] [train step44820] D loss: 0.33137 G loss: 2.48466 (0.036 sec/batch, 1798.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:13,655] [train step44831] D loss: 0.32714 G loss: 2.26311 (0.039 sec/batch, 1639.411 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:14,043] [train step44840] D loss: 0.32846 G loss: 2.37414 (0.042 sec/batch, 1535.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:14,420] [train step44850] D loss: 0.32855 G loss: 2.32160 (0.038 sec/batch, 1688.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:14,794] [train step44861] D loss: 0.32809 G loss: 2.27497 (0.034 sec/batch, 1857.351 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:15,187] [train step44870] D loss: 0.32707 G loss: 2.32843 (0.037 sec/batch, 1745.320 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:15,552] [train step44880] D loss: 0.32743 G loss: 2.32952 (0.027 sec/batch, 2396.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:15,944] [train step44890] D loss: 0.32670 G loss: 2.26671 (0.039 sec/batch, 1641.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:16,351] [train step44900] D loss: 0.32766 G loss: 2.29777 (0.038 sec/batch, 1696.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:16,725] [train step44910] D loss: 0.32691 G loss: 2.35147 (0.037 sec/batch, 1737.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:17,111] [train step44921] D loss: 0.32669 G loss: 2.30126 (0.036 sec/batch, 1795.615 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:17,483] [train step44930] D loss: 0.33171 G loss: 2.29023 (0.034 sec/batch, 1859.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:17,867] [train step44940] D loss: 0.32904 G loss: 2.18561 (0.037 sec/batch, 1720.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:18,254] [train step44951] D loss: 0.32918 G loss: 2.42241 (0.037 sec/batch, 1733.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:18,624] [train step44960] D loss: 0.32626 G loss: 2.33819 (0.036 sec/batch, 1775.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:19,007] [train step44970] D loss: 0.33168 G loss: 2.23528 (0.038 sec/batch, 1689.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:19,399] [train step44980] D loss: 0.32973 G loss: 2.34527 (0.036 sec/batch, 1802.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:19,790] [train step44991] D loss: 0.33076 G loss: 2.18867 (0.037 sec/batch, 1723.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:20,185] [train step45000] D loss: 0.33116 G loss: 2.41228 (0.038 sec/batch, 1706.183 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:38:20,185] Saved checkpoint at 45000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:20,784] [train step45011] D loss: 0.32645 G loss: 2.30563 (0.038 sec/batch, 1674.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:21,174] [train step45020] D loss: 0.32905 G loss: 2.43504 (0.048 sec/batch, 1332.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:21,566] [train step45030] D loss: 0.32656 G loss: 2.32098 (0.037 sec/batch, 1719.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:21,949] [train step45041] D loss: 0.32684 G loss: 2.24812 (0.038 sec/batch, 1675.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:22,337] [train step45051] D loss: 0.32948 G loss: 2.37433 (0.036 sec/batch, 1765.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:22,723] [train step45060] D loss: 0.32669 G loss: 2.35672 (0.038 sec/batch, 1663.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:23,107] [train step45071] D loss: 0.32668 G loss: 2.30828 (0.040 sec/batch, 1608.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:23,497] [train step45080] D loss: 0.32635 G loss: 2.25477 (0.040 sec/batch, 1602.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:23,882] [train step45090] D loss: 0.33445 G loss: 2.45714 (0.038 sec/batch, 1703.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:24,276] [train step45100] D loss: 0.32602 G loss: 2.28222 (0.041 sec/batch, 1578.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:24,658] [train step45110] D loss: 0.32759 G loss: 2.28835 (0.041 sec/batch, 1546.234 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:25,035] [train step45120] D loss: 0.32910 G loss: 2.30387 (0.033 sec/batch, 1933.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:25,430] [train step45130] D loss: 0.33022 G loss: 2.26169 (0.034 sec/batch, 1881.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:25,823] [train step45141] D loss: 0.32823 G loss: 2.37343 (0.036 sec/batch, 1758.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:26,213] [train step45150] D loss: 0.32841 G loss: 2.34871 (0.038 sec/batch, 1670.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:26,604] [train step45161] D loss: 0.32796 G loss: 2.20990 (0.037 sec/batch, 1751.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:27,010] [train step45171] D loss: 0.33045 G loss: 2.39477 (0.043 sec/batch, 1499.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:27,393] [train step45180] D loss: 0.32829 G loss: 2.34337 (0.036 sec/batch, 1754.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:27,779] [train step45191] D loss: 0.32691 G loss: 2.31728 (0.041 sec/batch, 1557.999 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:28,166] [train step45201] D loss: 0.32663 G loss: 2.31412 (0.040 sec/batch, 1619.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:28,571] [train step45210] D loss: 0.32672 G loss: 2.36101 (0.035 sec/batch, 1818.656 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:28,954] [train step45221] D loss: 0.32785 G loss: 2.23168 (0.040 sec/batch, 1618.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:29,335] [train step45231] D loss: 0.32702 G loss: 2.36353 (0.038 sec/batch, 1680.473 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:29,734] [train step45240] D loss: 0.32818 G loss: 2.39820 (0.036 sec/batch, 1788.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:30,125] [train step45251] D loss: 0.32841 G loss: 2.29360 (0.037 sec/batch, 1719.395 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:30,520] [train step45261] D loss: 0.32719 G loss: 2.29575 (0.037 sec/batch, 1734.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:30,904] [train step45270] D loss: 0.33105 G loss: 2.47825 (0.037 sec/batch, 1710.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:31,297] [train step45280] D loss: 0.32644 G loss: 2.25520 (0.038 sec/batch, 1681.336 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:31,681] [train step45290] D loss: 0.32731 G loss: 2.36144 (0.040 sec/batch, 1605.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:32,047] [train step45300] D loss: 0.32715 G loss: 2.33530 (0.029 sec/batch, 2245.608 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:32,448] [train step45311] D loss: 0.32661 G loss: 2.26673 (0.037 sec/batch, 1707.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:32,832] [train step45321] D loss: 0.32964 G loss: 2.40048 (0.036 sec/batch, 1793.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:33,208] [train step45330] D loss: 0.44170 G loss: 4.28180 (0.037 sec/batch, 1746.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:33,588] [train step45341] D loss: 0.34676 G loss: 1.96890 (0.036 sec/batch, 1780.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:33,959] [train step45351] D loss: 0.34440 G loss: 2.62118 (0.034 sec/batch, 1874.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:34,341] [train step45360] D loss: 0.34117 G loss: 2.04364 (0.035 sec/batch, 1819.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:34,727] [train step45370] D loss: 0.37001 G loss: 3.36212 (0.038 sec/batch, 1668.005 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:35,100] [train step45381] D loss: 0.32969 G loss: 2.19544 (0.033 sec/batch, 1913.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:35,484] [train step45390] D loss: 0.35927 G loss: 1.72392 (0.037 sec/batch, 1752.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:35,862] [train step45400] D loss: 0.33661 G loss: 2.43008 (0.039 sec/batch, 1634.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:36,243] [train step45410] D loss: 0.32908 G loss: 2.48373 (0.039 sec/batch, 1660.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:36,619] [train step45420] D loss: 0.33143 G loss: 2.54428 (0.049 sec/batch, 1305.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:36,998] [train step45431] D loss: 0.32721 G loss: 2.19918 (0.050 sec/batch, 1275.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:37,416] [train step45441] D loss: 0.32716 G loss: 2.25287 (0.047 sec/batch, 1375.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:37,822] [train step45450] D loss: 0.32831 G loss: 2.25179 (0.044 sec/batch, 1460.349 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:38,214] [train step45461] D loss: 0.32798 G loss: 2.46086 (0.048 sec/batch, 1321.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:38,625] [train step45471] D loss: 0.33122 G loss: 2.39275 (0.036 sec/batch, 1775.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:39,024] [train step45480] D loss: 0.32642 G loss: 2.25479 (0.038 sec/batch, 1680.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:39,410] [train step45490] D loss: 0.32825 G loss: 2.24783 (0.039 sec/batch, 1635.117 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:39,810] [train step45501] D loss: 0.32819 G loss: 2.37307 (0.035 sec/batch, 1811.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:40,209] [train step45510] D loss: 0.32778 G loss: 2.34901 (0.045 sec/batch, 1423.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:40,601] [train step45520] D loss: 0.32675 G loss: 2.33584 (0.040 sec/batch, 1610.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:41,008] [train step45531] D loss: 0.33036 G loss: 2.45264 (0.036 sec/batch, 1756.984 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:41,404] [train step45540] D loss: 0.32844 G loss: 2.36216 (0.042 sec/batch, 1539.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:41,804] [train step45551] D loss: 0.32997 G loss: 2.22501 (0.041 sec/batch, 1572.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:42,195] [train step45561] D loss: 0.33910 G loss: 2.55899 (0.039 sec/batch, 1632.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:42,576] [train step45570] D loss: 0.33326 G loss: 2.45270 (0.042 sec/batch, 1530.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:42,966] [train step45580] D loss: 0.32858 G loss: 2.17972 (0.040 sec/batch, 1606.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:43,348] [train step45591] D loss: 0.32745 G loss: 2.29077 (0.040 sec/batch, 1604.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:43,728] [train step45600] D loss: 0.33982 G loss: 2.34818 (0.037 sec/batch, 1748.583 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:44,128] [train step45611] D loss: 0.34836 G loss: 2.70856 (0.037 sec/batch, 1731.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:44,507] [train step45620] D loss: 0.34178 G loss: 2.22084 (0.040 sec/batch, 1616.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:44,908] [train step45630] D loss: 0.33126 G loss: 2.10624 (0.041 sec/batch, 1576.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:45,283] [train step45641] D loss: 0.32726 G loss: 2.42247 (0.034 sec/batch, 1909.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:45,670] [train step45651] D loss: 0.32602 G loss: 2.32780 (0.037 sec/batch, 1738.155 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:46,057] [train step45660] D loss: 0.33230 G loss: 2.49849 (0.041 sec/batch, 1556.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:46,443] [train step45670] D loss: 0.32862 G loss: 2.26524 (0.040 sec/batch, 1581.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:46,840] [train step45680] D loss: 0.34028 G loss: 2.54138 (0.038 sec/batch, 1705.283 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:47,230] [train step45690] D loss: 0.38419 G loss: 1.61275 (0.038 sec/batch, 1684.142 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:47,617] [train step45701] D loss: 0.38118 G loss: 3.43384 (0.037 sec/batch, 1724.587 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:48,036] [train step45711] D loss: 0.34346 G loss: 2.16873 (0.037 sec/batch, 1735.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:48,423] [train step45720] D loss: 0.33207 G loss: 2.52810 (0.040 sec/batch, 1615.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:48,799] [train step45730] D loss: 0.32834 G loss: 2.16635 (0.038 sec/batch, 1674.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:49,194] [train step45740] D loss: 0.32835 G loss: 2.34604 (0.038 sec/batch, 1669.011 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:49,582] [train step45750] D loss: 0.32757 G loss: 2.27348 (0.035 sec/batch, 1835.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:49,974] [train step45761] D loss: 0.32820 G loss: 2.42336 (0.044 sec/batch, 1462.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:50,370] [train step45771] D loss: 0.32938 G loss: 2.30763 (0.043 sec/batch, 1501.040 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:50,762] [train step45780] D loss: 0.33232 G loss: 2.53684 (0.039 sec/batch, 1628.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:51,151] [train step45791] D loss: 0.32798 G loss: 2.17558 (0.035 sec/batch, 1849.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:51,544] [train step45800] D loss: 0.32716 G loss: 2.37579 (0.037 sec/batch, 1732.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:51,931] [train step45810] D loss: 0.32702 G loss: 2.31711 (0.032 sec/batch, 2017.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:52,327] [train step45821] D loss: 0.32763 G loss: 2.30844 (0.037 sec/batch, 1712.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:52,713] [train step45830] D loss: 0.32736 G loss: 2.27623 (0.040 sec/batch, 1617.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:53,102] [train step45840] D loss: 0.32680 G loss: 2.30079 (0.045 sec/batch, 1423.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:53,485] [train step45850] D loss: 0.32822 G loss: 2.27868 (0.029 sec/batch, 2169.754 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:53,886] [train step45861] D loss: 0.32703 G loss: 2.33035 (0.039 sec/batch, 1625.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:54,278] [train step45870] D loss: 0.32762 G loss: 2.35562 (0.037 sec/batch, 1708.224 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:54,666] [train step45880] D loss: 0.32669 G loss: 2.32963 (0.039 sec/batch, 1653.080 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:55,054] [train step45890] D loss: 0.32953 G loss: 2.22014 (0.039 sec/batch, 1633.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:55,453] [train step45900] D loss: 0.32649 G loss: 2.26115 (0.040 sec/batch, 1582.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:55,837] [train step45911] D loss: 0.32761 G loss: 2.44730 (0.040 sec/batch, 1605.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:56,234] [train step45921] D loss: 0.32781 G loss: 2.22550 (0.038 sec/batch, 1688.666 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:56,615] [train step45930] D loss: 0.32741 G loss: 2.25300 (0.035 sec/batch, 1828.106 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:56,998] [train step45941] D loss: 0.32675 G loss: 2.27643 (0.039 sec/batch, 1654.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:57,391] [train step45951] D loss: 0.32712 G loss: 2.30260 (0.037 sec/batch, 1748.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:57,773] [train step45960] D loss: 0.32696 G loss: 2.38371 (0.038 sec/batch, 1701.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:58,159] [train step45970] D loss: 0.32696 G loss: 2.23750 (0.040 sec/batch, 1616.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:58,566] [train step45981] D loss: 0.32721 G loss: 2.40505 (0.036 sec/batch, 1791.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:58,942] [train step45990] D loss: 0.32765 G loss: 2.42452 (0.045 sec/batch, 1428.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:59,342] [train step46000] D loss: 0.32732 G loss: 2.24413 (0.041 sec/batch, 1575.122 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:38:59,343] Saved checkpoint at 46000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:38:59,933] [train step46011] D loss: 0.32725 G loss: 2.37673 (0.035 sec/batch, 1850.577 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:00,337] [train step46020] D loss: 0.32667 G loss: 2.31372 (0.041 sec/batch, 1551.677 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:00,714] [train step46031] D loss: 0.32673 G loss: 2.37279 (0.036 sec/batch, 1772.366 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:01,103] [train step46040] D loss: 0.32718 G loss: 2.25542 (0.038 sec/batch, 1666.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:01,493] [train step46050] D loss: 0.32657 G loss: 2.28589 (0.037 sec/batch, 1733.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:01,881] [train step46060] D loss: 0.32657 G loss: 2.35942 (0.037 sec/batch, 1739.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:02,253] [train step46071] D loss: 0.32682 G loss: 2.29284 (0.044 sec/batch, 1458.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:02,642] [train step46080] D loss: 0.32651 G loss: 2.32330 (0.041 sec/batch, 1565.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:03,012] [train step46090] D loss: 0.32699 G loss: 2.26106 (0.035 sec/batch, 1812.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:03,384] [train step46100] D loss: 0.32676 G loss: 2.30672 (0.039 sec/batch, 1644.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:03,755] [train step46110] D loss: 0.32651 G loss: 2.24691 (0.036 sec/batch, 1762.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:04,143] [train step46121] D loss: 0.32670 G loss: 2.31997 (0.034 sec/batch, 1870.265 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:04,528] [train step46130] D loss: 0.32668 G loss: 2.34249 (0.034 sec/batch, 1907.016 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:04,904] [train step46140] D loss: 0.32699 G loss: 2.36491 (0.038 sec/batch, 1682.095 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:05,280] [train step46150] D loss: 0.32725 G loss: 2.29553 (0.035 sec/batch, 1844.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:05,670] [train step46160] D loss: 0.32643 G loss: 2.29439 (0.040 sec/batch, 1582.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:06,043] [train step46170] D loss: 0.32733 G loss: 2.33593 (0.037 sec/batch, 1735.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:06,416] [train step46181] D loss: 0.32648 G loss: 2.27496 (0.035 sec/batch, 1811.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:06,809] [train step46191] D loss: 0.32730 G loss: 2.35048 (0.035 sec/batch, 1807.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:07,193] [train step46200] D loss: 0.32632 G loss: 2.36963 (0.040 sec/batch, 1613.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:07,585] [train step46210] D loss: 0.32860 G loss: 2.21823 (0.039 sec/batch, 1648.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:07,964] [train step46221] D loss: 0.32692 G loss: 2.38515 (0.037 sec/batch, 1731.819 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:08,335] [train step46230] D loss: 0.32668 G loss: 2.26730 (0.040 sec/batch, 1589.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:08,723] [train step46241] D loss: 0.32693 G loss: 2.37116 (0.038 sec/batch, 1692.521 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:09,120] [train step46251] D loss: 0.32640 G loss: 2.28805 (0.036 sec/batch, 1784.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:09,500] [train step46260] D loss: 0.32726 G loss: 2.27350 (0.036 sec/batch, 1756.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:09,882] [train step46271] D loss: 0.32641 G loss: 2.36914 (0.037 sec/batch, 1727.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:10,259] [train step46281] D loss: 0.32659 G loss: 2.31242 (0.038 sec/batch, 1701.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:10,651] [train step46290] D loss: 0.32709 G loss: 2.22813 (0.039 sec/batch, 1626.113 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:11,022] [train step46301] D loss: 0.32666 G loss: 2.31598 (0.037 sec/batch, 1746.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:11,392] [train step46310] D loss: 0.32654 G loss: 2.34582 (0.037 sec/batch, 1748.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:11,788] [train step46320] D loss: 0.32631 G loss: 2.31768 (0.035 sec/batch, 1842.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:12,166] [train step46331] D loss: 0.32646 G loss: 2.31043 (0.035 sec/batch, 1833.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:12,549] [train step46340] D loss: 0.32621 G loss: 2.30941 (0.041 sec/batch, 1547.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:12,939] [train step46350] D loss: 0.32646 G loss: 2.28306 (0.034 sec/batch, 1883.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:13,331] [train step46360] D loss: 0.32604 G loss: 2.32372 (0.038 sec/batch, 1694.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:13,727] [train step46371] D loss: 0.32637 G loss: 2.36652 (0.040 sec/batch, 1617.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:14,115] [train step46380] D loss: 0.32645 G loss: 2.26634 (0.031 sec/batch, 2056.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:14,511] [train step46390] D loss: 0.32656 G loss: 2.35942 (0.037 sec/batch, 1728.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:14,906] [train step46401] D loss: 0.32633 G loss: 2.23240 (0.036 sec/batch, 1785.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:15,297] [train step46410] D loss: 0.32615 G loss: 2.28594 (0.040 sec/batch, 1592.210 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:15,684] [train step46420] D loss: 0.32628 G loss: 2.31460 (0.038 sec/batch, 1703.984 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:16,090] [train step46431] D loss: 0.32630 G loss: 2.28658 (0.046 sec/batch, 1378.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:16,478] [train step46440] D loss: 0.32600 G loss: 2.31268 (0.039 sec/batch, 1643.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:16,871] [train step46451] D loss: 0.32632 G loss: 2.27042 (0.036 sec/batch, 1770.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:17,260] [train step46460] D loss: 0.32710 G loss: 2.23632 (0.041 sec/batch, 1555.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:17,643] [train step46470] D loss: 0.32656 G loss: 2.36403 (0.040 sec/batch, 1603.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:18,048] [train step46481] D loss: 0.32579 G loss: 2.29333 (0.036 sec/batch, 1776.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:18,436] [train step46491] D loss: 0.32600 G loss: 2.33439 (0.035 sec/batch, 1804.038 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:18,830] [train step46500] D loss: 0.32613 G loss: 2.28891 (0.044 sec/batch, 1444.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:19,226] [train step46510] D loss: 0.32614 G loss: 2.31524 (0.032 sec/batch, 1992.455 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:19,642] [train step46520] D loss: 0.32647 G loss: 2.25682 (0.040 sec/batch, 1615.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:20,056] [train step46530] D loss: 0.32624 G loss: 2.25333 (0.041 sec/batch, 1563.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:20,443] [train step46540] D loss: 0.32626 G loss: 2.33061 (0.040 sec/batch, 1596.861 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:20,840] [train step46550] D loss: 0.32607 G loss: 2.33643 (0.049 sec/batch, 1317.100 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:21,236] [train step46560] D loss: 0.32648 G loss: 2.37112 (0.041 sec/batch, 1543.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:21,631] [train step46571] D loss: 0.32651 G loss: 2.23694 (0.040 sec/batch, 1589.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:22,029] [train step46581] D loss: 0.32679 G loss: 2.28616 (0.039 sec/batch, 1636.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:22,421] [train step46590] D loss: 0.32631 G loss: 2.28572 (0.041 sec/batch, 1577.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:22,814] [train step46601] D loss: 0.32574 G loss: 2.31353 (0.034 sec/batch, 1889.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:23,217] [train step46611] D loss: 0.32645 G loss: 2.24916 (0.038 sec/batch, 1694.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:23,611] [train step46620] D loss: 0.32588 G loss: 2.34515 (0.038 sec/batch, 1681.610 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:24,000] [train step46630] D loss: 0.32622 G loss: 2.29125 (0.045 sec/batch, 1411.006 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:24,386] [train step46640] D loss: 0.32588 G loss: 2.28820 (0.039 sec/batch, 1649.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:24,766] [train step46650] D loss: 0.32666 G loss: 2.32135 (0.038 sec/batch, 1693.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:25,158] [train step46661] D loss: 0.32629 G loss: 2.26545 (0.036 sec/batch, 1763.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:25,538] [train step46671] D loss: 0.32609 G loss: 2.29829 (0.035 sec/batch, 1850.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:25,931] [train step46680] D loss: 0.32637 G loss: 2.36667 (0.040 sec/batch, 1596.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:26,327] [train step46691] D loss: 0.32691 G loss: 2.19141 (0.039 sec/batch, 1656.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:26,714] [train step46701] D loss: 0.32614 G loss: 2.35787 (0.038 sec/batch, 1690.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:27,107] [train step46710] D loss: 0.32637 G loss: 2.24946 (0.038 sec/batch, 1700.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:27,494] [train step46720] D loss: 0.32639 G loss: 2.36602 (0.036 sec/batch, 1782.298 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:27,887] [train step46731] D loss: 0.32602 G loss: 2.31829 (0.035 sec/batch, 1814.881 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:28,278] [train step46740] D loss: 0.32571 G loss: 2.29135 (0.039 sec/batch, 1621.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:28,660] [train step46751] D loss: 0.32604 G loss: 2.31843 (0.039 sec/batch, 1653.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:29,043] [train step46760] D loss: 0.32613 G loss: 2.27706 (0.037 sec/batch, 1731.473 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:29,437] [train step46770] D loss: 0.32608 G loss: 2.33469 (0.037 sec/batch, 1707.268 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:29,821] [train step46781] D loss: 0.32614 G loss: 2.25294 (0.039 sec/batch, 1659.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:30,239] [train step46791] D loss: 0.32594 G loss: 2.31813 (0.040 sec/batch, 1606.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:30,631] [train step46800] D loss: 0.32663 G loss: 2.29237 (0.040 sec/batch, 1619.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:31,016] [train step46811] D loss: 0.32600 G loss: 2.31068 (0.044 sec/batch, 1460.722 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:31,407] [train step46820] D loss: 0.32601 G loss: 2.31815 (0.039 sec/batch, 1621.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:31,794] [train step46830] D loss: 0.32612 G loss: 2.26070 (0.039 sec/batch, 1651.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:32,172] [train step46841] D loss: 0.32610 G loss: 2.36982 (0.038 sec/batch, 1706.487 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:32,556] [train step46850] D loss: 0.32583 G loss: 2.29866 (0.036 sec/batch, 1771.454 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:32,941] [train step46860] D loss: 0.32596 G loss: 2.29407 (0.038 sec/batch, 1675.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:33,327] [train step46871] D loss: 0.32636 G loss: 2.34031 (0.036 sec/batch, 1777.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:33,704] [train step46880] D loss: 0.32570 G loss: 2.29311 (0.037 sec/batch, 1751.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:34,078] [train step46890] D loss: 0.32604 G loss: 2.34568 (0.036 sec/batch, 1754.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:34,461] [train step46901] D loss: 0.32647 G loss: 2.24118 (0.038 sec/batch, 1675.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:34,838] [train step46910] D loss: 0.32579 G loss: 2.32087 (0.043 sec/batch, 1489.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:35,216] [train step46920] D loss: 0.32585 G loss: 2.26982 (0.034 sec/batch, 1909.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:35,610] [train step46930] D loss: 0.32706 G loss: 2.30895 (0.041 sec/batch, 1567.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:35,990] [train step46941] D loss: 0.32775 G loss: 2.25344 (0.037 sec/batch, 1742.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:36,368] [train step46950] D loss: 0.32598 G loss: 2.32148 (0.042 sec/batch, 1516.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:36,749] [train step46961] D loss: 0.32575 G loss: 2.32667 (0.040 sec/batch, 1584.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:37,124] [train step46970] D loss: 0.32631 G loss: 2.27724 (0.036 sec/batch, 1775.590 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:37,509] [train step46980] D loss: 0.32586 G loss: 2.30838 (0.037 sec/batch, 1740.296 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:37,883] [train step46990] D loss: 0.32611 G loss: 2.30392 (0.036 sec/batch, 1801.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:38,264] [train step47000] D loss: 0.32608 G loss: 2.31489 (0.035 sec/batch, 1815.790 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:39:38,264] Saved checkpoint at 47000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:38,881] [train step47010] D loss: 0.32637 G loss: 2.27653 (0.038 sec/batch, 1674.738 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:39,265] [train step47021] D loss: 0.32638 G loss: 2.25550 (0.038 sec/batch, 1684.449 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:39,652] [train step47031] D loss: 0.32595 G loss: 2.34544 (0.042 sec/batch, 1537.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:40,026] [train step47040] D loss: 0.32593 G loss: 2.29587 (0.035 sec/batch, 1816.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:40,407] [train step47051] D loss: 0.32590 G loss: 2.28537 (0.037 sec/batch, 1713.874 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:40,813] [train step47061] D loss: 0.32681 G loss: 2.31507 (0.040 sec/batch, 1599.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:41,189] [train step47070] D loss: 0.32608 G loss: 2.28221 (0.039 sec/batch, 1643.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:41,572] [train step47081] D loss: 0.32621 G loss: 2.31924 (0.036 sec/batch, 1759.725 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:41,957] [train step47091] D loss: 0.32582 G loss: 2.32528 (0.044 sec/batch, 1445.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:42,346] [train step47100] D loss: 0.32581 G loss: 2.29675 (0.037 sec/batch, 1722.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:42,721] [train step47110] D loss: 0.32556 G loss: 2.31092 (0.036 sec/batch, 1760.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:43,101] [train step47121] D loss: 0.32584 G loss: 2.34703 (0.036 sec/batch, 1760.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:43,482] [train step47130] D loss: 0.32607 G loss: 2.34740 (0.037 sec/batch, 1715.374 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:43,861] [train step47141] D loss: 0.32650 G loss: 2.24005 (0.038 sec/batch, 1702.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:44,249] [train step47150] D loss: 0.32679 G loss: 2.36705 (0.034 sec/batch, 1887.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:44,647] [train step47160] D loss: 0.32591 G loss: 2.33780 (0.037 sec/batch, 1728.107 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:45,025] [train step47171] D loss: 0.32698 G loss: 2.33502 (0.037 sec/batch, 1708.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:45,410] [train step47181] D loss: 0.32607 G loss: 2.35413 (0.038 sec/batch, 1669.188 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:45,800] [train step47190] D loss: 0.32573 G loss: 2.28355 (0.040 sec/batch, 1590.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:46,181] [train step47201] D loss: 0.32588 G loss: 2.31345 (0.036 sec/batch, 1761.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:46,572] [train step47211] D loss: 0.32600 G loss: 2.26751 (0.038 sec/batch, 1700.508 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:46,969] [train step47220] D loss: 0.32705 G loss: 2.23643 (0.042 sec/batch, 1513.429 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:47,351] [train step47230] D loss: 0.32627 G loss: 2.37640 (0.037 sec/batch, 1725.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:47,747] [train step47240] D loss: 0.32733 G loss: 2.24409 (0.045 sec/batch, 1437.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:48,132] [train step47250] D loss: 0.32564 G loss: 2.28278 (0.039 sec/batch, 1634.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:48,509] [train step47261] D loss: 0.32569 G loss: 2.33051 (0.033 sec/batch, 1938.050 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:48,903] [train step47270] D loss: 0.32587 G loss: 2.31335 (0.033 sec/batch, 1916.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:49,295] [train step47280] D loss: 0.32593 G loss: 2.29271 (0.038 sec/batch, 1699.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:49,686] [train step47291] D loss: 0.32598 G loss: 2.30148 (0.043 sec/batch, 1497.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:50,075] [train step47300] D loss: 0.32594 G loss: 2.34423 (0.037 sec/batch, 1721.027 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:50,459] [train step47310] D loss: 0.32558 G loss: 2.26444 (0.038 sec/batch, 1695.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:50,852] [train step47320] D loss: 0.32610 G loss: 2.32021 (0.035 sec/batch, 1827.397 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:51,253] [train step47330] D loss: 0.32619 G loss: 2.27865 (0.039 sec/batch, 1654.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:51,650] [train step47340] D loss: 0.32597 G loss: 2.29017 (0.037 sec/batch, 1741.210 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:52,052] [train step47350] D loss: 0.32587 G loss: 2.33869 (0.037 sec/batch, 1749.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:52,440] [train step47360] D loss: 0.32591 G loss: 2.29870 (0.041 sec/batch, 1555.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:52,836] [train step47370] D loss: 0.32710 G loss: 2.30215 (0.044 sec/batch, 1457.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:53,214] [train step47381] D loss: 0.32563 G loss: 2.32906 (0.032 sec/batch, 1983.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:53,605] [train step47391] D loss: 0.32618 G loss: 2.20883 (0.041 sec/batch, 1572.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:53,995] [train step47400] D loss: 0.33067 G loss: 2.27722 (0.037 sec/batch, 1719.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:54,380] [train step47411] D loss: 0.32608 G loss: 2.25672 (0.036 sec/batch, 1782.582 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:54,763] [train step47420] D loss: 0.32635 G loss: 2.24437 (0.044 sec/batch, 1460.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:55,154] [train step47430] D loss: 0.32587 G loss: 2.30478 (0.037 sec/batch, 1734.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:55,535] [train step47440] D loss: 0.32581 G loss: 2.36393 (0.036 sec/batch, 1770.215 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:55,932] [train step47450] D loss: 0.32603 G loss: 2.37086 (0.042 sec/batch, 1519.977 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:56,315] [train step47460] D loss: 0.32566 G loss: 2.30580 (0.035 sec/batch, 1836.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:56,711] [train step47471] D loss: 0.32576 G loss: 2.30873 (0.040 sec/batch, 1603.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:57,098] [train step47481] D loss: 0.32575 G loss: 2.30972 (0.038 sec/batch, 1688.454 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:57,480] [train step47490] D loss: 0.32585 G loss: 2.31848 (0.037 sec/batch, 1753.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:57,869] [train step47500] D loss: 0.32613 G loss: 2.32671 (0.039 sec/batch, 1659.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:58,266] [train step47511] D loss: 0.32565 G loss: 2.32246 (0.039 sec/batch, 1651.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:58,655] [train step47520] D loss: 0.32651 G loss: 2.35204 (0.039 sec/batch, 1623.114 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:59,054] [train step47531] D loss: 0.32831 G loss: 2.17896 (0.042 sec/batch, 1527.580 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:59,440] [train step47540] D loss: 0.32622 G loss: 2.35465 (0.041 sec/batch, 1549.268 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:39:59,828] [train step47550] D loss: 0.32637 G loss: 2.29597 (0.039 sec/batch, 1645.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:00,219] [train step47561] D loss: 0.32651 G loss: 2.39347 (0.037 sec/batch, 1727.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:00,613] [train step47570] D loss: 0.32582 G loss: 2.26988 (0.036 sec/batch, 1765.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:00,998] [train step47580] D loss: 0.32542 G loss: 2.32075 (0.040 sec/batch, 1604.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:01,402] [train step47590] D loss: 0.32618 G loss: 2.35559 (0.044 sec/batch, 1459.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:01,802] [train step47601] D loss: 0.32573 G loss: 2.27571 (0.037 sec/batch, 1734.696 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:02,203] [train step47610] D loss: 0.32580 G loss: 2.24955 (0.038 sec/batch, 1690.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:02,593] [train step47621] D loss: 0.32630 G loss: 2.33168 (0.037 sec/batch, 1726.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:02,962] [train step47631] D loss: 0.32640 G loss: 2.32027 (0.033 sec/batch, 1942.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:03,348] [train step47640] D loss: 0.32578 G loss: 2.35642 (0.036 sec/batch, 1796.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:03,720] [train step47650] D loss: 0.32568 G loss: 2.29549 (0.029 sec/batch, 2191.668 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:04,100] [train step47661] D loss: 0.32764 G loss: 2.38200 (0.037 sec/batch, 1739.101 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:04,490] [train step47670] D loss: 0.32578 G loss: 2.30490 (0.040 sec/batch, 1585.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:04,862] [train step47680] D loss: 0.32585 G loss: 2.26538 (0.041 sec/batch, 1571.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:05,251] [train step47691] D loss: 0.32640 G loss: 2.34723 (0.038 sec/batch, 1678.246 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:05,622] [train step47700] D loss: 0.32558 G loss: 2.29062 (0.036 sec/batch, 1757.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:05,998] [train step47711] D loss: 0.32577 G loss: 2.31966 (0.035 sec/batch, 1828.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:06,377] [train step47721] D loss: 0.32565 G loss: 2.31416 (0.037 sec/batch, 1728.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:06,751] [train step47730] D loss: 0.32628 G loss: 2.30072 (0.035 sec/batch, 1812.492 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:07,128] [train step47741] D loss: 0.32569 G loss: 2.32930 (0.038 sec/batch, 1664.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:07,518] [train step47751] D loss: 0.32558 G loss: 2.31650 (0.034 sec/batch, 1862.157 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:07,899] [train step47760] D loss: 0.32554 G loss: 2.28238 (0.039 sec/batch, 1621.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:08,280] [train step47771] D loss: 0.32615 G loss: 2.29150 (0.045 sec/batch, 1436.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:08,660] [train step47781] D loss: 0.32625 G loss: 2.27016 (0.038 sec/batch, 1678.089 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:09,047] [train step47790] D loss: 0.32546 G loss: 2.31864 (0.035 sec/batch, 1841.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:09,428] [train step47801] D loss: 0.32609 G loss: 2.35210 (0.035 sec/batch, 1832.487 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:09,807] [train step47811] D loss: 0.32750 G loss: 2.19049 (0.036 sec/batch, 1761.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:10,183] [train step47820] D loss: 0.32820 G loss: 2.14778 (0.037 sec/batch, 1746.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:10,580] [train step47831] D loss: 0.32689 G loss: 2.37562 (0.034 sec/batch, 1861.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:10,963] [train step47840] D loss: 0.32731 G loss: 2.27754 (0.041 sec/batch, 1577.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:11,337] [train step47850] D loss: 0.32623 G loss: 2.38064 (0.037 sec/batch, 1727.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:11,730] [train step47860] D loss: 0.32618 G loss: 2.26575 (0.038 sec/batch, 1667.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:12,116] [train step47871] D loss: 0.32580 G loss: 2.28930 (0.040 sec/batch, 1614.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:12,528] [train step47880] D loss: 0.32591 G loss: 2.27883 (0.037 sec/batch, 1721.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:12,917] [train step47891] D loss: 0.32570 G loss: 2.28836 (0.042 sec/batch, 1537.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:13,283] [train step47900] D loss: 0.32603 G loss: 2.25997 (0.027 sec/batch, 2329.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:13,684] [train step47910] D loss: 0.32644 G loss: 2.25937 (0.037 sec/batch, 1707.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:14,071] [train step47920] D loss: 0.32653 G loss: 2.40598 (0.039 sec/batch, 1635.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:14,459] [train step47931] D loss: 0.32610 G loss: 2.28266 (0.043 sec/batch, 1495.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:14,840] [train step47940] D loss: 0.32761 G loss: 2.20114 (0.033 sec/batch, 1929.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:15,233] [train step47951] D loss: 0.32611 G loss: 2.36660 (0.037 sec/batch, 1736.053 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:15,619] [train step47960] D loss: 0.32584 G loss: 2.24128 (0.034 sec/batch, 1882.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:16,016] [train step47970] D loss: 0.32821 G loss: 2.48930 (0.044 sec/batch, 1464.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:16,405] [train step47981] D loss: 0.32743 G loss: 2.18286 (0.044 sec/batch, 1448.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:16,803] [train step47990] D loss: 0.32688 G loss: 2.40272 (0.037 sec/batch, 1721.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:17,186] [train step48000] D loss: 0.32597 G loss: 2.37828 (0.036 sec/batch, 1756.719 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:40:17,187] Saved checkpoint at 48000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:17,799] [train step48011] D loss: 0.32614 G loss: 2.31123 (0.039 sec/batch, 1628.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:18,183] [train step48020] D loss: 0.32630 G loss: 2.35977 (0.040 sec/batch, 1613.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:18,577] [train step48030] D loss: 0.32622 G loss: 2.20608 (0.050 sec/batch, 1268.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:18,967] [train step48041] D loss: 0.32657 G loss: 2.39578 (0.039 sec/batch, 1639.661 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:19,350] [train step48051] D loss: 0.32627 G loss: 2.27451 (0.037 sec/batch, 1735.806 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:19,744] [train step48060] D loss: 0.32593 G loss: 2.30863 (0.035 sec/batch, 1808.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:20,133] [train step48071] D loss: 0.32601 G loss: 2.29021 (0.035 sec/batch, 1845.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:20,522] [train step48080] D loss: 0.32602 G loss: 2.26399 (0.041 sec/batch, 1574.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:20,912] [train step48090] D loss: 0.32591 G loss: 2.36003 (0.036 sec/batch, 1794.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:21,302] [train step48100] D loss: 0.32595 G loss: 2.25754 (0.039 sec/batch, 1627.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:21,698] [train step48110] D loss: 0.32612 G loss: 2.27054 (0.037 sec/batch, 1738.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:22,077] [train step48120] D loss: 0.32637 G loss: 2.39153 (0.034 sec/batch, 1864.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:22,471] [train step48131] D loss: 0.32561 G loss: 2.27910 (0.040 sec/batch, 1584.036 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:22,882] [train step48140] D loss: 0.32575 G loss: 2.31037 (0.038 sec/batch, 1670.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:23,276] [train step48150] D loss: 0.32657 G loss: 2.22185 (0.045 sec/batch, 1423.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:23,670] [train step48160] D loss: 0.32568 G loss: 2.29872 (0.041 sec/batch, 1573.534 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:24,068] [train step48170] D loss: 0.32583 G loss: 2.29686 (0.038 sec/batch, 1691.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:24,453] [train step48180] D loss: 0.32577 G loss: 2.32177 (0.044 sec/batch, 1466.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:24,847] [train step48191] D loss: 0.32571 G loss: 2.28540 (0.040 sec/batch, 1606.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:25,231] [train step48200] D loss: 0.32621 G loss: 2.31226 (0.039 sec/batch, 1659.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:25,614] [train step48210] D loss: 0.32541 G loss: 2.30485 (0.035 sec/batch, 1825.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:26,007] [train step48221] D loss: 0.32743 G loss: 2.41315 (0.039 sec/batch, 1659.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:26,396] [train step48230] D loss: 0.32592 G loss: 2.25644 (0.040 sec/batch, 1619.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:26,786] [train step48240] D loss: 0.32624 G loss: 2.23637 (0.047 sec/batch, 1362.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:27,170] [train step48250] D loss: 0.32611 G loss: 2.39456 (0.040 sec/batch, 1610.368 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:27,552] [train step48261] D loss: 0.32581 G loss: 2.26109 (0.039 sec/batch, 1655.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:27,941] [train step48270] D loss: 0.32611 G loss: 2.27893 (0.038 sec/batch, 1693.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:28,326] [train step48281] D loss: 0.32592 G loss: 2.27299 (0.036 sec/batch, 1801.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:28,719] [train step48290] D loss: 0.32582 G loss: 2.29435 (0.040 sec/batch, 1601.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:29,117] [train step48300] D loss: 0.32612 G loss: 2.33143 (0.044 sec/batch, 1453.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:29,504] [train step48311] D loss: 0.32580 G loss: 2.25752 (0.036 sec/batch, 1796.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:29,895] [train step48320] D loss: 0.32578 G loss: 2.34088 (0.043 sec/batch, 1490.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:30,280] [train step48330] D loss: 0.32552 G loss: 2.26666 (0.038 sec/batch, 1694.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:30,674] [train step48340] D loss: 0.32621 G loss: 2.33582 (0.037 sec/batch, 1720.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:31,065] [train step48350] D loss: 0.32577 G loss: 2.30709 (0.037 sec/batch, 1718.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:31,451] [train step48360] D loss: 0.32623 G loss: 2.37812 (0.046 sec/batch, 1390.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:31,835] [train step48370] D loss: 0.32619 G loss: 2.23852 (0.041 sec/batch, 1557.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:32,233] [train step48381] D loss: 0.32577 G loss: 2.32046 (0.046 sec/batch, 1380.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:32,630] [train step48390] D loss: 0.32557 G loss: 2.27245 (0.043 sec/batch, 1501.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:33,022] [train step48400] D loss: 0.32629 G loss: 2.20686 (0.034 sec/batch, 1870.174 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:33,413] [train step48411] D loss: 0.32636 G loss: 2.36399 (0.035 sec/batch, 1820.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:33,803] [train step48420] D loss: 0.32617 G loss: 2.35236 (0.037 sec/batch, 1720.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:34,203] [train step48430] D loss: 0.32556 G loss: 2.30643 (0.037 sec/batch, 1711.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:34,582] [train step48440] D loss: 0.32648 G loss: 2.23510 (0.038 sec/batch, 1696.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:34,961] [train step48450] D loss: 0.32622 G loss: 2.35964 (0.043 sec/batch, 1497.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:35,344] [train step48460] D loss: 0.32586 G loss: 2.31893 (0.032 sec/batch, 2030.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:35,728] [train step48470] D loss: 0.32654 G loss: 2.37901 (0.036 sec/batch, 1802.645 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:36,116] [train step48480] D loss: 0.32580 G loss: 2.36196 (0.037 sec/batch, 1727.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:36,494] [train step48491] D loss: 0.32660 G loss: 2.34047 (0.043 sec/batch, 1487.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:36,873] [train step48500] D loss: 0.32585 G loss: 2.34307 (0.040 sec/batch, 1615.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:37,267] [train step48510] D loss: 0.32652 G loss: 2.22858 (0.038 sec/batch, 1699.959 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:37,641] [train step48521] D loss: 0.32695 G loss: 2.45086 (0.035 sec/batch, 1823.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:38,023] [train step48530] D loss: 0.32738 G loss: 2.20743 (0.041 sec/batch, 1553.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:38,409] [train step48540] D loss: 0.32603 G loss: 2.27689 (0.040 sec/batch, 1581.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:38,796] [train step48550] D loss: 0.32599 G loss: 2.35090 (0.042 sec/batch, 1511.172 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:39,183] [train step48561] D loss: 0.32590 G loss: 2.33636 (0.043 sec/batch, 1481.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:39,564] [train step48570] D loss: 0.32547 G loss: 2.29804 (0.034 sec/batch, 1857.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:39,942] [train step48581] D loss: 0.32641 G loss: 2.32660 (0.041 sec/batch, 1561.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:40,338] [train step48591] D loss: 0.32538 G loss: 2.30365 (0.036 sec/batch, 1767.337 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:40,715] [train step48600] D loss: 0.32600 G loss: 2.25547 (0.040 sec/batch, 1605.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:41,096] [train step48611] D loss: 0.32667 G loss: 2.39118 (0.036 sec/batch, 1790.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:41,489] [train step48621] D loss: 0.32636 G loss: 2.18209 (0.037 sec/batch, 1712.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:41,879] [train step48630] D loss: 0.32594 G loss: 2.25442 (0.041 sec/batch, 1564.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:42,266] [train step48640] D loss: 0.32601 G loss: 2.34349 (0.036 sec/batch, 1770.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:42,645] [train step48651] D loss: 0.32564 G loss: 2.31265 (0.038 sec/batch, 1696.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:43,031] [train step48660] D loss: 0.32578 G loss: 2.28787 (0.041 sec/batch, 1562.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:43,424] [train step48670] D loss: 0.32581 G loss: 2.34492 (0.038 sec/batch, 1668.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:43,816] [train step48681] D loss: 0.32563 G loss: 2.29130 (0.040 sec/batch, 1615.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:44,225] [train step48690] D loss: 0.32655 G loss: 2.37878 (0.035 sec/batch, 1818.176 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:44,617] [train step48701] D loss: 0.32607 G loss: 2.24129 (0.038 sec/batch, 1695.696 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:44,998] [train step48710] D loss: 0.32586 G loss: 2.33602 (0.035 sec/batch, 1821.173 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:45,397] [train step48720] D loss: 0.32569 G loss: 2.34652 (0.037 sec/batch, 1722.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:45,782] [train step48731] D loss: 0.32609 G loss: 2.32382 (0.042 sec/batch, 1523.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:46,176] [train step48741] D loss: 0.32560 G loss: 2.25404 (0.047 sec/batch, 1363.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:46,566] [train step48750] D loss: 0.33019 G loss: 2.06489 (0.037 sec/batch, 1743.472 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:46,954] [train step48760] D loss: 0.32766 G loss: 2.37800 (0.043 sec/batch, 1489.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:47,343] [train step48771] D loss: 0.32714 G loss: 2.17326 (0.044 sec/batch, 1438.384 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:47,718] [train step48780] D loss: 0.32571 G loss: 2.27317 (0.034 sec/batch, 1894.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:48,099] [train step48790] D loss: 0.32586 G loss: 2.35725 (0.034 sec/batch, 1906.732 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:48,497] [train step48800] D loss: 0.32663 G loss: 2.27509 (0.036 sec/batch, 1754.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:48,881] [train step48810] D loss: 0.32590 G loss: 2.36582 (0.037 sec/batch, 1738.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:49,259] [train step48821] D loss: 0.32599 G loss: 2.26046 (0.038 sec/batch, 1667.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:49,647] [train step48830] D loss: 0.32614 G loss: 2.31841 (0.027 sec/batch, 2344.578 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:50,037] [train step48840] D loss: 0.32551 G loss: 2.30182 (0.037 sec/batch, 1722.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:50,439] [train step48850] D loss: 0.32553 G loss: 2.29019 (0.052 sec/batch, 1225.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:50,828] [train step48861] D loss: 0.32616 G loss: 2.38723 (0.039 sec/batch, 1654.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:51,214] [train step48870] D loss: 0.32645 G loss: 2.31798 (0.042 sec/batch, 1523.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:51,629] [train step48881] D loss: 0.32617 G loss: 2.32141 (0.043 sec/batch, 1505.468 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:52,011] [train step48890] D loss: 0.32620 G loss: 2.30911 (0.038 sec/batch, 1665.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:52,396] [train step48900] D loss: 0.32724 G loss: 2.20878 (0.039 sec/batch, 1642.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:52,783] [train step48910] D loss: 0.32551 G loss: 2.31951 (0.035 sec/batch, 1810.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:53,165] [train step48921] D loss: 0.32649 G loss: 2.42314 (0.040 sec/batch, 1604.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:53,561] [train step48930] D loss: 0.32619 G loss: 2.37438 (0.041 sec/batch, 1558.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:53,939] [train step48941] D loss: 0.32583 G loss: 2.23913 (0.036 sec/batch, 1781.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:54,323] [train step48950] D loss: 0.32576 G loss: 2.24850 (0.042 sec/batch, 1535.285 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:54,736] [train step48960] D loss: 0.32571 G loss: 2.26051 (0.048 sec/batch, 1331.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:55,114] [train step48971] D loss: 0.32611 G loss: 2.25662 (0.034 sec/batch, 1868.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:55,514] [train step48980] D loss: 0.32642 G loss: 2.37421 (0.037 sec/batch, 1736.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:55,915] [train step48990] D loss: 0.32616 G loss: 2.26104 (0.036 sec/batch, 1798.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:56,298] [train step49000] D loss: 0.32586 G loss: 2.28333 (0.037 sec/batch, 1750.305 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:40:56,298] Saved checkpoint at 49000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:56,926] [train step49010] D loss: 0.32656 G loss: 2.38918 (0.039 sec/batch, 1643.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:57,309] [train step49020] D loss: 0.32674 G loss: 2.44014 (0.034 sec/batch, 1874.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:57,697] [train step49031] D loss: 0.32782 G loss: 2.18205 (0.035 sec/batch, 1825.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:58,081] [train step49041] D loss: 0.32905 G loss: 2.54784 (0.039 sec/batch, 1645.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:58,463] [train step49050] D loss: 0.32635 G loss: 2.40299 (0.036 sec/batch, 1763.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:58,853] [train step49061] D loss: 0.32638 G loss: 2.40313 (0.037 sec/batch, 1739.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:59,232] [train step49071] D loss: 0.32597 G loss: 2.30856 (0.035 sec/batch, 1834.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:40:59,621] [train step49080] D loss: 0.32606 G loss: 2.31042 (0.039 sec/batch, 1634.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:00,018] [train step49091] D loss: 0.32566 G loss: 2.28728 (0.034 sec/batch, 1868.781 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:00,405] [train step49101] D loss: 0.32616 G loss: 2.30215 (0.037 sec/batch, 1712.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:00,807] [train step49110] D loss: 0.32593 G loss: 2.32918 (0.039 sec/batch, 1629.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:01,187] [train step49120] D loss: 0.32564 G loss: 2.25999 (0.036 sec/batch, 1784.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:01,572] [train step49130] D loss: 0.32853 G loss: 2.53369 (0.042 sec/batch, 1511.657 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:01,960] [train step49140] D loss: 0.32654 G loss: 2.34487 (0.036 sec/batch, 1795.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:02,340] [train step49150] D loss: 0.32609 G loss: 2.20400 (0.038 sec/batch, 1698.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:02,714] [train step49160] D loss: 0.32600 G loss: 2.39275 (0.038 sec/batch, 1663.509 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:03,114] [train step49170] D loss: 0.32552 G loss: 2.28008 (0.037 sec/batch, 1736.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:03,507] [train step49180] D loss: 0.32593 G loss: 2.34845 (0.034 sec/batch, 1903.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:03,899] [train step49190] D loss: 0.32581 G loss: 2.35519 (0.037 sec/batch, 1709.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:04,283] [train step49200] D loss: 0.32591 G loss: 2.35817 (0.042 sec/batch, 1526.511 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:04,656] [train step49210] D loss: 0.32555 G loss: 2.26651 (0.033 sec/batch, 1930.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:05,055] [train step49221] D loss: 0.32631 G loss: 2.41485 (0.037 sec/batch, 1715.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:05,444] [train step49230] D loss: 0.32587 G loss: 2.35426 (0.036 sec/batch, 1795.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:05,814] [train step49240] D loss: 0.32768 G loss: 2.21185 (0.029 sec/batch, 2207.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:06,203] [train step49251] D loss: 0.32649 G loss: 2.36523 (0.038 sec/batch, 1662.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:06,567] [train step49260] D loss: 0.32584 G loss: 2.27158 (0.031 sec/batch, 2072.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:06,971] [train step49270] D loss: 0.32575 G loss: 2.25213 (0.040 sec/batch, 1582.738 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:07,348] [train step49280] D loss: 0.32576 G loss: 2.33322 (0.038 sec/batch, 1680.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:07,729] [train step49290] D loss: 0.32649 G loss: 2.33725 (0.041 sec/batch, 1576.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:08,124] [train step49301] D loss: 0.32596 G loss: 2.22667 (0.039 sec/batch, 1644.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:08,499] [train step49310] D loss: 0.32571 G loss: 2.32333 (0.037 sec/batch, 1721.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:08,879] [train step49320] D loss: 0.32629 G loss: 2.33594 (0.037 sec/batch, 1720.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:09,266] [train step49331] D loss: 0.32704 G loss: 2.26614 (0.034 sec/batch, 1865.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:09,645] [train step49341] D loss: 0.32569 G loss: 2.31652 (0.034 sec/batch, 1893.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:10,033] [train step49350] D loss: 0.32572 G loss: 2.34445 (0.042 sec/batch, 1519.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:10,418] [train step49360] D loss: 0.32626 G loss: 2.24047 (0.036 sec/batch, 1768.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:10,807] [train step49370] D loss: 0.32601 G loss: 2.38677 (0.039 sec/batch, 1659.078 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:11,197] [train step49380] D loss: 0.32576 G loss: 2.26207 (0.036 sec/batch, 1787.580 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:11,584] [train step49391] D loss: 0.32611 G loss: 2.34468 (0.041 sec/batch, 1567.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:11,964] [train step49401] D loss: 0.32564 G loss: 2.31659 (0.043 sec/batch, 1493.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:12,357] [train step49410] D loss: 0.32611 G loss: 2.31643 (0.035 sec/batch, 1814.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:12,736] [train step49421] D loss: 0.32689 G loss: 2.42259 (0.040 sec/batch, 1594.035 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:13,122] [train step49431] D loss: 0.32592 G loss: 2.22011 (0.037 sec/batch, 1724.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:13,514] [train step49440] D loss: 0.32593 G loss: 2.23681 (0.044 sec/batch, 1450.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:13,906] [train step49451] D loss: 0.32598 G loss: 2.31287 (0.040 sec/batch, 1615.174 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:14,300] [train step49460] D loss: 0.32742 G loss: 2.46731 (0.037 sec/batch, 1716.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:14,677] [train step49470] D loss: 0.32596 G loss: 2.32614 (0.033 sec/batch, 1939.128 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:15,068] [train step49480] D loss: 0.32577 G loss: 2.27487 (0.043 sec/batch, 1477.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:15,460] [train step49491] D loss: 0.32615 G loss: 2.31807 (0.039 sec/batch, 1638.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:15,844] [train step49500] D loss: 0.32564 G loss: 2.34195 (0.047 sec/batch, 1358.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:16,244] [train step49511] D loss: 0.32559 G loss: 2.32455 (0.035 sec/batch, 1823.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:16,631] [train step49520] D loss: 0.32569 G loss: 2.29411 (0.035 sec/batch, 1808.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:17,012] [train step49530] D loss: 0.32596 G loss: 2.35115 (0.037 sec/batch, 1729.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:17,404] [train step49540] D loss: 0.32745 G loss: 2.15533 (0.043 sec/batch, 1504.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:17,789] [train step49551] D loss: 0.32643 G loss: 2.36986 (0.041 sec/batch, 1550.960 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:18,182] [train step49560] D loss: 0.32572 G loss: 2.27732 (0.045 sec/batch, 1416.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:18,572] [train step49570] D loss: 0.32570 G loss: 2.35290 (0.038 sec/batch, 1691.678 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:18,960] [train step49581] D loss: 0.32595 G loss: 2.23038 (0.036 sec/batch, 1790.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:19,353] [train step49590] D loss: 0.32600 G loss: 2.35242 (0.040 sec/batch, 1590.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:19,751] [train step49600] D loss: 0.32584 G loss: 2.33670 (0.053 sec/batch, 1204.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:20,141] [train step49611] D loss: 0.32632 G loss: 2.20487 (0.035 sec/batch, 1816.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:20,542] [train step49620] D loss: 0.32608 G loss: 2.34561 (0.042 sec/batch, 1525.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:20,924] [train step49630] D loss: 0.32624 G loss: 2.31517 (0.032 sec/batch, 1971.210 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:21,321] [train step49640] D loss: 0.32587 G loss: 2.33615 (0.038 sec/batch, 1676.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:21,700] [train step49650] D loss: 0.32583 G loss: 2.29268 (0.037 sec/batch, 1709.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:22,071] [train step49661] D loss: 0.32605 G loss: 2.31718 (0.034 sec/batch, 1903.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:22,464] [train step49671] D loss: 0.32617 G loss: 2.30825 (0.041 sec/batch, 1577.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:22,846] [train step49680] D loss: 0.32556 G loss: 2.31788 (0.037 sec/batch, 1725.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:23,235] [train step49690] D loss: 0.32638 G loss: 2.23537 (0.035 sec/batch, 1818.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:23,636] [train step49701] D loss: 0.32592 G loss: 2.30367 (0.038 sec/batch, 1695.021 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:24,014] [train step49710] D loss: 0.32567 G loss: 2.29861 (0.043 sec/batch, 1498.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:24,409] [train step49721] D loss: 0.32605 G loss: 2.27302 (0.040 sec/batch, 1589.984 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:24,804] [train step49730] D loss: 0.32612 G loss: 2.35882 (0.037 sec/batch, 1708.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:25,190] [train step49740] D loss: 0.32603 G loss: 2.23782 (0.037 sec/batch, 1720.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:25,592] [train step49750] D loss: 0.32572 G loss: 2.29938 (0.040 sec/batch, 1599.782 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:25,974] [train step49761] D loss: 0.32558 G loss: 2.32659 (0.036 sec/batch, 1753.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:26,374] [train step49770] D loss: 0.32642 G loss: 2.39288 (0.046 sec/batch, 1380.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:26,766] [train step49781] D loss: 0.32756 G loss: 2.43971 (0.039 sec/batch, 1638.700 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:27,153] [train step49790] D loss: 0.32677 G loss: 2.18893 (0.039 sec/batch, 1647.157 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:27,547] [train step49800] D loss: 0.32569 G loss: 2.30355 (0.037 sec/batch, 1724.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:27,929] [train step49810] D loss: 0.32646 G loss: 2.23269 (0.035 sec/batch, 1824.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:28,316] [train step49820] D loss: 0.32576 G loss: 2.35270 (0.039 sec/batch, 1646.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:28,718] [train step49830] D loss: 0.32574 G loss: 2.31727 (0.036 sec/batch, 1759.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:29,103] [train step49840] D loss: 0.32563 G loss: 2.27523 (0.041 sec/batch, 1564.546 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:29,499] [train step49850] D loss: 0.32544 G loss: 2.33042 (0.050 sec/batch, 1280.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:29,884] [train step49860] D loss: 0.32559 G loss: 2.28274 (0.036 sec/batch, 1796.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:30,282] [train step49870] D loss: 0.32592 G loss: 2.33716 (0.038 sec/batch, 1682.411 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:30,676] [train step49880] D loss: 0.32554 G loss: 2.28083 (0.038 sec/batch, 1703.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:31,060] [train step49890] D loss: 0.32590 G loss: 2.29063 (0.036 sec/batch, 1757.329 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:31,461] [train step49900] D loss: 0.32564 G loss: 2.26807 (0.038 sec/batch, 1676.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:31,851] [train step49911] D loss: 0.32564 G loss: 2.33119 (0.037 sec/batch, 1746.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:32,231] [train step49920] D loss: 0.32545 G loss: 2.27564 (0.048 sec/batch, 1329.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:32,628] [train step49930] D loss: 0.32669 G loss: 2.41794 (0.041 sec/batch, 1579.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:33,008] [train step49940] D loss: 0.32653 G loss: 2.34718 (0.034 sec/batch, 1884.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:33,397] [train step49950] D loss: 0.32620 G loss: 2.31228 (0.036 sec/batch, 1802.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:33,791] [train step49961] D loss: 0.32602 G loss: 2.34258 (0.035 sec/batch, 1822.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:34,181] [train step49971] D loss: 0.32550 G loss: 2.28112 (0.035 sec/batch, 1829.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:34,560] [train step49980] D loss: 0.32558 G loss: 2.31524 (0.038 sec/batch, 1677.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:34,949] [train step49990] D loss: 0.32568 G loss: 2.25645 (0.039 sec/batch, 1635.784 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:35,317] [train step50001] D loss: 0.32556 G loss: 2.31991 (0.036 sec/batch, 1795.435 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:41:35,317] Saved checkpoint at 50000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:35,923] [train step50010] D loss: 0.32552 G loss: 2.28699 (0.035 sec/batch, 1817.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:36,294] [train step50020] D loss: 0.32684 G loss: 2.36323 (0.036 sec/batch, 1761.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:36,662] [train step50031] D loss: 0.32575 G loss: 2.34580 (0.038 sec/batch, 1681.568 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:37,060] [train step50040] D loss: 0.32563 G loss: 2.27572 (0.035 sec/batch, 1833.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:37,435] [train step50050] D loss: 0.32629 G loss: 2.38376 (0.032 sec/batch, 1971.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:37,817] [train step50060] D loss: 0.32567 G loss: 2.31561 (0.035 sec/batch, 1834.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:38,200] [train step50070] D loss: 0.32565 G loss: 2.31982 (0.036 sec/batch, 1790.656 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:38,577] [train step50080] D loss: 0.32602 G loss: 2.33021 (0.040 sec/batch, 1585.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:38,959] [train step50091] D loss: 0.32579 G loss: 2.35407 (0.036 sec/batch, 1780.418 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:39,331] [train step50100] D loss: 0.32573 G loss: 2.24370 (0.037 sec/batch, 1742.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:39,706] [train step50110] D loss: 0.32578 G loss: 2.33133 (0.035 sec/batch, 1823.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:40,097] [train step50121] D loss: 0.32763 G loss: 2.47908 (0.036 sec/batch, 1777.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:40,482] [train step50130] D loss: 0.32644 G loss: 2.19978 (0.040 sec/batch, 1580.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:40,866] [train step50140] D loss: 0.32583 G loss: 2.22948 (0.034 sec/batch, 1863.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:41,247] [train step50151] D loss: 0.32569 G loss: 2.29183 (0.043 sec/batch, 1504.455 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:41,627] [train step50160] D loss: 0.32616 G loss: 2.37057 (0.038 sec/batch, 1697.981 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:42,007] [train step50171] D loss: 0.32571 G loss: 2.27399 (0.036 sec/batch, 1790.692 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:42,397] [train step50181] D loss: 0.32577 G loss: 2.31139 (0.037 sec/batch, 1724.100 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:42,799] [train step50190] D loss: 0.32563 G loss: 2.30398 (0.038 sec/batch, 1686.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:43,189] [train step50200] D loss: 0.32548 G loss: 2.29263 (0.038 sec/batch, 1666.887 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:43,565] [train step50211] D loss: 0.32581 G loss: 2.32429 (0.036 sec/batch, 1777.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:43,958] [train step50220] D loss: 0.32661 G loss: 2.22832 (0.042 sec/batch, 1518.240 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:44,344] [train step50230] D loss: 0.32560 G loss: 2.27442 (0.043 sec/batch, 1505.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:44,727] [train step50240] D loss: 0.32591 G loss: 2.34583 (0.038 sec/batch, 1674.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:45,118] [train step50250] D loss: 0.32601 G loss: 2.30380 (0.037 sec/batch, 1734.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:45,509] [train step50261] D loss: 0.32616 G loss: 2.40472 (0.037 sec/batch, 1721.667 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:45,901] [train step50270] D loss: 0.32625 G loss: 2.21353 (0.042 sec/batch, 1519.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:46,294] [train step50280] D loss: 0.32562 G loss: 2.30838 (0.037 sec/batch, 1714.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:46,680] [train step50290] D loss: 0.32582 G loss: 2.25392 (0.040 sec/batch, 1608.698 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:47,074] [train step50301] D loss: 0.32550 G loss: 2.28798 (0.036 sec/batch, 1794.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:47,481] [train step50310] D loss: 0.32609 G loss: 2.38403 (0.048 sec/batch, 1329.962 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:47,873] [train step50320] D loss: 0.32561 G loss: 2.30388 (0.040 sec/batch, 1615.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:48,278] [train step50331] D loss: 0.32555 G loss: 2.27890 (0.038 sec/batch, 1694.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:48,660] [train step50340] D loss: 0.32613 G loss: 2.35759 (0.036 sec/batch, 1774.264 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:49,061] [train step50351] D loss: 0.32598 G loss: 2.36422 (0.042 sec/batch, 1517.425 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:49,442] [train step50361] D loss: 0.32562 G loss: 2.25560 (0.036 sec/batch, 1783.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:49,834] [train step50370] D loss: 0.32642 G loss: 2.41296 (0.049 sec/batch, 1309.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:50,218] [train step50381] D loss: 0.32573 G loss: 2.30245 (0.042 sec/batch, 1525.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:50,598] [train step50390] D loss: 0.32595 G loss: 2.25266 (0.037 sec/batch, 1744.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:50,992] [train step50400] D loss: 0.32573 G loss: 2.26741 (0.039 sec/batch, 1631.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:51,388] [train step50411] D loss: 0.32603 G loss: 2.33651 (0.037 sec/batch, 1741.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:51,772] [train step50420] D loss: 0.32649 G loss: 2.22931 (0.040 sec/batch, 1598.867 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:52,161] [train step50430] D loss: 0.32658 G loss: 2.42734 (0.038 sec/batch, 1666.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:52,540] [train step50440] D loss: 0.32614 G loss: 2.24539 (0.035 sec/batch, 1815.667 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:52,933] [train step50450] D loss: 0.32564 G loss: 2.32391 (0.040 sec/batch, 1603.327 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:53,323] [train step50460] D loss: 0.32613 G loss: 2.24057 (0.034 sec/batch, 1858.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:53,724] [train step50470] D loss: 0.32554 G loss: 2.32296 (0.036 sec/batch, 1760.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:54,106] [train step50480] D loss: 0.32592 G loss: 2.33999 (0.034 sec/batch, 1877.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:54,499] [train step50490] D loss: 0.32559 G loss: 2.33057 (0.040 sec/batch, 1596.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:54,887] [train step50500] D loss: 0.32566 G loss: 2.29294 (0.039 sec/batch, 1641.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:55,275] [train step50511] D loss: 0.32575 G loss: 2.22895 (0.033 sec/batch, 1928.249 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:55,659] [train step50520] D loss: 0.32588 G loss: 2.29682 (0.038 sec/batch, 1705.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:56,043] [train step50531] D loss: 0.32570 G loss: 2.28325 (0.043 sec/batch, 1479.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:56,440] [train step50541] D loss: 0.32591 G loss: 2.36676 (0.036 sec/batch, 1799.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:56,833] [train step50550] D loss: 0.32622 G loss: 2.38007 (0.037 sec/batch, 1721.733 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:57,229] [train step50561] D loss: 0.32618 G loss: 2.31610 (0.041 sec/batch, 1543.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:57,626] [train step50570] D loss: 0.32578 G loss: 2.36193 (0.040 sec/batch, 1593.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:58,021] [train step50580] D loss: 0.32553 G loss: 2.27623 (0.037 sec/batch, 1722.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:58,417] [train step50590] D loss: 0.32583 G loss: 2.37005 (0.038 sec/batch, 1671.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:58,802] [train step50601] D loss: 0.32602 G loss: 2.27538 (0.038 sec/batch, 1701.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:59,184] [train step50610] D loss: 0.32549 G loss: 2.33500 (0.040 sec/batch, 1593.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:59,580] [train step50620] D loss: 0.32560 G loss: 2.31917 (0.040 sec/batch, 1609.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:41:59,967] [train step50630] D loss: 0.32558 G loss: 2.34712 (0.041 sec/batch, 1554.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:00,368] [train step50640] D loss: 0.32582 G loss: 2.31232 (0.036 sec/batch, 1800.565 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:00,757] [train step50650] D loss: 0.32570 G loss: 2.36012 (0.037 sec/batch, 1752.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:01,155] [train step50661] D loss: 0.32544 G loss: 2.29367 (0.039 sec/batch, 1629.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:01,551] [train step50670] D loss: 0.32569 G loss: 2.24576 (0.040 sec/batch, 1593.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:01,931] [train step50681] D loss: 0.32595 G loss: 2.38411 (0.035 sec/batch, 1826.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:02,326] [train step50691] D loss: 0.32560 G loss: 2.30078 (0.034 sec/batch, 1894.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:02,719] [train step50700] D loss: 0.32744 G loss: 2.45339 (0.033 sec/batch, 1936.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:03,112] [train step50710] D loss: 0.32580 G loss: 2.24252 (0.039 sec/batch, 1636.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:03,503] [train step50720] D loss: 0.32588 G loss: 2.36940 (0.040 sec/batch, 1618.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:03,889] [train step50730] D loss: 0.32554 G loss: 2.26641 (0.042 sec/batch, 1508.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:04,271] [train step50741] D loss: 0.32598 G loss: 2.25581 (0.038 sec/batch, 1676.716 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:04,664] [train step50750] D loss: 0.32565 G loss: 2.30947 (0.039 sec/batch, 1658.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:05,039] [train step50760] D loss: 0.32568 G loss: 2.25517 (0.036 sec/batch, 1800.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:05,416] [train step50770] D loss: 0.32722 G loss: 2.45422 (0.040 sec/batch, 1614.766 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:05,807] [train step50780] D loss: 0.32574 G loss: 2.30130 (0.036 sec/batch, 1800.287 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:06,177] [train step50790] D loss: 0.32580 G loss: 2.31153 (0.040 sec/batch, 1609.904 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:06,558] [train step50800] D loss: 0.32602 G loss: 2.25110 (0.042 sec/batch, 1530.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:06,934] [train step50811] D loss: 0.32550 G loss: 2.29689 (0.039 sec/batch, 1653.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:07,309] [train step50820] D loss: 0.32553 G loss: 2.26012 (0.034 sec/batch, 1890.337 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:07,702] [train step50830] D loss: 0.32580 G loss: 2.28448 (0.043 sec/batch, 1493.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:08,087] [train step50841] D loss: 0.32553 G loss: 2.33325 (0.036 sec/batch, 1760.810 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:08,508] [train step50850] D loss: 0.32569 G loss: 2.26129 (0.038 sec/batch, 1680.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:08,918] [train step50860] D loss: 0.32572 G loss: 2.30510 (0.038 sec/batch, 1699.970 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:09,306] [train step50870] D loss: 0.32564 G loss: 2.26490 (0.048 sec/batch, 1322.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:09,701] [train step50880] D loss: 0.32556 G loss: 2.29370 (0.039 sec/batch, 1634.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:10,087] [train step50890] D loss: 0.32555 G loss: 2.29882 (0.039 sec/batch, 1643.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:10,486] [train step50901] D loss: 0.32598 G loss: 2.29912 (0.041 sec/batch, 1560.816 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:10,887] [train step50910] D loss: 0.32571 G loss: 2.34594 (0.043 sec/batch, 1487.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:11,278] [train step50920] D loss: 0.32589 G loss: 2.35662 (0.039 sec/batch, 1653.590 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:11,679] [train step50930] D loss: 0.32552 G loss: 2.30331 (0.046 sec/batch, 1390.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:12,075] [train step50940] D loss: 0.32600 G loss: 2.25558 (0.037 sec/batch, 1726.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:12,465] [train step50950] D loss: 0.32568 G loss: 2.25993 (0.040 sec/batch, 1584.176 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:12,868] [train step50961] D loss: 0.32599 G loss: 2.35687 (0.042 sec/batch, 1509.277 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:13,244] [train step50970] D loss: 0.32591 G loss: 2.28891 (0.034 sec/batch, 1881.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:13,635] [train step50981] D loss: 0.32580 G loss: 2.26441 (0.037 sec/batch, 1709.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:14,035] [train step50990] D loss: 0.32561 G loss: 2.32039 (0.038 sec/batch, 1706.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:14,424] [train step51000] D loss: 0.32578 G loss: 2.27853 (0.035 sec/batch, 1807.184 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:42:14,424] Saved checkpoint at 51000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:15,057] [train step51010] D loss: 0.32559 G loss: 2.30375 (0.040 sec/batch, 1614.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:15,452] [train step51020] D loss: 0.32559 G loss: 2.27600 (0.045 sec/batch, 1417.788 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:15,847] [train step51030] D loss: 0.32602 G loss: 2.37855 (0.039 sec/batch, 1655.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:16,240] [train step51040] D loss: 0.32577 G loss: 2.32252 (0.038 sec/batch, 1699.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:16,643] [train step51051] D loss: 0.32593 G loss: 2.38193 (0.041 sec/batch, 1573.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:17,036] [train step51060] D loss: 0.32544 G loss: 2.30077 (0.038 sec/batch, 1694.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:17,422] [train step51071] D loss: 0.32558 G loss: 2.32460 (0.033 sec/batch, 1913.692 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:17,831] [train step51080] D loss: 0.32581 G loss: 2.29168 (0.045 sec/batch, 1411.370 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:18,234] [train step51090] D loss: 0.32601 G loss: 2.37801 (0.040 sec/batch, 1587.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:18,635] [train step51101] D loss: 0.32584 G loss: 2.24230 (0.036 sec/batch, 1773.549 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:19,036] [train step51110] D loss: 0.32557 G loss: 2.31566 (0.040 sec/batch, 1600.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:19,443] [train step51120] D loss: 0.32561 G loss: 2.30339 (0.041 sec/batch, 1578.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:19,827] [train step51131] D loss: 0.32575 G loss: 2.33110 (0.035 sec/batch, 1826.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:20,216] [train step51140] D loss: 0.32566 G loss: 2.30469 (0.041 sec/batch, 1549.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:20,604] [train step51150] D loss: 0.32562 G loss: 2.26757 (0.042 sec/batch, 1525.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:20,995] [train step51161] D loss: 0.32553 G loss: 2.32102 (0.037 sec/batch, 1751.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:21,389] [train step51170] D loss: 0.32552 G loss: 2.33971 (0.043 sec/batch, 1480.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:21,765] [train step51180] D loss: 0.32547 G loss: 2.28913 (0.028 sec/batch, 2263.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:22,171] [train step51190] D loss: 0.32611 G loss: 2.40464 (0.040 sec/batch, 1619.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:22,561] [train step51200] D loss: 0.32554 G loss: 2.25915 (0.037 sec/batch, 1744.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:22,952] [train step51210] D loss: 0.32540 G loss: 2.30464 (0.039 sec/batch, 1633.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:23,344] [train step51221] D loss: 0.32582 G loss: 2.36687 (0.032 sec/batch, 2000.726 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:23,727] [train step51231] D loss: 0.32548 G loss: 2.29984 (0.035 sec/batch, 1830.512 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:24,122] [train step51240] D loss: 0.32614 G loss: 2.28782 (0.039 sec/batch, 1650.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:24,509] [train step51250] D loss: 0.32562 G loss: 2.29499 (0.040 sec/batch, 1614.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:24,894] [train step51260] D loss: 0.32560 G loss: 2.33478 (0.036 sec/batch, 1764.606 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:25,298] [train step51270] D loss: 0.32664 G loss: 2.38283 (0.037 sec/batch, 1724.454 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:25,680] [train step51281] D loss: 0.32679 G loss: 2.21384 (0.036 sec/batch, 1761.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:26,073] [train step51290] D loss: 0.32553 G loss: 2.32987 (0.052 sec/batch, 1219.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:26,457] [train step51300] D loss: 0.32579 G loss: 2.26029 (0.037 sec/batch, 1708.018 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:26,843] [train step51310] D loss: 0.32550 G loss: 2.32138 (0.031 sec/batch, 2045.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:27,245] [train step51320] D loss: 0.32554 G loss: 2.30490 (0.037 sec/batch, 1750.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:27,630] [train step51330] D loss: 0.32646 G loss: 2.39325 (0.035 sec/batch, 1834.014 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:28,022] [train step51341] D loss: 0.32557 G loss: 2.27549 (0.038 sec/batch, 1691.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:28,418] [train step51351] D loss: 0.32551 G loss: 2.31677 (0.035 sec/batch, 1849.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:28,793] [train step51360] D loss: 0.32563 G loss: 2.26859 (0.028 sec/batch, 2252.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:29,197] [train step51370] D loss: 0.32633 G loss: 2.19560 (0.038 sec/batch, 1676.025 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:29,579] [train step51380] D loss: 0.32548 G loss: 2.31545 (0.039 sec/batch, 1622.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:29,988] [train step51390] D loss: 0.32575 G loss: 2.23168 (0.037 sec/batch, 1715.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:30,381] [train step51400] D loss: 0.32557 G loss: 2.28973 (0.039 sec/batch, 1659.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:30,770] [train step51410] D loss: 0.32558 G loss: 2.28886 (0.038 sec/batch, 1686.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:31,161] [train step51420] D loss: 0.32559 G loss: 2.28245 (0.041 sec/batch, 1567.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:31,569] [train step51430] D loss: 0.32558 G loss: 2.25644 (0.042 sec/batch, 1525.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:31,959] [train step51440] D loss: 0.32583 G loss: 2.33673 (0.037 sec/batch, 1731.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:32,354] [train step51450] D loss: 0.32581 G loss: 2.25841 (0.043 sec/batch, 1476.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:32,735] [train step51461] D loss: 0.32566 G loss: 2.28853 (0.036 sec/batch, 1781.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:33,127] [train step51470] D loss: 0.32557 G loss: 2.34094 (0.039 sec/batch, 1651.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:33,517] [train step51480] D loss: 0.32557 G loss: 2.34041 (0.039 sec/batch, 1644.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:33,903] [train step51491] D loss: 0.32547 G loss: 2.30788 (0.038 sec/batch, 1706.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:34,288] [train step51500] D loss: 0.32583 G loss: 2.24345 (0.045 sec/batch, 1434.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:34,693] [train step51510] D loss: 0.32560 G loss: 2.32353 (0.037 sec/batch, 1751.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:35,083] [train step51521] D loss: 0.32543 G loss: 2.32196 (0.036 sec/batch, 1763.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:35,480] [train step51530] D loss: 0.32542 G loss: 2.30993 (0.040 sec/batch, 1614.533 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:35,853] [train step51540] D loss: 0.32587 G loss: 2.33941 (0.036 sec/batch, 1775.790 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:36,228] [train step51551] D loss: 0.32557 G loss: 2.27097 (0.035 sec/batch, 1824.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:36,612] [train step51561] D loss: 0.32567 G loss: 2.36666 (0.044 sec/batch, 1454.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:36,992] [train step51570] D loss: 0.32554 G loss: 2.32044 (0.045 sec/batch, 1408.208 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:37,370] [train step51581] D loss: 0.32569 G loss: 2.33842 (0.041 sec/batch, 1558.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:37,749] [train step51591] D loss: 0.32568 G loss: 2.29974 (0.036 sec/batch, 1798.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:38,139] [train step51600] D loss: 0.32636 G loss: 2.34385 (0.039 sec/batch, 1634.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:38,522] [train step51611] D loss: 0.32565 G loss: 2.28501 (0.036 sec/batch, 1792.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:38,895] [train step51620] D loss: 0.32579 G loss: 2.24048 (0.036 sec/batch, 1758.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:39,270] [train step51630] D loss: 0.32569 G loss: 2.32283 (0.039 sec/batch, 1653.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:39,656] [train step51640] D loss: 0.32635 G loss: 2.42760 (0.041 sec/batch, 1552.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:40,043] [train step51651] D loss: 0.32562 G loss: 2.23903 (0.032 sec/batch, 1991.228 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:40,428] [train step51660] D loss: 0.32627 G loss: 2.42086 (0.043 sec/batch, 1505.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:40,839] [train step51671] D loss: 0.32580 G loss: 2.32148 (0.035 sec/batch, 1850.807 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:41,216] [train step51681] D loss: 0.32593 G loss: 2.25706 (0.035 sec/batch, 1806.041 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:41,605] [train step51690] D loss: 0.32551 G loss: 2.31149 (0.036 sec/batch, 1758.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:41,985] [train step51700] D loss: 0.32554 G loss: 2.27939 (0.039 sec/batch, 1639.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:42,357] [train step51711] D loss: 0.32594 G loss: 2.38295 (0.036 sec/batch, 1798.249 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:42,751] [train step51720] D loss: 0.32565 G loss: 2.31540 (0.036 sec/batch, 1771.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:43,133] [train step51730] D loss: 0.32715 G loss: 2.47251 (0.036 sec/batch, 1784.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:43,517] [train step51741] D loss: 0.32544 G loss: 2.26301 (0.039 sec/batch, 1623.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:43,906] [train step51750] D loss: 0.32539 G loss: 2.30372 (0.041 sec/batch, 1553.536 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:44,312] [train step51761] D loss: 0.32587 G loss: 2.37127 (0.036 sec/batch, 1759.875 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:44,698] [train step51770] D loss: 0.32550 G loss: 2.24894 (0.035 sec/batch, 1848.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:45,086] [train step51780] D loss: 0.32581 G loss: 2.29827 (0.043 sec/batch, 1495.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:45,477] [train step51790] D loss: 0.32776 G loss: 2.12640 (0.040 sec/batch, 1585.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:45,877] [train step51800] D loss: 0.32596 G loss: 2.38840 (0.036 sec/batch, 1800.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:46,263] [train step51810] D loss: 0.32579 G loss: 2.26476 (0.036 sec/batch, 1796.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:46,657] [train step51820] D loss: 0.32573 G loss: 2.37362 (0.049 sec/batch, 1293.908 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:47,045] [train step51831] D loss: 0.32603 G loss: 2.34631 (0.035 sec/batch, 1806.418 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:47,432] [train step51840] D loss: 0.32565 G loss: 2.35342 (0.039 sec/batch, 1661.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:47,828] [train step51851] D loss: 0.32587 G loss: 2.21974 (0.037 sec/batch, 1741.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:48,225] [train step51861] D loss: 0.32567 G loss: 2.25120 (0.045 sec/batch, 1414.903 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:48,602] [train step51870] D loss: 0.32667 G loss: 2.18543 (0.037 sec/batch, 1726.884 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:48,999] [train step51881] D loss: 0.32579 G loss: 2.37626 (0.037 sec/batch, 1713.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:49,381] [train step51890] D loss: 0.32565 G loss: 2.33336 (0.037 sec/batch, 1710.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:49,779] [train step51900] D loss: 0.32569 G loss: 2.35667 (0.040 sec/batch, 1591.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:50,167] [train step51911] D loss: 0.32548 G loss: 2.28826 (0.037 sec/batch, 1750.065 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:50,553] [train step51921] D loss: 0.32597 G loss: 2.23071 (0.040 sec/batch, 1612.554 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:50,942] [train step51930] D loss: 0.32549 G loss: 2.34056 (0.034 sec/batch, 1859.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:51,343] [train step51940] D loss: 0.32572 G loss: 2.33603 (0.056 sec/batch, 1143.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:51,742] [train step51950] D loss: 0.32590 G loss: 2.26462 (0.042 sec/batch, 1522.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:52,142] [train step51960] D loss: 0.32544 G loss: 2.30171 (0.037 sec/batch, 1746.944 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:52,525] [train step51971] D loss: 0.32580 G loss: 2.37026 (0.037 sec/batch, 1749.666 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:52,919] [train step51981] D loss: 0.32563 G loss: 2.25762 (0.037 sec/batch, 1734.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:53,303] [train step51990] D loss: 0.32585 G loss: 2.38123 (0.042 sec/batch, 1515.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:53,690] [train step52000] D loss: 0.32559 G loss: 2.25482 (0.045 sec/batch, 1430.450 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:42:53,691] Saved checkpoint at 52000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:54,311] [train step52010] D loss: 0.32565 G loss: 2.33242 (0.041 sec/batch, 1564.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:54,688] [train step52020] D loss: 0.32551 G loss: 2.31877 (0.037 sec/batch, 1741.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:55,087] [train step52030] D loss: 0.32554 G loss: 2.28032 (0.040 sec/batch, 1594.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:55,467] [train step52040] D loss: 0.32551 G loss: 2.27516 (0.039 sec/batch, 1656.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:55,840] [train step52050] D loss: 0.32566 G loss: 2.28375 (0.029 sec/batch, 2181.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:56,241] [train step52061] D loss: 0.32564 G loss: 2.23640 (0.037 sec/batch, 1747.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:56,630] [train step52070] D loss: 0.32552 G loss: 2.32467 (0.042 sec/batch, 1541.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:57,021] [train step52080] D loss: 0.32563 G loss: 2.37158 (0.043 sec/batch, 1476.543 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:57,412] [train step52091] D loss: 0.32589 G loss: 2.37725 (0.035 sec/batch, 1853.298 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:57,794] [train step52100] D loss: 0.32551 G loss: 2.29423 (0.040 sec/batch, 1609.509 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:58,181] [train step52110] D loss: 0.32578 G loss: 2.34978 (0.038 sec/batch, 1686.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:58,559] [train step52120] D loss: 0.32555 G loss: 2.28552 (0.035 sec/batch, 1811.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:58,960] [train step52130] D loss: 0.32580 G loss: 2.37402 (0.049 sec/batch, 1297.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:59,348] [train step52140] D loss: 0.32544 G loss: 2.27928 (0.038 sec/batch, 1692.766 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:42:59,735] [train step52151] D loss: 0.32608 G loss: 2.41032 (0.040 sec/batch, 1591.983 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:00,127] [train step52161] D loss: 0.32562 G loss: 2.30100 (0.037 sec/batch, 1712.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:00,534] [train step52170] D loss: 0.32551 G loss: 2.33587 (0.037 sec/batch, 1708.398 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:00,912] [train step52180] D loss: 0.32549 G loss: 2.30958 (0.033 sec/batch, 1921.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:01,307] [train step52191] D loss: 0.32550 G loss: 2.30008 (0.036 sec/batch, 1799.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:01,701] [train step52200] D loss: 0.32544 G loss: 2.29595 (0.036 sec/batch, 1781.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:02,109] [train step52211] D loss: 0.32656 G loss: 2.43383 (0.043 sec/batch, 1496.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:02,490] [train step52221] D loss: 0.32548 G loss: 2.26770 (0.039 sec/batch, 1626.832 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:02,882] [train step52230] D loss: 0.32595 G loss: 2.28765 (0.041 sec/batch, 1572.492 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:03,283] [train step52240] D loss: 0.32576 G loss: 2.25993 (0.037 sec/batch, 1710.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:03,670] [train step52251] D loss: 0.32613 G loss: 2.40476 (0.041 sec/batch, 1574.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:04,057] [train step52260] D loss: 0.32550 G loss: 2.28122 (0.035 sec/batch, 1817.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:04,451] [train step52271] D loss: 0.32591 G loss: 2.21526 (0.038 sec/batch, 1668.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:04,838] [train step52280] D loss: 0.32667 G loss: 2.44421 (0.038 sec/batch, 1688.199 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:05,222] [train step52290] D loss: 0.32574 G loss: 2.31791 (0.041 sec/batch, 1571.609 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:05,611] [train step52300] D loss: 0.32552 G loss: 2.31508 (0.041 sec/batch, 1572.621 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:05,992] [train step52311] D loss: 0.32579 G loss: 2.21788 (0.034 sec/batch, 1888.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:06,371] [train step52320] D loss: 0.32543 G loss: 2.28074 (0.036 sec/batch, 1785.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:06,744] [train step52331] D loss: 0.32551 G loss: 2.34900 (0.035 sec/batch, 1820.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:07,119] [train step52340] D loss: 0.32632 G loss: 2.19951 (0.038 sec/batch, 1697.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:07,505] [train step52350] D loss: 0.32632 G loss: 2.37508 (0.037 sec/batch, 1749.677 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:07,879] [train step52360] D loss: 0.32542 G loss: 2.31377 (0.035 sec/batch, 1823.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:08,262] [train step52370] D loss: 0.32564 G loss: 2.33162 (0.034 sec/batch, 1909.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:08,639] [train step52380] D loss: 0.32613 G loss: 2.22401 (0.038 sec/batch, 1697.809 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:09,014] [train step52391] D loss: 0.32629 G loss: 2.37100 (0.038 sec/batch, 1674.133 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:09,394] [train step52401] D loss: 0.32571 G loss: 2.32639 (0.038 sec/batch, 1685.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:09,772] [train step52410] D loss: 0.32610 G loss: 2.37885 (0.040 sec/batch, 1612.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:10,142] [train step52421] D loss: 0.32564 G loss: 2.27195 (0.036 sec/batch, 1755.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:10,544] [train step52431] D loss: 0.32555 G loss: 2.36054 (0.034 sec/batch, 1881.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:10,925] [train step52440] D loss: 0.32587 G loss: 2.22997 (0.034 sec/batch, 1893.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:11,307] [train step52451] D loss: 0.32628 G loss: 2.34509 (0.043 sec/batch, 1495.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:11,680] [train step52460] D loss: 0.32545 G loss: 2.28007 (0.038 sec/batch, 1681.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:12,049] [train step52470] D loss: 0.32707 G loss: 2.19175 (0.037 sec/batch, 1740.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:12,434] [train step52480] D loss: 0.32635 G loss: 2.35056 (0.038 sec/batch, 1690.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:12,832] [train step52490] D loss: 0.32581 G loss: 2.36795 (0.034 sec/batch, 1907.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:13,219] [train step52500] D loss: 0.32542 G loss: 2.26023 (0.036 sec/batch, 1769.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:13,605] [train step52511] D loss: 0.32617 G loss: 2.38350 (0.038 sec/batch, 1701.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:13,980] [train step52521] D loss: 0.32539 G loss: 2.28274 (0.036 sec/batch, 1795.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:14,359] [train step52530] D loss: 0.32559 G loss: 2.32782 (0.038 sec/batch, 1689.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:14,750] [train step52540] D loss: 0.32574 G loss: 2.36407 (0.045 sec/batch, 1413.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:15,133] [train step52551] D loss: 0.32661 G loss: 2.24212 (0.038 sec/batch, 1684.703 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:15,519] [train step52560] D loss: 0.32565 G loss: 2.31544 (0.037 sec/batch, 1732.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:15,916] [train step52570] D loss: 0.32944 G loss: 2.49921 (0.039 sec/batch, 1661.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:16,299] [train step52580] D loss: 0.32731 G loss: 2.17092 (0.038 sec/batch, 1685.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:16,691] [train step52590] D loss: 0.32629 G loss: 2.35559 (0.036 sec/batch, 1757.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:17,066] [train step52601] D loss: 0.32577 G loss: 2.28439 (0.038 sec/batch, 1691.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:17,470] [train step52610] D loss: 0.32604 G loss: 2.40080 (0.050 sec/batch, 1276.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:17,858] [train step52620] D loss: 0.32560 G loss: 2.30107 (0.039 sec/batch, 1651.585 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:18,247] [train step52631] D loss: 0.32542 G loss: 2.28762 (0.039 sec/batch, 1631.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:18,651] [train step52641] D loss: 0.32574 G loss: 2.31917 (0.044 sec/batch, 1467.967 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:19,036] [train step52650] D loss: 0.32567 G loss: 2.29277 (0.036 sec/batch, 1767.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:19,425] [train step52661] D loss: 0.32626 G loss: 2.25821 (0.042 sec/batch, 1539.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:19,829] [train step52671] D loss: 0.32549 G loss: 2.34403 (0.037 sec/batch, 1717.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:20,220] [train step52680] D loss: 0.32585 G loss: 2.24778 (0.042 sec/batch, 1508.268 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:20,622] [train step52691] D loss: 0.32547 G loss: 2.33382 (0.043 sec/batch, 1484.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:21,008] [train step52701] D loss: 0.32545 G loss: 2.28805 (0.038 sec/batch, 1704.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:21,396] [train step52710] D loss: 0.32565 G loss: 2.29781 (0.042 sec/batch, 1514.574 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:21,800] [train step52721] D loss: 0.32557 G loss: 2.35578 (0.040 sec/batch, 1583.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:22,186] [train step52730] D loss: 0.32545 G loss: 2.31521 (0.038 sec/batch, 1666.970 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:22,568] [train step52740] D loss: 0.32584 G loss: 2.30929 (0.042 sec/batch, 1517.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:22,966] [train step52751] D loss: 0.32556 G loss: 2.26461 (0.037 sec/batch, 1749.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:23,371] [train step52761] D loss: 0.32552 G loss: 2.28530 (0.037 sec/batch, 1728.040 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:23,765] [train step52770] D loss: 0.32581 G loss: 2.24060 (0.036 sec/batch, 1788.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:24,155] [train step52780] D loss: 0.32552 G loss: 2.30048 (0.042 sec/batch, 1529.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:24,540] [train step52791] D loss: 0.32562 G loss: 2.32145 (0.035 sec/batch, 1850.692 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:24,942] [train step52800] D loss: 0.32613 G loss: 2.38908 (0.036 sec/batch, 1767.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:25,346] [train step52811] D loss: 0.32547 G loss: 2.30100 (0.037 sec/batch, 1723.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:25,744] [train step52821] D loss: 0.32546 G loss: 2.27586 (0.041 sec/batch, 1548.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:26,133] [train step52830] D loss: 0.32544 G loss: 2.29315 (0.040 sec/batch, 1591.285 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:26,514] [train step52841] D loss: 0.33247 G loss: 2.04539 (0.039 sec/batch, 1647.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:26,905] [train step52851] D loss: 0.32818 G loss: 2.44977 (0.038 sec/batch, 1675.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:27,290] [train step52860] D loss: 0.32602 G loss: 2.22568 (0.041 sec/batch, 1544.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:27,675] [train step52871] D loss: 0.32593 G loss: 2.35575 (0.053 sec/batch, 1201.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:28,080] [train step52880] D loss: 0.32576 G loss: 2.26854 (0.038 sec/batch, 1668.856 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:28,466] [train step52890] D loss: 0.32550 G loss: 2.32083 (0.034 sec/batch, 1888.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:28,856] [train step52901] D loss: 0.32551 G loss: 2.29899 (0.038 sec/batch, 1665.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:29,242] [train step52911] D loss: 0.32538 G loss: 2.30005 (0.040 sec/batch, 1595.418 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:29,637] [train step52920] D loss: 0.32613 G loss: 2.22069 (0.044 sec/batch, 1461.884 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:30,033] [train step52930] D loss: 0.32552 G loss: 2.32366 (0.039 sec/batch, 1656.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:30,426] [train step52940] D loss: 0.32544 G loss: 2.28461 (0.043 sec/batch, 1489.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:30,813] [train step52950] D loss: 0.32559 G loss: 2.29904 (0.036 sec/batch, 1788.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:31,215] [train step52960] D loss: 0.32553 G loss: 2.27228 (0.037 sec/batch, 1729.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:31,607] [train step52970] D loss: 0.32636 G loss: 2.42596 (0.041 sec/batch, 1579.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:31,993] [train step52980] D loss: 0.32554 G loss: 2.30980 (0.026 sec/batch, 2423.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:32,389] [train step52990] D loss: 0.32554 G loss: 2.31334 (0.039 sec/batch, 1651.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:32,778] [train step53001] D loss: 0.32560 G loss: 2.27655 (0.032 sec/batch, 2025.531 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:43:32,778] Saved checkpoint at 53000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:33,395] [train step53010] D loss: 0.32566 G loss: 2.26608 (0.037 sec/batch, 1723.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:33,798] [train step53021] D loss: 0.32556 G loss: 2.28548 (0.039 sec/batch, 1646.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:34,201] [train step53030] D loss: 0.32595 G loss: 2.35439 (0.035 sec/batch, 1813.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:34,588] [train step53040] D loss: 0.32596 G loss: 2.25949 (0.040 sec/batch, 1616.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:34,982] [train step53051] D loss: 0.32618 G loss: 2.36832 (0.037 sec/batch, 1716.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:35,385] [train step53061] D loss: 0.32568 G loss: 2.31902 (0.038 sec/batch, 1695.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:35,766] [train step53070] D loss: 0.32599 G loss: 2.35419 (0.038 sec/batch, 1697.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:36,155] [train step53080] D loss: 0.32560 G loss: 2.31743 (0.033 sec/batch, 1956.085 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:36,548] [train step53090] D loss: 0.32565 G loss: 2.30273 (0.039 sec/batch, 1624.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:36,926] [train step53100] D loss: 0.32573 G loss: 2.32034 (0.033 sec/batch, 1946.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:37,310] [train step53111] D loss: 0.32578 G loss: 2.26854 (0.039 sec/batch, 1657.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:37,679] [train step53121] D loss: 0.32540 G loss: 2.30988 (0.041 sec/batch, 1578.809 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:38,063] [train step53130] D loss: 0.32571 G loss: 2.26058 (0.038 sec/batch, 1696.950 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:38,441] [train step53140] D loss: 0.32560 G loss: 2.27895 (0.039 sec/batch, 1655.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:38,823] [train step53150] D loss: 0.32559 G loss: 2.31210 (0.040 sec/batch, 1582.215 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:39,216] [train step53160] D loss: 0.32562 G loss: 2.26112 (0.035 sec/batch, 1831.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:39,592] [train step53170] D loss: 0.32549 G loss: 2.33918 (0.037 sec/batch, 1741.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:39,979] [train step53181] D loss: 0.32562 G loss: 2.25535 (0.040 sec/batch, 1595.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:40,368] [train step53190] D loss: 0.32547 G loss: 2.28174 (0.038 sec/batch, 1676.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:40,745] [train step53201] D loss: 0.32567 G loss: 2.33111 (0.035 sec/batch, 1803.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:41,132] [train step53211] D loss: 0.32573 G loss: 2.28902 (0.041 sec/batch, 1576.112 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:41,517] [train step53220] D loss: 0.32601 G loss: 2.37529 (0.038 sec/batch, 1690.283 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:41,889] [train step53230] D loss: 0.32573 G loss: 2.25761 (0.035 sec/batch, 1827.409 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:42,282] [train step53241] D loss: 0.32582 G loss: 2.28878 (0.042 sec/batch, 1517.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:42,689] [train step53250] D loss: 0.32567 G loss: 2.30179 (0.035 sec/batch, 1811.440 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:43,083] [train step53261] D loss: 0.32570 G loss: 2.33105 (0.039 sec/batch, 1654.089 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:43,474] [train step53271] D loss: 0.32560 G loss: 2.29085 (0.037 sec/batch, 1712.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:43,879] [train step53280] D loss: 0.32776 G loss: 2.17455 (0.049 sec/batch, 1310.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:44,287] [train step53291] D loss: 0.32634 G loss: 2.35681 (0.038 sec/batch, 1706.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:44,672] [train step53300] D loss: 0.32561 G loss: 2.35070 (0.037 sec/batch, 1741.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:45,064] [train step53310] D loss: 0.32704 G loss: 2.40601 (0.037 sec/batch, 1721.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:45,464] [train step53321] D loss: 0.32575 G loss: 2.32046 (0.039 sec/batch, 1660.289 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:45,862] [train step53331] D loss: 0.32545 G loss: 2.28865 (0.038 sec/batch, 1677.407 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:46,258] [train step53340] D loss: 0.32560 G loss: 2.34884 (0.043 sec/batch, 1504.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:46,651] [train step53351] D loss: 0.32641 G loss: 2.41900 (0.039 sec/batch, 1657.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:47,052] [train step53361] D loss: 0.32556 G loss: 2.23980 (0.040 sec/batch, 1589.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:47,448] [train step53370] D loss: 0.32554 G loss: 2.26567 (0.040 sec/batch, 1582.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:47,832] [train step53381] D loss: 0.32580 G loss: 2.28846 (0.034 sec/batch, 1891.230 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:48,226] [train step53391] D loss: 0.32560 G loss: 2.24098 (0.037 sec/batch, 1738.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:48,611] [train step53400] D loss: 0.32552 G loss: 2.27275 (0.035 sec/batch, 1847.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:49,010] [train step53410] D loss: 0.32591 G loss: 2.36783 (0.040 sec/batch, 1587.690 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:49,410] [train step53420] D loss: 0.32642 G loss: 2.20307 (0.041 sec/batch, 1551.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:49,800] [train step53430] D loss: 0.32545 G loss: 2.32328 (0.037 sec/batch, 1720.972 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:50,185] [train step53440] D loss: 0.32544 G loss: 2.28161 (0.036 sec/batch, 1769.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:50,577] [train step53450] D loss: 0.33012 G loss: 2.57802 (0.040 sec/batch, 1602.016 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:50,965] [train step53460] D loss: 0.32646 G loss: 2.33852 (0.034 sec/batch, 1909.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:51,362] [train step53470] D loss: 0.32648 G loss: 2.42535 (0.039 sec/batch, 1620.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:51,756] [train step53481] D loss: 0.32620 G loss: 2.21502 (0.037 sec/batch, 1749.096 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:52,149] [train step53490] D loss: 3.80999 G loss: 0.01754 (0.045 sec/batch, 1430.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:52,550] [train step53501] D loss: 0.73245 G loss: 2.48289 (0.044 sec/batch, 1454.887 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:52,934] [train step53511] D loss: 0.39815 G loss: 2.79881 (0.036 sec/batch, 1754.457 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:53,315] [train step53520] D loss: 0.45070 G loss: 3.10317 (0.038 sec/batch, 1704.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:53,699] [train step53530] D loss: 0.46662 G loss: 3.01555 (0.035 sec/batch, 1826.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:54,087] [train step53541] D loss: 0.41214 G loss: 3.10666 (0.036 sec/batch, 1772.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:54,480] [train step53550] D loss: 0.51175 G loss: 1.53429 (0.044 sec/batch, 1465.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:54,880] [train step53560] D loss: 0.37133 G loss: 1.98818 (0.039 sec/batch, 1643.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:55,271] [train step53571] D loss: 0.45489 G loss: 4.02695 (0.040 sec/batch, 1611.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:55,669] [train step53580] D loss: 0.52106 G loss: 1.27510 (0.034 sec/batch, 1877.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:56,059] [train step53591] D loss: 0.35874 G loss: 2.69517 (0.037 sec/batch, 1719.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:56,442] [train step53600] D loss: 0.57695 G loss: 5.48673 (0.043 sec/batch, 1501.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:56,842] [train step53610] D loss: 0.35747 G loss: 2.58987 (0.035 sec/batch, 1814.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:57,228] [train step53621] D loss: 0.99210 G loss: 6.17667 (0.034 sec/batch, 1857.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:57,629] [train step53631] D loss: 1.52862 G loss: 10.05725 (0.038 sec/batch, 1684.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:58,024] [train step53640] D loss: 0.51888 G loss: 3.93476 (0.042 sec/batch, 1530.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:58,405] [train step53650] D loss: 1.25971 G loss: 9.34317 (0.038 sec/batch, 1668.005 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:58,797] [train step53661] D loss: 0.49931 G loss: 1.45596 (0.036 sec/batch, 1780.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:59,184] [train step53670] D loss: 0.54452 G loss: 5.17797 (0.036 sec/batch, 1778.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:59,563] [train step53681] D loss: 0.36736 G loss: 2.66814 (0.035 sec/batch, 1831.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:43:59,954] [train step53691] D loss: 0.53188 G loss: 4.97731 (0.036 sec/batch, 1791.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:00,349] [train step53700] D loss: 0.41015 G loss: 3.57730 (0.041 sec/batch, 1568.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:00,742] [train step53711] D loss: 0.64422 G loss: 6.17081 (0.041 sec/batch, 1547.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:01,137] [train step53720] D loss: 0.40976 G loss: 3.79979 (0.037 sec/batch, 1712.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:01,519] [train step53730] D loss: 0.41505 G loss: 1.53584 (0.035 sec/batch, 1817.080 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:01,913] [train step53740] D loss: 0.36116 G loss: 3.11502 (0.031 sec/batch, 2037.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:02,305] [train step53750] D loss: 0.34049 G loss: 2.38311 (0.037 sec/batch, 1720.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:02,703] [train step53760] D loss: 0.34162 G loss: 2.43350 (0.042 sec/batch, 1527.788 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:03,095] [train step53770] D loss: 0.34267 G loss: 2.43093 (0.043 sec/batch, 1490.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:03,479] [train step53781] D loss: 0.35853 G loss: 3.06465 (0.037 sec/batch, 1735.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:03,878] [train step53790] D loss: 0.35148 G loss: 2.83920 (0.042 sec/batch, 1524.543 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:04,263] [train step53800] D loss: 0.35228 G loss: 2.10254 (0.041 sec/batch, 1573.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:04,652] [train step53810] D loss: 0.34231 G loss: 2.54315 (0.050 sec/batch, 1279.080 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:05,048] [train step53820] D loss: 0.33852 G loss: 2.43383 (0.038 sec/batch, 1688.688 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:05,452] [train step53831] D loss: 0.34489 G loss: 2.47184 (0.036 sec/batch, 1762.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:05,843] [train step53841] D loss: 0.34869 G loss: 2.45760 (0.037 sec/batch, 1712.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:06,229] [train step53850] D loss: 0.35242 G loss: 2.16836 (0.037 sec/batch, 1739.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:06,616] [train step53860] D loss: 0.34727 G loss: 2.72488 (0.038 sec/batch, 1701.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:07,008] [train step53870] D loss: 0.33946 G loss: 2.29450 (0.034 sec/batch, 1861.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:07,391] [train step53880] D loss: 0.34107 G loss: 2.25268 (0.044 sec/batch, 1444.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:07,775] [train step53891] D loss: 0.34321 G loss: 2.39564 (0.038 sec/batch, 1692.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:08,165] [train step53900] D loss: 0.34126 G loss: 2.52206 (0.039 sec/batch, 1641.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:08,540] [train step53910] D loss: 0.33908 G loss: 2.37789 (0.038 sec/batch, 1697.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:08,920] [train step53920] D loss: 0.34418 G loss: 2.20596 (0.037 sec/batch, 1723.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:09,300] [train step53930] D loss: 0.33909 G loss: 2.56970 (0.037 sec/batch, 1712.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:09,676] [train step53940] D loss: 0.34115 G loss: 2.30473 (0.042 sec/batch, 1539.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:10,064] [train step53950] D loss: 0.34146 G loss: 2.54623 (0.039 sec/batch, 1629.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:10,442] [train step53961] D loss: 0.33864 G loss: 2.37941 (0.036 sec/batch, 1790.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:10,826] [train step53970] D loss: 0.33902 G loss: 2.59747 (0.036 sec/batch, 1795.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:11,206] [train step53980] D loss: 0.34429 G loss: 2.19902 (0.037 sec/batch, 1748.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:11,576] [train step53991] D loss: 0.33990 G loss: 2.49514 (0.035 sec/batch, 1818.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:11,961] [train step54000] D loss: 0.34166 G loss: 2.25508 (0.049 sec/batch, 1295.669 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:44:11,961] Saved checkpoint at 54000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:12,551] [train step54011] D loss: 0.33781 G loss: 2.43559 (0.034 sec/batch, 1891.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:12,936] [train step54020] D loss: 0.33814 G loss: 2.30405 (0.036 sec/batch, 1781.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:13,329] [train step54030] D loss: 0.33676 G loss: 2.51495 (0.041 sec/batch, 1569.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:13,718] [train step54041] D loss: 0.34354 G loss: 2.63239 (0.036 sec/batch, 1788.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:14,102] [train step54051] D loss: 0.33914 G loss: 2.30639 (0.036 sec/batch, 1781.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:14,474] [train step54060] D loss: 0.33713 G loss: 2.39899 (0.037 sec/batch, 1744.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:14,861] [train step54071] D loss: 0.33664 G loss: 2.23275 (0.039 sec/batch, 1655.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:15,253] [train step54080] D loss: 0.33481 G loss: 2.32201 (0.038 sec/batch, 1681.621 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:15,646] [train step54090] D loss: 0.33601 G loss: 2.31205 (0.047 sec/batch, 1355.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:16,039] [train step54100] D loss: 0.33672 G loss: 2.47406 (0.043 sec/batch, 1475.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:16,426] [train step54111] D loss: 0.33732 G loss: 2.44648 (0.037 sec/batch, 1733.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:16,804] [train step54120] D loss: 0.33731 G loss: 2.42149 (0.036 sec/batch, 1798.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:17,200] [train step54130] D loss: 0.33904 G loss: 2.55553 (0.041 sec/batch, 1562.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:17,584] [train step54140] D loss: 0.33740 G loss: 2.29946 (0.047 sec/batch, 1373.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:17,961] [train step54150] D loss: 0.33651 G loss: 2.36493 (0.039 sec/batch, 1658.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:18,348] [train step54161] D loss: 0.34038 G loss: 2.34572 (0.037 sec/batch, 1735.615 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:18,743] [train step54170] D loss: 0.33540 G loss: 2.47361 (0.041 sec/batch, 1566.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:19,135] [train step54180] D loss: 0.33838 G loss: 2.36335 (0.041 sec/batch, 1563.016 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:19,522] [train step54190] D loss: 0.33847 G loss: 2.30243 (0.034 sec/batch, 1871.857 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:19,914] [train step54201] D loss: 0.33545 G loss: 2.26556 (0.037 sec/batch, 1717.668 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:20,306] [train step54210] D loss: 0.33631 G loss: 2.33973 (0.038 sec/batch, 1705.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:20,696] [train step54221] D loss: 0.33468 G loss: 2.19226 (0.038 sec/batch, 1665.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:21,087] [train step54230] D loss: 0.33544 G loss: 2.52983 (0.038 sec/batch, 1664.117 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:21,483] [train step54240] D loss: 0.33895 G loss: 2.16221 (0.038 sec/batch, 1694.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:21,870] [train step54250] D loss: 0.33430 G loss: 2.37527 (0.039 sec/batch, 1639.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:22,266] [train step54261] D loss: 0.33470 G loss: 2.38162 (0.043 sec/batch, 1480.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:22,669] [train step54270] D loss: 0.33662 G loss: 2.26898 (0.038 sec/batch, 1697.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:23,051] [train step54281] D loss: 0.33743 G loss: 2.26993 (0.036 sec/batch, 1759.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:23,444] [train step54291] D loss: 0.33813 G loss: 2.56904 (0.043 sec/batch, 1481.882 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:23,833] [train step54300] D loss: 0.33726 G loss: 2.38807 (0.042 sec/batch, 1527.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:24,212] [train step54311] D loss: 0.33526 G loss: 2.32933 (0.039 sec/batch, 1645.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:24,608] [train step54320] D loss: 0.33960 G loss: 2.14731 (0.036 sec/batch, 1766.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:25,002] [train step54330] D loss: 0.33934 G loss: 2.57182 (0.042 sec/batch, 1529.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:25,395] [train step54341] D loss: 0.33433 G loss: 2.34936 (0.035 sec/batch, 1816.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:25,786] [train step54350] D loss: 0.33516 G loss: 2.36869 (0.033 sec/batch, 1944.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:26,177] [train step54360] D loss: 0.33544 G loss: 2.38944 (0.039 sec/batch, 1657.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:26,587] [train step54370] D loss: 0.33706 G loss: 2.33543 (0.038 sec/batch, 1693.824 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:26,975] [train step54381] D loss: 0.33756 G loss: 2.47272 (0.037 sec/batch, 1720.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:27,360] [train step54390] D loss: 0.33564 G loss: 2.44155 (0.035 sec/batch, 1852.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:27,757] [train step54401] D loss: 0.33642 G loss: 2.31604 (0.044 sec/batch, 1466.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:28,136] [train step54410] D loss: 0.33408 G loss: 2.53410 (0.039 sec/batch, 1648.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:28,539] [train step54420] D loss: 0.33550 G loss: 2.27455 (0.042 sec/batch, 1532.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:28,935] [train step54430] D loss: 0.33698 G loss: 2.29755 (0.037 sec/batch, 1710.357 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:29,327] [train step54440] D loss: 0.33563 G loss: 2.53800 (0.038 sec/batch, 1673.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:29,728] [train step54450] D loss: 0.33673 G loss: 2.59891 (0.040 sec/batch, 1598.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:30,116] [train step54461] D loss: 0.33268 G loss: 2.35810 (0.034 sec/batch, 1858.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:30,515] [train step54471] D loss: 0.33709 G loss: 2.30860 (0.039 sec/batch, 1647.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:30,911] [train step54480] D loss: 0.33543 G loss: 2.23864 (0.040 sec/batch, 1610.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:31,293] [train step54490] D loss: 0.33510 G loss: 2.40208 (0.034 sec/batch, 1879.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:31,692] [train step54501] D loss: 0.33312 G loss: 2.54400 (0.042 sec/batch, 1514.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:32,102] [train step54510] D loss: 0.33442 G loss: 2.25813 (0.038 sec/batch, 1702.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:32,483] [train step54520] D loss: 0.34339 G loss: 2.77860 (0.029 sec/batch, 2211.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:32,883] [train step54531] D loss: 0.33405 G loss: 2.14394 (0.040 sec/batch, 1611.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:33,266] [train step54540] D loss: 0.33712 G loss: 2.42627 (0.036 sec/batch, 1760.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:33,664] [train step54551] D loss: 0.33493 G loss: 2.46730 (0.037 sec/batch, 1735.481 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:34,044] [train step54561] D loss: 0.33411 G loss: 2.33194 (0.033 sec/batch, 1916.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:34,440] [train step54570] D loss: 0.33787 G loss: 2.63092 (0.040 sec/batch, 1605.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:34,841] [train step54581] D loss: 0.33611 G loss: 2.37260 (0.037 sec/batch, 1716.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:35,235] [train step54591] D loss: 0.33251 G loss: 2.40469 (0.037 sec/batch, 1736.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:35,635] [train step54600] D loss: 0.33365 G loss: 2.26839 (0.052 sec/batch, 1237.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:36,031] [train step54610] D loss: 0.33237 G loss: 2.33377 (0.041 sec/batch, 1557.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:36,426] [train step54621] D loss: 0.33044 G loss: 2.44045 (0.037 sec/batch, 1737.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:36,824] [train step54630] D loss: 0.33366 G loss: 2.48950 (0.038 sec/batch, 1662.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:37,226] [train step54641] D loss: 0.33340 G loss: 2.30145 (0.038 sec/batch, 1690.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:37,610] [train step54651] D loss: 0.33195 G loss: 2.43046 (0.038 sec/batch, 1699.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:38,006] [train step54660] D loss: 0.33510 G loss: 2.21554 (0.035 sec/batch, 1807.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:38,390] [train step54671] D loss: 0.33362 G loss: 2.30422 (0.037 sec/batch, 1713.688 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:38,784] [train step54680] D loss: 0.33493 G loss: 2.51910 (0.040 sec/batch, 1608.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:39,174] [train step54690] D loss: 0.33390 G loss: 2.32596 (0.039 sec/batch, 1628.016 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:39,544] [train step54701] D loss: 0.33210 G loss: 2.32716 (0.039 sec/batch, 1649.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:39,929] [train step54711] D loss: 0.33427 G loss: 2.49998 (0.034 sec/batch, 1867.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:40,308] [train step54720] D loss: 0.33543 G loss: 2.44256 (0.036 sec/batch, 1797.587 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:40,685] [train step54731] D loss: 0.33455 G loss: 2.20575 (0.043 sec/batch, 1504.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:41,071] [train step54740] D loss: 0.33326 G loss: 2.38224 (0.039 sec/batch, 1636.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:41,452] [train step54750] D loss: 0.33203 G loss: 2.37432 (0.034 sec/batch, 1898.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:41,861] [train step54760] D loss: 0.33363 G loss: 2.48161 (0.039 sec/batch, 1632.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:42,245] [train step54770] D loss: 0.33325 G loss: 2.34230 (0.040 sec/batch, 1587.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:42,632] [train step54780] D loss: 0.33051 G loss: 2.31584 (0.041 sec/batch, 1563.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:43,019] [train step54791] D loss: 0.33049 G loss: 2.33393 (0.038 sec/batch, 1684.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:43,408] [train step54801] D loss: 0.33142 G loss: 2.32311 (0.038 sec/batch, 1687.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:43,787] [train step54810] D loss: 0.33087 G loss: 2.20126 (0.034 sec/batch, 1879.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:44,201] [train step54821] D loss: 0.33208 G loss: 2.44653 (0.036 sec/batch, 1791.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:44,607] [train step54831] D loss: 0.33120 G loss: 2.36164 (0.041 sec/batch, 1545.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:44,995] [train step54840] D loss: 0.32984 G loss: 2.29981 (0.040 sec/batch, 1600.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:45,377] [train step54851] D loss: 0.33264 G loss: 2.33979 (0.035 sec/batch, 1820.049 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:45,759] [train step54861] D loss: 0.33409 G loss: 2.37616 (0.033 sec/batch, 1940.656 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:46,154] [train step54870] D loss: 0.33207 G loss: 2.33235 (0.039 sec/batch, 1628.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:46,523] [train step54880] D loss: 0.33329 G loss: 2.31799 (0.037 sec/batch, 1709.257 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:46,923] [train step54891] D loss: 0.33129 G loss: 2.29611 (0.040 sec/batch, 1618.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:47,313] [train step54900] D loss: 0.33722 G loss: 2.69349 (0.040 sec/batch, 1604.794 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:47,728] [train step54910] D loss: 0.33110 G loss: 2.36622 (0.044 sec/batch, 1469.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:48,125] [train step54921] D loss: 0.33346 G loss: 2.36334 (0.035 sec/batch, 1822.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:48,507] [train step54930] D loss: 0.33070 G loss: 2.30692 (0.038 sec/batch, 1701.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:48,886] [train step54940] D loss: 0.33388 G loss: 2.48829 (0.036 sec/batch, 1754.423 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:49,277] [train step54951] D loss: 0.33342 G loss: 2.32853 (0.038 sec/batch, 1672.058 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:49,666] [train step54960] D loss: 0.33673 G loss: 2.25772 (0.043 sec/batch, 1481.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:50,058] [train step54971] D loss: 0.33426 G loss: 2.49609 (0.037 sec/batch, 1723.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:50,443] [train step54980] D loss: 0.33334 G loss: 2.29884 (0.039 sec/batch, 1632.392 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:50,830] [train step54990] D loss: 0.33312 G loss: 2.54165 (0.042 sec/batch, 1507.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:51,220] [train step55000] D loss: 0.33063 G loss: 2.28289 (0.035 sec/batch, 1813.251 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:44:51,221] Saved checkpoint at 55000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:51,838] [train step55011] D loss: 0.33317 G loss: 2.20631 (0.040 sec/batch, 1610.610 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:52,249] [train step55020] D loss: 0.33075 G loss: 2.35055 (0.042 sec/batch, 1519.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:52,632] [train step55031] D loss: 0.33334 G loss: 2.35638 (0.037 sec/batch, 1749.324 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:53,024] [train step55040] D loss: 0.33302 G loss: 2.42432 (0.046 sec/batch, 1381.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:53,421] [train step55050] D loss: 0.33202 G loss: 2.51142 (0.038 sec/batch, 1675.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:53,811] [train step55060] D loss: 0.33224 G loss: 2.19269 (0.043 sec/batch, 1483.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:54,217] [train step55070] D loss: 0.33144 G loss: 2.41609 (0.042 sec/batch, 1515.454 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:54,598] [train step55080] D loss: 0.33144 G loss: 2.42426 (0.034 sec/batch, 1875.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:54,988] [train step55090] D loss: 0.33061 G loss: 2.43801 (0.041 sec/batch, 1576.908 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:55,388] [train step55101] D loss: 0.33058 G loss: 2.28058 (0.041 sec/batch, 1557.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:55,780] [train step55110] D loss: 0.33174 G loss: 2.49664 (0.041 sec/batch, 1555.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:56,168] [train step55120] D loss: 0.33338 G loss: 2.39095 (0.041 sec/batch, 1566.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:56,571] [train step55131] D loss: 0.33190 G loss: 2.27109 (0.042 sec/batch, 1522.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:56,962] [train step55140] D loss: 0.33103 G loss: 2.23739 (0.040 sec/batch, 1600.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:57,359] [train step55151] D loss: 0.33065 G loss: 2.36644 (0.038 sec/batch, 1683.403 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:57,747] [train step55161] D loss: 0.33183 G loss: 2.54156 (0.036 sec/batch, 1801.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:58,130] [train step55170] D loss: 0.33193 G loss: 2.22415 (0.037 sec/batch, 1735.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:58,540] [train step55181] D loss: 0.33080 G loss: 2.23707 (0.036 sec/batch, 1754.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:58,920] [train step55191] D loss: 0.33202 G loss: 2.42983 (0.034 sec/batch, 1869.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:59,325] [train step55200] D loss: 0.33102 G loss: 2.21349 (0.040 sec/batch, 1583.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:44:59,696] [train step55210] D loss: 0.33311 G loss: 2.29806 (0.031 sec/batch, 2056.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:00,097] [train step55221] D loss: 0.34187 G loss: 2.61741 (0.037 sec/batch, 1750.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:00,496] [train step55230] D loss: 0.47843 G loss: 1.21580 (0.040 sec/batch, 1580.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:00,895] [train step55240] D loss: 0.67675 G loss: 6.73041 (0.038 sec/batch, 1687.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:01,287] [train step55250] D loss: 0.35166 G loss: 1.86856 (0.044 sec/batch, 1465.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:01,684] [train step55260] D loss: 0.36180 G loss: 1.69971 (0.042 sec/batch, 1540.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:02,079] [train step55271] D loss: 0.34480 G loss: 2.85721 (0.039 sec/batch, 1646.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:02,474] [train step55280] D loss: 0.33528 G loss: 2.08038 (0.036 sec/batch, 1770.426 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:02,867] [train step55290] D loss: 0.33739 G loss: 2.66034 (0.041 sec/batch, 1548.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:03,259] [train step55300] D loss: 0.33053 G loss: 2.31732 (0.041 sec/batch, 1543.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:03,647] [train step55310] D loss: 0.33238 G loss: 2.27792 (0.039 sec/batch, 1624.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:04,042] [train step55320] D loss: 0.33053 G loss: 2.38839 (0.037 sec/batch, 1727.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:04,430] [train step55331] D loss: 0.33230 G loss: 2.40074 (0.039 sec/batch, 1639.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:04,824] [train step55341] D loss: 0.32888 G loss: 2.30446 (0.038 sec/batch, 1705.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:05,218] [train step55350] D loss: 0.33134 G loss: 2.34519 (0.035 sec/batch, 1841.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:05,611] [train step55361] D loss: 0.33052 G loss: 2.32173 (0.037 sec/batch, 1728.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:06,000] [train step55371] D loss: 0.33059 G loss: 2.39302 (0.035 sec/batch, 1824.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:06,389] [train step55380] D loss: 0.33045 G loss: 2.45635 (0.040 sec/batch, 1612.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:06,793] [train step55391] D loss: 0.33087 G loss: 2.40902 (0.037 sec/batch, 1722.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:07,192] [train step55400] D loss: 0.33093 G loss: 2.31789 (0.046 sec/batch, 1401.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:07,590] [train step55410] D loss: 0.32987 G loss: 2.34045 (0.043 sec/batch, 1477.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:07,971] [train step55421] D loss: 0.33171 G loss: 2.30726 (0.035 sec/batch, 1812.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:08,366] [train step55431] D loss: 0.33074 G loss: 2.44268 (0.045 sec/batch, 1437.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:08,779] [train step55440] D loss: 0.33038 G loss: 2.31511 (0.044 sec/batch, 1459.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:09,157] [train step55451] D loss: 0.33135 G loss: 2.29539 (0.037 sec/batch, 1728.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:09,547] [train step55461] D loss: 0.32875 G loss: 2.39372 (0.047 sec/batch, 1348.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:09,915] [train step55470] D loss: 0.32994 G loss: 2.30500 (0.035 sec/batch, 1830.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:10,296] [train step55480] D loss: 0.32943 G loss: 2.28377 (0.036 sec/batch, 1761.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:10,690] [train step55491] D loss: 0.33021 G loss: 2.32922 (0.037 sec/batch, 1718.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:11,067] [train step55500] D loss: 0.33115 G loss: 2.40830 (0.039 sec/batch, 1649.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:11,452] [train step55511] D loss: 0.33052 G loss: 2.42452 (0.039 sec/batch, 1662.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:11,839] [train step55521] D loss: 0.33026 G loss: 2.33464 (0.044 sec/batch, 1444.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:12,211] [train step55530] D loss: 0.33080 G loss: 2.36538 (0.037 sec/batch, 1741.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:12,610] [train step55540] D loss: 0.33051 G loss: 2.40030 (0.043 sec/batch, 1477.656 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:13,000] [train step55550] D loss: 0.32982 G loss: 2.39658 (0.045 sec/batch, 1436.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:13,374] [train step55560] D loss: 0.33057 G loss: 2.31254 (0.037 sec/batch, 1711.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:13,766] [train step55571] D loss: 0.32983 G loss: 2.38618 (0.040 sec/batch, 1601.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:14,139] [train step55581] D loss: 0.32838 G loss: 2.31106 (0.037 sec/batch, 1709.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:14,518] [train step55590] D loss: 0.32945 G loss: 2.30549 (0.037 sec/batch, 1733.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:14,904] [train step55600] D loss: 0.32884 G loss: 2.34096 (0.036 sec/batch, 1756.018 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:15,279] [train step55610] D loss: 0.32920 G loss: 2.36482 (0.037 sec/batch, 1745.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:15,662] [train step55620] D loss: 0.32928 G loss: 2.30988 (0.040 sec/batch, 1617.217 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:16,047] [train step55631] D loss: 0.32956 G loss: 2.32486 (0.039 sec/batch, 1628.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:16,442] [train step55640] D loss: 0.32908 G loss: 2.35077 (0.040 sec/batch, 1592.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:16,837] [train step55650] D loss: 0.33012 G loss: 2.23171 (0.039 sec/batch, 1654.323 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:17,219] [train step55661] D loss: 0.32920 G loss: 2.41594 (0.036 sec/batch, 1788.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:17,603] [train step55671] D loss: 0.32966 G loss: 2.32262 (0.039 sec/batch, 1629.875 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:17,999] [train step55680] D loss: 0.33018 G loss: 2.20712 (0.041 sec/batch, 1571.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:18,385] [train step55690] D loss: 0.33140 G loss: 2.52616 (0.045 sec/batch, 1415.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:18,791] [train step55700] D loss: 0.32883 G loss: 2.32246 (0.051 sec/batch, 1249.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:19,177] [train step55710] D loss: 0.33033 G loss: 2.25398 (0.038 sec/batch, 1662.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:19,577] [train step55721] D loss: 0.33094 G loss: 2.48113 (0.036 sec/batch, 1767.534 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:19,974] [train step55731] D loss: 0.32909 G loss: 2.26960 (0.038 sec/batch, 1683.287 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:20,358] [train step55740] D loss: 0.32885 G loss: 2.34743 (0.040 sec/batch, 1601.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:20,732] [train step55750] D loss: 0.33014 G loss: 2.26687 (0.028 sec/batch, 2289.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:21,137] [train step55761] D loss: 0.32865 G loss: 2.24879 (0.041 sec/batch, 1552.754 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:21,519] [train step55770] D loss: 0.32954 G loss: 2.35670 (0.033 sec/batch, 1936.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:21,918] [train step55780] D loss: 0.32879 G loss: 2.30871 (0.037 sec/batch, 1707.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:22,306] [train step55791] D loss: 0.32928 G loss: 2.27637 (0.042 sec/batch, 1525.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:22,694] [train step55800] D loss: 0.33006 G loss: 2.40935 (0.037 sec/batch, 1707.366 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:23,091] [train step55810] D loss: 0.32897 G loss: 2.30648 (0.039 sec/batch, 1620.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:23,483] [train step55820] D loss: 0.32809 G loss: 2.31119 (0.037 sec/batch, 1725.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:23,868] [train step55830] D loss: 0.33009 G loss: 2.19871 (0.036 sec/batch, 1802.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:24,264] [train step55840] D loss: 0.32968 G loss: 2.41836 (0.035 sec/batch, 1831.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:24,671] [train step55850] D loss: 0.32931 G loss: 2.23130 (0.038 sec/batch, 1679.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:25,064] [train step55860] D loss: 0.32902 G loss: 2.34547 (0.034 sec/batch, 1885.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:25,450] [train step55870] D loss: 0.33052 G loss: 2.43020 (0.036 sec/batch, 1786.224 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:25,842] [train step55881] D loss: 0.32950 G loss: 2.32923 (0.042 sec/batch, 1524.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:26,241] [train step55890] D loss: 0.32852 G loss: 2.35640 (0.047 sec/batch, 1359.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:26,630] [train step55900] D loss: 0.32960 G loss: 2.36710 (0.042 sec/batch, 1534.539 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:27,036] [train step55910] D loss: 0.32865 G loss: 2.36673 (0.043 sec/batch, 1475.293 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:27,432] [train step55920] D loss: 0.32813 G loss: 2.31916 (0.037 sec/batch, 1722.429 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:27,820] [train step55930] D loss: 0.32912 G loss: 2.25734 (0.041 sec/batch, 1569.284 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:28,219] [train step55941] D loss: 0.32835 G loss: 2.36683 (0.039 sec/batch, 1653.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:28,605] [train step55950] D loss: 0.32833 G loss: 2.39376 (0.048 sec/batch, 1346.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:28,991] [train step55960] D loss: 0.33038 G loss: 2.30022 (0.039 sec/batch, 1650.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:29,383] [train step55970] D loss: 0.32930 G loss: 2.37887 (0.046 sec/batch, 1396.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:29,769] [train step55980] D loss: 0.32894 G loss: 2.38194 (0.039 sec/batch, 1645.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:30,166] [train step55990] D loss: 0.32856 G loss: 2.32630 (0.040 sec/batch, 1583.765 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:30,572] [train step56001] D loss: 0.32842 G loss: 2.33237 (0.037 sec/batch, 1737.716 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:45:30,572] Saved checkpoint at 56000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:31,185] [train step56010] D loss: 0.32880 G loss: 2.38022 (0.037 sec/batch, 1713.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:31,572] [train step56020] D loss: 0.32932 G loss: 2.26067 (0.040 sec/batch, 1595.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:31,950] [train step56030] D loss: 0.32861 G loss: 2.36556 (0.033 sec/batch, 1928.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:32,380] [train step56040] D loss: 0.32872 G loss: 2.35460 (0.041 sec/batch, 1579.153 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:32,777] [train step56051] D loss: 0.32875 G loss: 2.29735 (0.037 sec/batch, 1736.244 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:33,191] [train step56060] D loss: 0.32855 G loss: 2.30464 (0.049 sec/batch, 1316.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:33,581] [train step56070] D loss: 0.32738 G loss: 2.33575 (0.036 sec/batch, 1765.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:33,966] [train step56080] D loss: 0.32946 G loss: 2.35822 (0.036 sec/batch, 1756.512 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:34,374] [train step56090] D loss: 0.32873 G loss: 2.26952 (0.034 sec/batch, 1871.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:34,767] [train step56100] D loss: 0.32865 G loss: 2.31340 (0.036 sec/batch, 1793.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:35,165] [train step56111] D loss: 0.32901 G loss: 2.27969 (0.038 sec/batch, 1695.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:35,558] [train step56120] D loss: 0.32885 G loss: 2.32622 (0.040 sec/batch, 1585.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:35,945] [train step56130] D loss: 0.32895 G loss: 2.37022 (0.040 sec/batch, 1580.799 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:36,353] [train step56141] D loss: 0.32878 G loss: 2.24061 (0.045 sec/batch, 1437.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:36,746] [train step56150] D loss: 0.32810 G loss: 2.38505 (0.040 sec/batch, 1583.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:37,140] [train step56160] D loss: 0.32888 G loss: 2.38223 (0.038 sec/batch, 1673.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:37,541] [train step56171] D loss: 0.32844 G loss: 2.34873 (0.040 sec/batch, 1587.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:37,925] [train step56180] D loss: 0.32882 G loss: 2.29209 (0.037 sec/batch, 1706.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:38,320] [train step56190] D loss: 0.32875 G loss: 2.23364 (0.047 sec/batch, 1349.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:38,701] [train step56200] D loss: 0.32921 G loss: 2.30457 (0.027 sec/batch, 2337.128 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:39,093] [train step56210] D loss: 0.32988 G loss: 2.53386 (0.039 sec/batch, 1652.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:39,482] [train step56220] D loss: 0.32870 G loss: 2.36332 (0.034 sec/batch, 1866.910 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:39,866] [train step56231] D loss: 0.32936 G loss: 2.34224 (0.035 sec/batch, 1818.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:40,247] [train step56240] D loss: 0.32794 G loss: 2.33603 (0.039 sec/batch, 1650.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:40,633] [train step56250] D loss: 0.32921 G loss: 2.25018 (0.036 sec/batch, 1801.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:41,027] [train step56260] D loss: 0.32875 G loss: 2.38622 (0.037 sec/batch, 1731.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:41,409] [train step56271] D loss: 0.32892 G loss: 2.31263 (0.039 sec/batch, 1642.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:41,787] [train step56280] D loss: 0.32815 G loss: 2.34526 (0.035 sec/batch, 1828.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:42,162] [train step56290] D loss: 0.32832 G loss: 2.38473 (0.045 sec/batch, 1433.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:42,539] [train step56300] D loss: 0.32912 G loss: 2.34500 (0.031 sec/batch, 2070.207 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:42,921] [train step56310] D loss: 0.32855 G loss: 2.40350 (0.041 sec/batch, 1566.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:43,304] [train step56320] D loss: 0.32823 G loss: 2.22864 (0.040 sec/batch, 1609.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:43,694] [train step56330] D loss: 0.32934 G loss: 2.35537 (0.038 sec/batch, 1695.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:44,073] [train step56340] D loss: 0.32805 G loss: 2.36249 (0.033 sec/batch, 1932.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:44,479] [train step56350] D loss: 0.32853 G loss: 2.37674 (0.038 sec/batch, 1696.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:44,866] [train step56361] D loss: 0.32844 G loss: 2.36769 (0.043 sec/batch, 1477.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:45,240] [train step56370] D loss: 0.32881 G loss: 2.22794 (0.034 sec/batch, 1878.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:45,629] [train step56381] D loss: 0.32830 G loss: 2.33351 (0.040 sec/batch, 1619.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:46,018] [train step56391] D loss: 0.32800 G loss: 2.32630 (0.037 sec/batch, 1725.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:46,402] [train step56400] D loss: 0.32797 G loss: 2.25269 (0.033 sec/batch, 1961.430 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:46,798] [train step56411] D loss: 0.32956 G loss: 2.37213 (0.036 sec/batch, 1766.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:47,183] [train step56420] D loss: 0.32907 G loss: 2.23938 (0.038 sec/batch, 1692.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:47,573] [train step56430] D loss: 0.32785 G loss: 2.33952 (0.048 sec/batch, 1342.970 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:47,958] [train step56440] D loss: 0.32753 G loss: 2.36289 (0.047 sec/batch, 1359.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:48,337] [train step56450] D loss: 0.32800 G loss: 2.36012 (0.036 sec/batch, 1786.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:48,737] [train step56460] D loss: 0.32861 G loss: 2.20236 (0.041 sec/batch, 1560.508 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:49,134] [train step56470] D loss: 0.32736 G loss: 2.36838 (0.040 sec/batch, 1585.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:49,515] [train step56481] D loss: 0.32817 G loss: 2.35801 (0.037 sec/batch, 1736.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:49,902] [train step56490] D loss: 0.32782 G loss: 2.29099 (0.035 sec/batch, 1854.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:50,291] [train step56500] D loss: 0.32809 G loss: 2.26959 (0.037 sec/batch, 1726.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:50,690] [train step56510] D loss: 0.32763 G loss: 2.34145 (0.041 sec/batch, 1551.614 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:51,081] [train step56520] D loss: 0.32858 G loss: 2.34402 (0.038 sec/batch, 1678.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:51,483] [train step56531] D loss: 0.32808 G loss: 2.27680 (0.035 sec/batch, 1804.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:51,875] [train step56541] D loss: 0.32867 G loss: 2.29516 (0.040 sec/batch, 1598.677 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:52,268] [train step56550] D loss: 0.32905 G loss: 2.34760 (0.045 sec/batch, 1431.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:52,647] [train step56560] D loss: 0.32876 G loss: 2.46405 (0.038 sec/batch, 1684.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:53,036] [train step56571] D loss: 0.32853 G loss: 2.30745 (0.033 sec/batch, 1912.874 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:53,432] [train step56580] D loss: 0.32816 G loss: 2.24584 (0.038 sec/batch, 1692.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:53,822] [train step56590] D loss: 0.32811 G loss: 2.33377 (0.044 sec/batch, 1457.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:54,211] [train step56600] D loss: 0.32866 G loss: 2.29811 (0.041 sec/batch, 1550.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:54,595] [train step56610] D loss: 0.32834 G loss: 2.38769 (0.039 sec/batch, 1644.231 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:54,985] [train step56621] D loss: 0.32928 G loss: 2.25418 (0.039 sec/batch, 1652.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:55,379] [train step56630] D loss: 0.32847 G loss: 2.35519 (0.039 sec/batch, 1624.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:55,776] [train step56640] D loss: 0.33018 G loss: 2.19720 (0.044 sec/batch, 1443.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:56,162] [train step56651] D loss: 0.32896 G loss: 2.38275 (0.035 sec/batch, 1830.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:56,566] [train step56661] D loss: 0.32947 G loss: 2.17249 (0.039 sec/batch, 1636.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:56,963] [train step56670] D loss: 0.32983 G loss: 2.32722 (0.040 sec/batch, 1581.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:57,356] [train step56681] D loss: 0.32961 G loss: 2.47828 (0.040 sec/batch, 1609.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:57,746] [train step56690] D loss: 0.32877 G loss: 2.34166 (0.040 sec/batch, 1596.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:58,139] [train step56700] D loss: 0.32839 G loss: 2.38029 (0.040 sec/batch, 1610.011 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:58,537] [train step56711] D loss: 0.32965 G loss: 2.22482 (0.041 sec/batch, 1566.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:58,940] [train step56721] D loss: 0.32803 G loss: 2.37542 (0.046 sec/batch, 1397.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:59,331] [train step56730] D loss: 0.32763 G loss: 2.23777 (0.037 sec/batch, 1713.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:45:59,718] [train step56740] D loss: 0.32844 G loss: 2.27328 (0.040 sec/batch, 1608.920 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:00,128] [train step56751] D loss: 0.32809 G loss: 2.27112 (0.041 sec/batch, 1544.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:00,521] [train step56760] D loss: 0.32905 G loss: 2.36289 (0.045 sec/batch, 1425.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:00,915] [train step56770] D loss: 0.32866 G loss: 2.19938 (0.049 sec/batch, 1306.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:01,300] [train step56780] D loss: 0.32793 G loss: 2.36360 (0.034 sec/batch, 1859.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:01,700] [train step56790] D loss: 0.32816 G loss: 2.37998 (0.039 sec/batch, 1632.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:02,123] [train step56801] D loss: 0.32812 G loss: 2.40046 (0.039 sec/batch, 1649.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:02,516] [train step56811] D loss: 0.32875 G loss: 2.35405 (0.038 sec/batch, 1671.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:02,901] [train step56820] D loss: 0.32845 G loss: 2.30784 (0.036 sec/batch, 1784.751 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:03,291] [train step56830] D loss: 0.32876 G loss: 2.34484 (0.038 sec/batch, 1675.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:03,673] [train step56840] D loss: 0.33102 G loss: 2.16905 (0.035 sec/batch, 1839.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:04,077] [train step56850] D loss: 0.32913 G loss: 2.36759 (0.041 sec/batch, 1568.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:04,471] [train step56860] D loss: 0.32851 G loss: 2.30686 (0.041 sec/batch, 1570.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:04,859] [train step56870] D loss: 0.32825 G loss: 2.35918 (0.038 sec/batch, 1669.188 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:05,257] [train step56880] D loss: 0.32899 G loss: 2.30602 (0.038 sec/batch, 1676.244 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:05,646] [train step56891] D loss: 0.32893 G loss: 2.36950 (0.038 sec/batch, 1688.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:06,037] [train step56901] D loss: 0.32860 G loss: 2.29657 (0.040 sec/batch, 1599.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:06,440] [train step56910] D loss: 0.32942 G loss: 2.24441 (0.038 sec/batch, 1666.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:06,825] [train step56920] D loss: 0.32774 G loss: 2.32385 (0.036 sec/batch, 1760.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:07,220] [train step56931] D loss: 0.32866 G loss: 2.29280 (0.030 sec/batch, 2165.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:07,626] [train step56940] D loss: 0.32887 G loss: 2.35327 (0.036 sec/batch, 1784.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:08,015] [train step56950] D loss: 0.32858 G loss: 2.28578 (0.036 sec/batch, 1791.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:08,413] [train step56960] D loss: 0.32806 G loss: 2.28868 (0.042 sec/batch, 1536.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:08,797] [train step56970] D loss: 0.32912 G loss: 2.30569 (0.038 sec/batch, 1706.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:09,194] [train step56980] D loss: 0.33021 G loss: 2.52446 (0.041 sec/batch, 1578.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:09,584] [train step56991] D loss: 0.32864 G loss: 2.24254 (0.039 sec/batch, 1640.703 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:09,968] [train step57000] D loss: 0.32997 G loss: 2.23909 (0.039 sec/batch, 1633.604 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:46:09,968] Saved checkpoint at 57000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:10,566] [train step57011] D loss: 0.32980 G loss: 2.40095 (0.033 sec/batch, 1937.505 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:10,941] [train step57020] D loss: 0.32948 G loss: 2.18274 (0.034 sec/batch, 1866.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:11,334] [train step57030] D loss: 0.32834 G loss: 2.36720 (0.039 sec/batch, 1641.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:11,718] [train step57041] D loss: 0.32822 G loss: 2.30850 (0.037 sec/batch, 1737.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:12,099] [train step57051] D loss: 0.33012 G loss: 2.48609 (0.036 sec/batch, 1776.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:12,492] [train step57060] D loss: 0.32838 G loss: 2.30725 (0.041 sec/batch, 1572.667 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:12,889] [train step57070] D loss: 0.32816 G loss: 2.41134 (0.026 sec/batch, 2424.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:13,285] [train step57080] D loss: 0.32953 G loss: 2.21949 (0.040 sec/batch, 1608.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:13,659] [train step57090] D loss: 0.32984 G loss: 2.37977 (0.034 sec/batch, 1872.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:14,039] [train step57101] D loss: 0.32834 G loss: 2.27985 (0.040 sec/batch, 1609.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:14,433] [train step57110] D loss: 0.32978 G loss: 2.27230 (0.038 sec/batch, 1703.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:14,819] [train step57120] D loss: 0.32895 G loss: 2.31754 (0.038 sec/batch, 1669.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:15,208] [train step57130] D loss: 0.32795 G loss: 2.25840 (0.040 sec/batch, 1617.217 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:15,602] [train step57140] D loss: 0.32743 G loss: 2.31641 (0.039 sec/batch, 1647.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:15,977] [train step57150] D loss: 0.32837 G loss: 2.25954 (0.035 sec/batch, 1848.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:16,371] [train step57161] D loss: 0.32960 G loss: 2.45662 (0.043 sec/batch, 1475.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:16,750] [train step57171] D loss: 0.32846 G loss: 2.19414 (0.037 sec/batch, 1712.977 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:17,138] [train step57180] D loss: 0.32815 G loss: 2.31046 (0.040 sec/batch, 1606.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:17,526] [train step57190] D loss: 0.32889 G loss: 2.25946 (0.040 sec/batch, 1616.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:17,909] [train step57200] D loss: 0.32772 G loss: 2.39615 (0.035 sec/batch, 1803.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:18,289] [train step57210] D loss: 0.32884 G loss: 2.37367 (0.037 sec/batch, 1708.387 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:18,682] [train step57221] D loss: 0.32757 G loss: 2.30375 (0.040 sec/batch, 1597.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:19,062] [train step57231] D loss: 0.32843 G loss: 2.24648 (0.033 sec/batch, 1911.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:19,454] [train step57240] D loss: 0.32791 G loss: 2.31333 (0.040 sec/batch, 1592.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:19,851] [train step57250] D loss: 0.32799 G loss: 2.37882 (0.042 sec/batch, 1528.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:20,231] [train step57261] D loss: 0.32818 G loss: 2.24612 (0.038 sec/batch, 1690.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:20,623] [train step57270] D loss: 0.32824 G loss: 2.27898 (0.037 sec/batch, 1742.725 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:21,008] [train step57280] D loss: 0.32746 G loss: 2.26650 (0.032 sec/batch, 2004.416 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:21,398] [train step57291] D loss: 0.32862 G loss: 2.37237 (0.039 sec/batch, 1654.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:21,796] [train step57300] D loss: 0.32884 G loss: 2.33111 (0.040 sec/batch, 1617.666 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:22,184] [train step57310] D loss: 0.32767 G loss: 2.33567 (0.036 sec/batch, 1800.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:22,570] [train step57321] D loss: 0.32873 G loss: 2.36563 (0.034 sec/batch, 1908.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:22,958] [train step57330] D loss: 0.32916 G loss: 2.40963 (0.036 sec/batch, 1760.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:23,364] [train step57340] D loss: 0.32919 G loss: 2.41427 (0.042 sec/batch, 1531.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:23,753] [train step57350] D loss: 0.32995 G loss: 2.24958 (0.034 sec/batch, 1905.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:24,141] [train step57360] D loss: 0.32876 G loss: 2.38630 (0.035 sec/batch, 1850.960 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:24,554] [train step57371] D loss: 0.32903 G loss: 2.34352 (0.044 sec/batch, 1452.730 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:24,946] [train step57380] D loss: 0.32902 G loss: 2.21818 (0.037 sec/batch, 1709.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:25,331] [train step57390] D loss: 0.33072 G loss: 2.51130 (0.035 sec/batch, 1829.788 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:25,730] [train step57401] D loss: 0.32897 G loss: 2.21138 (0.035 sec/batch, 1822.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:26,125] [train step57410] D loss: 0.32925 G loss: 2.48840 (0.038 sec/batch, 1678.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:26,512] [train step57420] D loss: 0.32856 G loss: 2.28345 (0.037 sec/batch, 1736.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:26,905] [train step57430] D loss: 0.32862 G loss: 2.45508 (0.037 sec/batch, 1731.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:27,289] [train step57441] D loss: 0.33016 G loss: 2.18653 (0.035 sec/batch, 1811.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:27,684] [train step57450] D loss: 0.33036 G loss: 2.19224 (0.038 sec/batch, 1665.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:28,076] [train step57461] D loss: 0.32864 G loss: 2.28310 (0.037 sec/batch, 1735.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:28,459] [train step57471] D loss: 0.32980 G loss: 2.23301 (0.035 sec/batch, 1838.549 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:28,859] [train step57480] D loss: 0.32780 G loss: 2.33852 (0.037 sec/batch, 1735.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:29,255] [train step57490] D loss: 0.33036 G loss: 2.24543 (0.038 sec/batch, 1705.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:29,639] [train step57500] D loss: 0.32970 G loss: 2.48890 (0.036 sec/batch, 1781.257 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:30,034] [train step57510] D loss: 0.32961 G loss: 2.22911 (0.035 sec/batch, 1811.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:30,417] [train step57520] D loss: 0.32750 G loss: 2.34882 (0.037 sec/batch, 1752.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:30,818] [train step57531] D loss: 0.32892 G loss: 2.38491 (0.038 sec/batch, 1705.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:31,205] [train step57540] D loss: 0.32951 G loss: 2.32039 (0.037 sec/batch, 1707.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:31,589] [train step57551] D loss: 0.32870 G loss: 2.26956 (0.037 sec/batch, 1712.583 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:31,989] [train step57561] D loss: 0.32842 G loss: 2.33809 (0.037 sec/batch, 1739.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:32,383] [train step57570] D loss: 0.32947 G loss: 2.27317 (0.040 sec/batch, 1590.861 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:32,770] [train step57580] D loss: 0.32841 G loss: 2.29188 (0.040 sec/batch, 1584.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:33,164] [train step57590] D loss: 0.32779 G loss: 2.37667 (0.034 sec/batch, 1875.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:33,565] [train step57600] D loss: 0.32818 G loss: 2.28457 (0.042 sec/batch, 1506.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:33,960] [train step57610] D loss: 0.32817 G loss: 2.36234 (0.032 sec/batch, 2010.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:34,354] [train step57621] D loss: 0.32890 G loss: 2.26928 (0.038 sec/batch, 1694.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:34,730] [train step57630] D loss: 0.33013 G loss: 2.52373 (0.037 sec/batch, 1752.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:35,120] [train step57640] D loss: 0.32875 G loss: 2.28225 (0.038 sec/batch, 1694.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:35,506] [train step57651] D loss: 0.32804 G loss: 2.28072 (0.036 sec/batch, 1792.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:35,902] [train step57660] D loss: 0.32912 G loss: 2.18591 (0.047 sec/batch, 1351.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:36,294] [train step57670] D loss: 0.32776 G loss: 2.36791 (0.039 sec/batch, 1636.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:36,674] [train step57680] D loss: 0.32762 G loss: 2.33376 (0.037 sec/batch, 1726.140 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:37,081] [train step57690] D loss: 0.32831 G loss: 2.37376 (0.044 sec/batch, 1454.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:37,466] [train step57701] D loss: 0.33067 G loss: 2.39592 (0.040 sec/batch, 1588.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:37,851] [train step57711] D loss: 0.32785 G loss: 2.29214 (0.037 sec/batch, 1744.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:38,246] [train step57720] D loss: 0.32798 G loss: 2.38468 (0.036 sec/batch, 1757.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:38,635] [train step57731] D loss: 0.32910 G loss: 2.23789 (0.036 sec/batch, 1793.983 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:39,038] [train step57741] D loss: 0.32811 G loss: 2.32207 (0.038 sec/batch, 1678.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:39,427] [train step57750] D loss: 0.32943 G loss: 2.23322 (0.038 sec/batch, 1689.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:39,810] [train step57761] D loss: 0.32827 G loss: 2.34278 (0.042 sec/batch, 1531.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:40,221] [train step57770] D loss: 0.32735 G loss: 2.31283 (0.047 sec/batch, 1357.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:40,605] [train step57780] D loss: 0.32747 G loss: 2.35004 (0.038 sec/batch, 1700.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:40,989] [train step57790] D loss: 0.32824 G loss: 2.27789 (0.038 sec/batch, 1670.871 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:41,369] [train step57801] D loss: 0.32806 G loss: 2.29756 (0.037 sec/batch, 1727.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:41,749] [train step57810] D loss: 0.32794 G loss: 2.27815 (0.038 sec/batch, 1700.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:42,134] [train step57821] D loss: 0.32787 G loss: 2.34578 (0.036 sec/batch, 1763.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:42,519] [train step57831] D loss: 0.32759 G loss: 2.41147 (0.039 sec/batch, 1657.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:42,897] [train step57840] D loss: 0.32786 G loss: 2.36221 (0.039 sec/batch, 1632.283 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:43,283] [train step57850] D loss: 0.32755 G loss: 2.24993 (0.037 sec/batch, 1752.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:43,661] [train step57860] D loss: 0.32780 G loss: 2.41919 (0.036 sec/batch, 1756.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:44,047] [train step57870] D loss: 0.32838 G loss: 2.26394 (0.037 sec/batch, 1728.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:44,442] [train step57881] D loss: 0.32750 G loss: 2.28575 (0.030 sec/batch, 2103.726 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:44,823] [train step57891] D loss: 0.32865 G loss: 2.37231 (0.034 sec/batch, 1867.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:45,211] [train step57900] D loss: 0.32857 G loss: 2.19498 (0.036 sec/batch, 1794.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:45,591] [train step57910] D loss: 0.32915 G loss: 2.17668 (0.040 sec/batch, 1584.662 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:45,968] [train step57921] D loss: 0.32899 G loss: 2.44969 (0.034 sec/batch, 1881.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:46,362] [train step57930] D loss: 0.32828 G loss: 2.17343 (0.039 sec/batch, 1641.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:46,749] [train step57940] D loss: 0.32794 G loss: 2.33194 (0.039 sec/batch, 1659.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:47,139] [train step57950] D loss: 0.33020 G loss: 2.53760 (0.038 sec/batch, 1706.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:47,531] [train step57960] D loss: 0.32878 G loss: 2.32913 (0.037 sec/batch, 1732.054 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:47,916] [train step57971] D loss: 0.32838 G loss: 2.47368 (0.039 sec/batch, 1641.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:48,307] [train step57980] D loss: 0.32835 G loss: 2.18768 (0.038 sec/batch, 1681.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:48,692] [train step57990] D loss: 0.32843 G loss: 2.33226 (0.037 sec/batch, 1718.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:49,076] [train step58001] D loss: 0.32798 G loss: 2.24756 (0.032 sec/batch, 1989.192 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:46:49,077] Saved checkpoint at 58000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:49,691] [train step58010] D loss: 0.32929 G loss: 2.51145 (0.037 sec/batch, 1714.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:50,092] [train step58020] D loss: 0.32791 G loss: 2.31512 (0.040 sec/batch, 1586.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:50,489] [train step58031] D loss: 0.32781 G loss: 2.30776 (0.035 sec/batch, 1822.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:50,868] [train step58040] D loss: 0.32785 G loss: 2.28222 (0.039 sec/batch, 1647.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:51,247] [train step58050] D loss: 0.32865 G loss: 2.44633 (0.036 sec/batch, 1763.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:51,630] [train step58061] D loss: 0.32924 G loss: 2.21987 (0.035 sec/batch, 1807.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:52,016] [train step58070] D loss: 0.32754 G loss: 2.29530 (0.037 sec/batch, 1742.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:52,416] [train step58080] D loss: 0.32833 G loss: 2.25698 (0.037 sec/batch, 1732.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:52,804] [train step58091] D loss: 0.32858 G loss: 2.32785 (0.038 sec/batch, 1666.897 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:53,190] [train step58100] D loss: 0.32740 G loss: 2.28603 (0.037 sec/batch, 1714.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:53,587] [train step58110] D loss: 0.32786 G loss: 2.39933 (0.038 sec/batch, 1703.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:53,975] [train step58120] D loss: 0.32800 G loss: 2.39793 (0.039 sec/batch, 1644.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:54,365] [train step58130] D loss: 0.32734 G loss: 2.30510 (0.040 sec/batch, 1606.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:54,761] [train step58140] D loss: 0.32775 G loss: 2.26922 (0.038 sec/batch, 1688.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:55,163] [train step58150] D loss: 0.32780 G loss: 2.27411 (0.038 sec/batch, 1676.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:55,558] [train step58160] D loss: 0.32766 G loss: 2.32939 (0.037 sec/batch, 1740.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:55,952] [train step58170] D loss: 0.32855 G loss: 2.42951 (0.037 sec/batch, 1715.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:56,339] [train step58181] D loss: 0.32786 G loss: 2.28629 (0.038 sec/batch, 1670.050 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:56,739] [train step58191] D loss: 0.32798 G loss: 2.22109 (0.036 sec/batch, 1753.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:57,128] [train step58200] D loss: 0.32744 G loss: 2.32807 (0.039 sec/batch, 1655.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:57,521] [train step58211] D loss: 0.32778 G loss: 2.31634 (0.044 sec/batch, 1438.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:57,899] [train step58221] D loss: 0.32848 G loss: 2.29054 (0.036 sec/batch, 1772.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:58,286] [train step58230] D loss: 0.32794 G loss: 2.30379 (0.040 sec/batch, 1605.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:58,684] [train step58240] D loss: 0.32777 G loss: 2.30383 (0.040 sec/batch, 1588.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:59,065] [train step58250] D loss: 0.32760 G loss: 2.37857 (0.038 sec/batch, 1663.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:59,454] [train step58260] D loss: 0.32805 G loss: 2.41501 (0.037 sec/batch, 1714.366 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:46:59,845] [train step58270] D loss: 0.32719 G loss: 2.27913 (0.035 sec/batch, 1839.645 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:00,241] [train step58281] D loss: 0.32852 G loss: 2.19991 (0.038 sec/batch, 1701.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:00,639] [train step58290] D loss: 0.32891 G loss: 2.40622 (0.028 sec/batch, 2323.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:01,038] [train step58301] D loss: 0.32766 G loss: 2.31552 (0.043 sec/batch, 1499.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:01,424] [train step58310] D loss: 0.32886 G loss: 2.32896 (0.042 sec/batch, 1531.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:01,822] [train step58320] D loss: 0.32834 G loss: 2.36874 (0.037 sec/batch, 1720.068 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:02,221] [train step58331] D loss: 0.32822 G loss: 2.33602 (0.048 sec/batch, 1323.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:02,624] [train step58340] D loss: 0.33231 G loss: 2.62192 (0.056 sec/batch, 1150.967 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:03,022] [train step58350] D loss: 0.32799 G loss: 2.22290 (0.042 sec/batch, 1506.702 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:03,405] [train step58360] D loss: 0.33162 G loss: 2.60537 (0.034 sec/batch, 1873.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:03,806] [train step58371] D loss: 0.32867 G loss: 2.19159 (0.039 sec/batch, 1651.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:04,189] [train step58380] D loss: 0.32840 G loss: 2.43465 (0.036 sec/batch, 1771.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:04,589] [train step58391] D loss: 0.32727 G loss: 2.27544 (0.039 sec/batch, 1639.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:04,987] [train step58400] D loss: 0.32835 G loss: 2.41959 (0.039 sec/batch, 1649.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:05,369] [train step58410] D loss: 0.32777 G loss: 2.26386 (0.039 sec/batch, 1637.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:05,779] [train step58420] D loss: 0.32680 G loss: 2.28361 (0.040 sec/batch, 1613.252 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:06,171] [train step58431] D loss: 0.32797 G loss: 2.37711 (0.045 sec/batch, 1431.816 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:06,556] [train step58440] D loss: 0.32711 G loss: 2.33292 (0.036 sec/batch, 1800.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:06,952] [train step58451] D loss: 0.32824 G loss: 2.40179 (0.036 sec/batch, 1800.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:07,342] [train step58460] D loss: 0.32709 G loss: 2.33020 (0.037 sec/batch, 1742.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:07,734] [train step58470] D loss: 0.32752 G loss: 2.28679 (0.035 sec/batch, 1811.085 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:08,137] [train step58480] D loss: 0.32699 G loss: 2.34513 (0.038 sec/batch, 1699.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:08,521] [train step58490] D loss: 0.33105 G loss: 2.13134 (0.040 sec/batch, 1599.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:08,919] [train step58500] D loss: 0.33503 G loss: 2.66337 (0.042 sec/batch, 1520.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:09,304] [train step58511] D loss: 0.32858 G loss: 2.19546 (0.037 sec/batch, 1707.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:09,693] [train step58521] D loss: 0.32899 G loss: 2.26921 (0.036 sec/batch, 1785.689 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:10,086] [train step58530] D loss: 0.32876 G loss: 2.30581 (0.042 sec/batch, 1540.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:10,470] [train step58541] D loss: 0.32797 G loss: 2.23603 (0.036 sec/batch, 1791.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:10,865] [train step58550] D loss: 0.32923 G loss: 2.30826 (0.049 sec/batch, 1301.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:11,260] [train step58560] D loss: 0.32959 G loss: 2.33811 (0.039 sec/batch, 1648.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:11,639] [train step58571] D loss: 0.32817 G loss: 2.35783 (0.037 sec/batch, 1739.169 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:12,018] [train step58581] D loss: 0.33141 G loss: 2.59169 (0.036 sec/batch, 1766.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:12,400] [train step58590] D loss: 0.33132 G loss: 2.10970 (0.035 sec/batch, 1813.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:12,777] [train step58601] D loss: 0.32841 G loss: 2.32751 (0.036 sec/batch, 1758.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:13,167] [train step58610] D loss: 0.33178 G loss: 2.58315 (0.038 sec/batch, 1685.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:13,540] [train step58620] D loss: 0.32834 G loss: 2.28157 (0.043 sec/batch, 1496.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:13,915] [train step58631] D loss: 0.33255 G loss: 2.60095 (0.035 sec/batch, 1815.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:14,302] [train step58640] D loss: 0.33098 G loss: 2.12934 (0.031 sec/batch, 2048.656 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:14,693] [train step58650] D loss: 0.34022 G loss: 2.81361 (0.036 sec/batch, 1777.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:15,078] [train step58660] D loss: 0.33635 G loss: 2.73952 (0.037 sec/batch, 1719.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:15,456] [train step58671] D loss: 0.32945 G loss: 2.20437 (0.039 sec/batch, 1624.745 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:15,841] [train step58680] D loss: 0.34576 G loss: 2.95129 (0.040 sec/batch, 1589.711 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:16,242] [train step58690] D loss: 0.33009 G loss: 2.39717 (0.037 sec/batch, 1716.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:16,633] [train step58701] D loss: 0.33005 G loss: 2.23235 (0.041 sec/batch, 1550.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:17,025] [train step58710] D loss: 0.32854 G loss: 2.30631 (0.041 sec/batch, 1542.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:17,415] [train step58721] D loss: 0.32895 G loss: 2.28740 (0.033 sec/batch, 1920.730 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:17,808] [train step58730] D loss: 0.32766 G loss: 2.37058 (0.039 sec/batch, 1646.813 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:18,202] [train step58740] D loss: 0.32762 G loss: 2.34035 (0.036 sec/batch, 1761.041 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:18,586] [train step58750] D loss: 0.32876 G loss: 2.44138 (0.034 sec/batch, 1865.003 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:19,007] [train step58761] D loss: 0.32817 G loss: 2.42097 (0.038 sec/batch, 1693.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:19,404] [train step58770] D loss: 0.32811 G loss: 2.30361 (0.037 sec/batch, 1741.640 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:19,801] [train step58780] D loss: 0.32813 G loss: 2.36785 (0.042 sec/batch, 1528.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:20,204] [train step58791] D loss: 0.32793 G loss: 2.36857 (0.037 sec/batch, 1746.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:20,597] [train step58800] D loss: 0.32777 G loss: 2.25632 (0.040 sec/batch, 1599.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:20,995] [train step58810] D loss: 0.32849 G loss: 2.28145 (0.036 sec/batch, 1795.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:21,404] [train step58821] D loss: 0.32861 G loss: 2.45956 (0.040 sec/batch, 1599.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:21,793] [train step58830] D loss: 0.32821 G loss: 2.20475 (0.037 sec/batch, 1743.959 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:22,195] [train step58841] D loss: 0.32768 G loss: 2.29512 (0.050 sec/batch, 1279.153 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:22,591] [train step58850] D loss: 0.32792 G loss: 2.32820 (0.038 sec/batch, 1684.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:22,989] [train step58860] D loss: 0.32771 G loss: 2.31414 (0.037 sec/batch, 1731.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:23,401] [train step58870] D loss: 0.32775 G loss: 2.38068 (0.041 sec/batch, 1562.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:23,796] [train step58880] D loss: 0.32781 G loss: 2.40106 (0.038 sec/batch, 1668.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:24,187] [train step58890] D loss: 0.32834 G loss: 2.42216 (0.036 sec/batch, 1775.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:24,595] [train step58900] D loss: 0.32770 G loss: 2.39205 (0.039 sec/batch, 1636.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:24,990] [train step58910] D loss: 0.32724 G loss: 2.25388 (0.038 sec/batch, 1672.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:25,395] [train step58920] D loss: 0.32876 G loss: 2.39402 (0.042 sec/batch, 1536.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:25,779] [train step58930] D loss: 0.41713 G loss: 3.99026 (0.033 sec/batch, 1941.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:26,191] [train step58940] D loss: 0.36837 G loss: 1.60045 (0.040 sec/batch, 1586.357 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:26,609] [train step58950] D loss: 0.35748 G loss: 3.17761 (0.046 sec/batch, 1385.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:27,002] [train step58960] D loss: 0.33903 G loss: 1.91752 (0.040 sec/batch, 1582.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:27,406] [train step58970] D loss: 0.33140 G loss: 2.55401 (0.041 sec/batch, 1550.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:27,802] [train step58980] D loss: 0.32844 G loss: 2.28751 (0.038 sec/batch, 1668.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:28,198] [train step58990] D loss: 0.33090 G loss: 2.53277 (0.036 sec/batch, 1783.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:28,613] [train step59001] D loss: 0.32929 G loss: 2.22747 (0.040 sec/batch, 1609.856 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:47:28,613] Saved checkpoint at 59000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:29,232] [train step59010] D loss: 0.32926 G loss: 2.40764 (0.038 sec/batch, 1699.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:29,633] [train step59021] D loss: 0.32956 G loss: 2.22563 (0.035 sec/batch, 1834.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:30,020] [train step59030] D loss: 0.32778 G loss: 2.30189 (0.040 sec/batch, 1603.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:30,420] [train step59040] D loss: 0.32880 G loss: 2.24023 (0.046 sec/batch, 1384.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:30,810] [train step59050] D loss: 0.32800 G loss: 2.34651 (0.039 sec/batch, 1626.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:31,196] [train step59060] D loss: 0.32710 G loss: 2.33788 (0.042 sec/batch, 1538.585 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:31,590] [train step59070] D loss: 0.32786 G loss: 2.29845 (0.032 sec/batch, 2000.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:31,988] [train step59080] D loss: 0.32817 G loss: 2.35081 (0.042 sec/batch, 1509.447 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:32,375] [train step59090] D loss: 0.32888 G loss: 2.44657 (0.034 sec/batch, 1904.419 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:32,798] [train step59100] D loss: 0.32761 G loss: 2.26525 (0.044 sec/batch, 1461.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:33,188] [train step59111] D loss: 0.32837 G loss: 2.43365 (0.040 sec/batch, 1604.113 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:33,583] [train step59121] D loss: 0.32759 G loss: 2.25886 (0.041 sec/batch, 1559.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:33,984] [train step59130] D loss: 0.32936 G loss: 2.17418 (0.036 sec/batch, 1785.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:34,374] [train step59140] D loss: 0.32804 G loss: 2.30934 (0.039 sec/batch, 1652.775 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:34,771] [train step59151] D loss: 0.32849 G loss: 2.47256 (0.041 sec/batch, 1567.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:35,167] [train step59160] D loss: 0.32776 G loss: 2.38787 (0.039 sec/batch, 1637.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:35,551] [train step59171] D loss: 0.32721 G loss: 2.35759 (0.033 sec/batch, 1924.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:35,953] [train step59180] D loss: 0.32731 G loss: 2.29451 (0.038 sec/batch, 1694.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:36,362] [train step59190] D loss: 0.32776 G loss: 2.41722 (0.037 sec/batch, 1735.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:36,752] [train step59200] D loss: 0.32770 G loss: 2.29392 (0.037 sec/batch, 1714.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:37,153] [train step59210] D loss: 0.32710 G loss: 2.28454 (0.036 sec/batch, 1774.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:37,543] [train step59220] D loss: 0.32843 G loss: 2.32686 (0.038 sec/batch, 1694.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:37,947] [train step59231] D loss: 0.32825 G loss: 2.41184 (0.040 sec/batch, 1595.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:38,333] [train step59240] D loss: 0.32767 G loss: 2.23476 (0.038 sec/batch, 1703.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:38,729] [train step59250] D loss: 0.32743 G loss: 2.37222 (0.041 sec/batch, 1558.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:39,117] [train step59261] D loss: 0.32711 G loss: 2.29782 (0.038 sec/batch, 1663.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:39,508] [train step59271] D loss: 0.32726 G loss: 2.27000 (0.037 sec/batch, 1712.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:39,897] [train step59280] D loss: 0.32770 G loss: 2.37898 (0.042 sec/batch, 1534.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:40,286] [train step59291] D loss: 0.32766 G loss: 2.25945 (0.038 sec/batch, 1682.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:40,675] [train step59300] D loss: 0.32757 G loss: 2.26953 (0.037 sec/batch, 1743.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:41,068] [train step59310] D loss: 0.32780 G loss: 2.31309 (0.036 sec/batch, 1753.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:41,446] [train step59321] D loss: 0.32756 G loss: 2.26446 (0.035 sec/batch, 1843.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:41,846] [train step59330] D loss: 0.32715 G loss: 2.29890 (0.036 sec/batch, 1769.772 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:42,241] [train step59340] D loss: 0.32771 G loss: 2.32769 (0.039 sec/batch, 1639.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:42,626] [train step59350] D loss: 0.32726 G loss: 2.28228 (0.040 sec/batch, 1595.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:43,002] [train step59361] D loss: 0.32754 G loss: 2.33884 (0.035 sec/batch, 1803.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:43,377] [train step59370] D loss: 0.32781 G loss: 2.27573 (0.036 sec/batch, 1771.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:43,758] [train step59381] D loss: 0.32675 G loss: 2.30244 (0.039 sec/batch, 1654.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:44,139] [train step59391] D loss: 0.32758 G loss: 2.40801 (0.036 sec/batch, 1775.038 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:44,527] [train step59400] D loss: 0.32768 G loss: 2.31835 (0.035 sec/batch, 1819.100 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:44,916] [train step59411] D loss: 0.32739 G loss: 2.28870 (0.034 sec/batch, 1856.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:45,295] [train step59420] D loss: 0.32719 G loss: 2.37152 (0.036 sec/batch, 1770.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:45,671] [train step59430] D loss: 0.32715 G loss: 2.30264 (0.035 sec/batch, 1817.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:46,070] [train step59440] D loss: 0.32737 G loss: 2.30663 (0.038 sec/batch, 1686.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:46,444] [train step59450] D loss: 0.32750 G loss: 2.31855 (0.036 sec/batch, 1782.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:46,834] [train step59460] D loss: 0.32703 G loss: 2.29167 (0.040 sec/batch, 1604.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:47,226] [train step59470] D loss: 0.32688 G loss: 2.29321 (0.039 sec/batch, 1630.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:47,607] [train step59481] D loss: 0.32691 G loss: 2.38715 (0.046 sec/batch, 1398.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:48,003] [train step59490] D loss: 0.32715 G loss: 2.34572 (0.035 sec/batch, 1803.287 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:48,386] [train step59500] D loss: 0.32706 G loss: 2.26266 (0.039 sec/batch, 1639.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:48,768] [train step59511] D loss: 0.32663 G loss: 2.28405 (0.038 sec/batch, 1693.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:49,160] [train step59520] D loss: 0.32682 G loss: 2.27747 (0.037 sec/batch, 1725.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:49,545] [train step59531] D loss: 0.32684 G loss: 2.28663 (0.037 sec/batch, 1747.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:49,926] [train step59540] D loss: 0.32696 G loss: 2.33906 (0.039 sec/batch, 1655.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:50,322] [train step59550] D loss: 0.32710 G loss: 2.26050 (0.039 sec/batch, 1655.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:50,703] [train step59560] D loss: 0.32746 G loss: 2.37666 (0.036 sec/batch, 1753.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:51,099] [train step59571] D loss: 0.32784 G loss: 2.25290 (0.036 sec/batch, 1767.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:51,483] [train step59580] D loss: 0.32721 G loss: 2.27899 (0.036 sec/batch, 1754.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:51,884] [train step59590] D loss: 0.32683 G loss: 2.30246 (0.039 sec/batch, 1659.508 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:52,282] [train step59601] D loss: 0.32749 G loss: 2.40254 (0.036 sec/batch, 1797.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:52,669] [train step59610] D loss: 0.32685 G loss: 2.25432 (0.043 sec/batch, 1488.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:53,054] [train step59621] D loss: 0.32658 G loss: 2.35140 (0.046 sec/batch, 1389.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:53,434] [train step59630] D loss: 0.32690 G loss: 2.30442 (0.037 sec/batch, 1714.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:53,818] [train step59640] D loss: 0.32716 G loss: 2.26560 (0.035 sec/batch, 1850.807 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:54,222] [train step59651] D loss: 0.32710 G loss: 2.27734 (0.035 sec/batch, 1844.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:54,610] [train step59661] D loss: 0.32677 G loss: 2.35871 (0.039 sec/batch, 1631.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:54,989] [train step59670] D loss: 0.32654 G loss: 2.26597 (0.036 sec/batch, 1764.954 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:55,382] [train step59681] D loss: 0.32740 G loss: 2.36850 (0.035 sec/batch, 1853.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:55,772] [train step59691] D loss: 0.32696 G loss: 2.32983 (0.037 sec/batch, 1728.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:56,171] [train step59700] D loss: 0.32698 G loss: 2.33167 (0.041 sec/batch, 1556.897 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:56,551] [train step59711] D loss: 0.32727 G loss: 2.21183 (0.035 sec/batch, 1844.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:56,945] [train step59720] D loss: 0.32735 G loss: 2.36865 (0.041 sec/batch, 1555.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:57,336] [train step59730] D loss: 0.32764 G loss: 2.26472 (0.038 sec/batch, 1674.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:57,721] [train step59741] D loss: 0.32670 G loss: 2.34447 (0.038 sec/batch, 1684.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:58,142] [train step59750] D loss: 0.32694 G loss: 2.32011 (0.041 sec/batch, 1548.347 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:58,545] [train step59760] D loss: 0.32752 G loss: 2.32953 (0.039 sec/batch, 1641.014 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:58,923] [train step59771] D loss: 0.32758 G loss: 2.30903 (0.036 sec/batch, 1801.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:59,315] [train step59780] D loss: 0.32779 G loss: 2.24464 (0.041 sec/batch, 1561.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:47:59,711] [train step59790] D loss: 0.32805 G loss: 2.38326 (0.040 sec/batch, 1587.549 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:00,101] [train step59801] D loss: 0.32738 G loss: 2.24834 (0.035 sec/batch, 1808.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:00,495] [train step59810] D loss: 0.32696 G loss: 2.33220 (0.034 sec/batch, 1873.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:00,887] [train step59820] D loss: 0.32780 G loss: 2.26738 (0.038 sec/batch, 1670.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:01,283] [train step59831] D loss: 0.32673 G loss: 2.32745 (0.044 sec/batch, 1451.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:01,675] [train step59841] D loss: 0.32756 G loss: 2.23100 (0.040 sec/batch, 1585.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:02,057] [train step59850] D loss: 0.32730 G loss: 2.27545 (0.041 sec/batch, 1545.967 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:02,453] [train step59861] D loss: 0.32769 G loss: 2.18765 (0.039 sec/batch, 1658.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:02,841] [train step59870] D loss: 0.32812 G loss: 2.48671 (0.041 sec/batch, 1544.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:03,240] [train step59880] D loss: 0.32770 G loss: 2.30952 (0.044 sec/batch, 1456.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:03,640] [train step59890] D loss: 0.32752 G loss: 2.39012 (0.038 sec/batch, 1702.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:04,023] [train step59901] D loss: 0.32752 G loss: 2.23400 (0.039 sec/batch, 1631.370 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:04,429] [train step59910] D loss: 0.32733 G loss: 2.37359 (0.039 sec/batch, 1639.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:04,815] [train step59920] D loss: 0.33001 G loss: 2.13093 (0.035 sec/batch, 1836.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:05,203] [train step59930] D loss: 0.32736 G loss: 2.36266 (0.035 sec/batch, 1844.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:05,594] [train step59940] D loss: 0.32764 G loss: 2.23445 (0.040 sec/batch, 1597.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:05,988] [train step59950] D loss: 0.32739 G loss: 2.31924 (0.039 sec/batch, 1630.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:06,378] [train step59960] D loss: 0.32874 G loss: 2.48160 (0.041 sec/batch, 1573.700 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:06,776] [train step59970] D loss: 0.32699 G loss: 2.33403 (0.041 sec/batch, 1577.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:07,163] [train step59981] D loss: 0.32795 G loss: 2.43818 (0.041 sec/batch, 1564.482 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:07,566] [train step59991] D loss: 0.32713 G loss: 2.33408 (0.040 sec/batch, 1597.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:07,948] [train step60000] D loss: 0.32718 G loss: 2.26685 (0.038 sec/batch, 1666.432 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:48:07,948] Saved checkpoint at 60000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:08,570] [train step60010] D loss: 0.32670 G loss: 2.36047 (0.048 sec/batch, 1336.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:08,972] [train step60020] D loss: 0.32716 G loss: 2.21690 (0.039 sec/batch, 1652.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:09,361] [train step60030] D loss: 0.32811 G loss: 2.45210 (0.044 sec/batch, 1471.129 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:09,754] [train step60041] D loss: 0.32842 G loss: 2.17455 (0.041 sec/batch, 1568.102 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:10,142] [train step60050] D loss: 0.32701 G loss: 2.35741 (0.042 sec/batch, 1514.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:10,529] [train step60060] D loss: 0.32725 G loss: 2.32772 (0.047 sec/batch, 1371.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:10,926] [train step60071] D loss: 0.32698 G loss: 2.26634 (0.042 sec/batch, 1514.565 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:11,320] [train step60080] D loss: 0.32718 G loss: 2.37539 (0.038 sec/batch, 1698.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:11,713] [train step60090] D loss: 0.32830 G loss: 2.19254 (0.041 sec/batch, 1576.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:12,097] [train step60100] D loss: 0.32751 G loss: 2.42842 (0.039 sec/batch, 1634.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:12,485] [train step60110] D loss: 0.32700 G loss: 2.28511 (0.037 sec/batch, 1747.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:12,882] [train step60120] D loss: 0.32711 G loss: 2.36300 (0.039 sec/batch, 1623.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:13,264] [train step60131] D loss: 0.32698 G loss: 2.28418 (0.036 sec/batch, 1766.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:13,654] [train step60140] D loss: 0.32690 G loss: 2.31029 (0.042 sec/batch, 1510.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:14,034] [train step60150] D loss: 0.32683 G loss: 2.27695 (0.039 sec/batch, 1650.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:14,406] [train step60160] D loss: 0.32665 G loss: 2.33347 (0.036 sec/batch, 1764.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:14,793] [train step60171] D loss: 0.32751 G loss: 2.42368 (0.031 sec/batch, 2064.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:15,174] [train step60180] D loss: 0.32653 G loss: 2.25484 (0.040 sec/batch, 1603.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:15,552] [train step60190] D loss: 0.32709 G loss: 2.25195 (0.041 sec/batch, 1573.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:15,942] [train step60200] D loss: 0.32678 G loss: 2.30939 (0.040 sec/batch, 1601.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:16,318] [train step60210] D loss: 0.32714 G loss: 2.31580 (0.039 sec/batch, 1638.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:16,703] [train step60220] D loss: 0.32684 G loss: 2.30875 (0.048 sec/batch, 1343.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:17,090] [train step60231] D loss: 0.32715 G loss: 2.25636 (0.044 sec/batch, 1438.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:17,466] [train step60240] D loss: 0.32661 G loss: 2.33565 (0.033 sec/batch, 1966.618 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:17,858] [train step60251] D loss: 0.32713 G loss: 2.30747 (0.038 sec/batch, 1675.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:18,236] [train step60261] D loss: 0.32745 G loss: 2.21150 (0.036 sec/batch, 1799.286 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:18,615] [train step60270] D loss: 0.32680 G loss: 2.30953 (0.038 sec/batch, 1666.597 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:19,007] [train step60281] D loss: 0.32720 G loss: 2.33523 (0.038 sec/batch, 1697.508 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:19,407] [train step60290] D loss: 0.32644 G loss: 2.31954 (0.041 sec/batch, 1564.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:19,784] [train step60300] D loss: 0.32654 G loss: 2.33021 (0.040 sec/batch, 1583.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:20,171] [train step60310] D loss: 0.32682 G loss: 2.34108 (0.035 sec/batch, 1805.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:20,559] [train step60320] D loss: 0.32649 G loss: 2.32163 (0.038 sec/batch, 1688.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:20,944] [train step60330] D loss: 0.32679 G loss: 2.29434 (0.038 sec/batch, 1698.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:21,333] [train step60340] D loss: 0.32727 G loss: 2.39150 (0.040 sec/batch, 1580.948 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:21,717] [train step60351] D loss: 0.32742 G loss: 2.25026 (0.035 sec/batch, 1823.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:22,106] [train step60360] D loss: 0.32723 G loss: 2.37475 (0.038 sec/batch, 1689.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:22,503] [train step60371] D loss: 0.32714 G loss: 2.35612 (0.035 sec/batch, 1815.237 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:22,897] [train step60381] D loss: 0.32732 G loss: 2.36839 (0.044 sec/batch, 1466.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:23,282] [train step60390] D loss: 0.32729 G loss: 2.38028 (0.039 sec/batch, 1626.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:23,661] [train step60401] D loss: 0.32669 G loss: 2.27864 (0.034 sec/batch, 1906.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:24,061] [train step60411] D loss: 0.32702 G loss: 2.22467 (0.038 sec/batch, 1688.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:24,438] [train step60420] D loss: 0.32640 G loss: 2.33072 (0.032 sec/batch, 1993.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:24,831] [train step60430] D loss: 0.32711 G loss: 2.34338 (0.043 sec/batch, 1471.944 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:25,236] [train step60440] D loss: 0.32683 G loss: 2.29282 (0.041 sec/batch, 1556.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:25,629] [train step60450] D loss: 0.32718 G loss: 2.31738 (0.038 sec/batch, 1672.735 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:26,022] [train step60461] D loss: 0.32748 G loss: 2.20090 (0.038 sec/batch, 1687.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:26,405] [train step60471] D loss: 0.32725 G loss: 2.21850 (0.036 sec/batch, 1772.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:26,806] [train step60480] D loss: 0.32670 G loss: 2.31714 (0.039 sec/batch, 1645.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:27,195] [train step60490] D loss: 0.32703 G loss: 2.25969 (0.038 sec/batch, 1676.726 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:27,580] [train step60501] D loss: 0.32631 G loss: 2.27940 (0.037 sec/batch, 1746.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:27,963] [train step60510] D loss: 0.32683 G loss: 2.26348 (0.036 sec/batch, 1756.018 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:28,363] [train step60520] D loss: 0.32649 G loss: 2.32323 (0.044 sec/batch, 1445.229 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:28,755] [train step60530] D loss: 0.32664 G loss: 2.27179 (0.039 sec/batch, 1635.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:29,155] [train step60540] D loss: 0.32667 G loss: 2.25189 (0.044 sec/batch, 1458.849 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:29,540] [train step60551] D loss: 0.32701 G loss: 2.35556 (0.038 sec/batch, 1703.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:29,943] [train step60560] D loss: 0.32643 G loss: 2.25881 (0.036 sec/batch, 1791.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:30,336] [train step60570] D loss: 0.32642 G loss: 2.34544 (0.037 sec/batch, 1720.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:30,721] [train step60581] D loss: 0.32649 G loss: 2.37259 (0.036 sec/batch, 1753.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:31,117] [train step60591] D loss: 0.32683 G loss: 2.28806 (0.046 sec/batch, 1382.882 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:31,500] [train step60600] D loss: 0.32674 G loss: 2.33085 (0.036 sec/batch, 1764.386 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:31,887] [train step60610] D loss: 0.32687 G loss: 2.33263 (0.038 sec/batch, 1677.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:32,286] [train step60621] D loss: 0.32742 G loss: 2.23574 (0.037 sec/batch, 1712.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:32,674] [train step60630] D loss: 0.32748 G loss: 2.18943 (0.038 sec/batch, 1704.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:33,057] [train step60641] D loss: 0.32664 G loss: 2.25425 (0.039 sec/batch, 1647.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:33,461] [train step60650] D loss: 0.34625 G loss: 2.03038 (0.042 sec/batch, 1517.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:33,847] [train step60660] D loss: 0.67316 G loss: 0.78872 (0.043 sec/batch, 1490.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:34,247] [train step60670] D loss: 0.99242 G loss: 9.89531 (0.039 sec/batch, 1658.514 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:34,635] [train step60681] D loss: 0.62164 G loss: 6.10493 (0.042 sec/batch, 1540.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:35,020] [train step60690] D loss: 0.38012 G loss: 3.42643 (0.034 sec/batch, 1906.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:35,429] [train step60700] D loss: 0.33703 G loss: 2.18895 (0.040 sec/batch, 1609.962 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:35,818] [train step60710] D loss: 0.33680 G loss: 2.65026 (0.040 sec/batch, 1582.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:36,216] [train step60720] D loss: 0.33148 G loss: 2.22584 (0.048 sec/batch, 1334.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:36,608] [train step60731] D loss: 0.33103 G loss: 2.35342 (0.039 sec/batch, 1641.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:36,990] [train step60740] D loss: 0.33073 G loss: 2.37488 (0.038 sec/batch, 1688.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:37,390] [train step60750] D loss: 0.33008 G loss: 2.32770 (0.038 sec/batch, 1683.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:37,798] [train step60761] D loss: 0.32917 G loss: 2.37756 (0.038 sec/batch, 1690.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:38,187] [train step60770] D loss: 0.32937 G loss: 2.31353 (0.040 sec/batch, 1618.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:38,591] [train step60780] D loss: 0.32923 G loss: 2.33111 (0.043 sec/batch, 1494.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:38,988] [train step60791] D loss: 0.32990 G loss: 2.22033 (0.039 sec/batch, 1620.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:39,389] [train step60800] D loss: 0.32915 G loss: 2.34281 (0.046 sec/batch, 1390.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:39,780] [train step60810] D loss: 0.32938 G loss: 2.35757 (0.043 sec/batch, 1496.888 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:40,173] [train step60820] D loss: 0.32875 G loss: 2.27726 (0.040 sec/batch, 1619.520 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:40,583] [train step60831] D loss: 0.32885 G loss: 2.26554 (0.039 sec/batch, 1643.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:40,968] [train step60840] D loss: 0.32940 G loss: 2.37434 (0.036 sec/batch, 1759.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:41,360] [train step60851] D loss: 0.32923 G loss: 2.33029 (0.044 sec/batch, 1466.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:41,756] [train step60860] D loss: 0.34209 G loss: 2.85372 (0.040 sec/batch, 1593.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:42,147] [train step60870] D loss: 0.33225 G loss: 2.06631 (0.042 sec/batch, 1537.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:42,546] [train step60881] D loss: 0.32944 G loss: 2.42082 (0.039 sec/batch, 1628.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:42,936] [train step60891] D loss: 0.32858 G loss: 2.40275 (0.037 sec/batch, 1739.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:43,332] [train step60900] D loss: 0.32843 G loss: 2.39476 (0.038 sec/batch, 1663.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:43,725] [train step60911] D loss: 0.32817 G loss: 2.40461 (0.034 sec/batch, 1873.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:44,106] [train step60921] D loss: 0.32909 G loss: 2.20034 (0.036 sec/batch, 1788.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:44,497] [train step60930] D loss: 0.32808 G loss: 2.41472 (0.039 sec/batch, 1662.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:44,867] [train step60940] D loss: 0.32760 G loss: 2.28215 (0.030 sec/batch, 2153.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:45,256] [train step60951] D loss: 0.32759 G loss: 2.31603 (0.034 sec/batch, 1881.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:45,652] [train step60960] D loss: 0.32851 G loss: 2.22410 (0.040 sec/batch, 1596.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:46,030] [train step60971] D loss: 0.32986 G loss: 2.51934 (0.041 sec/batch, 1559.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:46,422] [train step60980] D loss: 0.32889 G loss: 2.19041 (0.044 sec/batch, 1442.286 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:46,799] [train step60990] D loss: 0.32902 G loss: 2.45653 (0.030 sec/batch, 2123.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:47,184] [train step61000] D loss: 0.32728 G loss: 2.27237 (0.036 sec/batch, 1756.949 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:48:47,184] Saved checkpoint at 61000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:47,818] [train step61011] D loss: 0.32862 G loss: 2.25669 (0.044 sec/batch, 1470.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:48,212] [train step61020] D loss: 0.32868 G loss: 2.21792 (0.039 sec/batch, 1644.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:48,611] [train step61031] D loss: 0.32733 G loss: 2.33067 (0.041 sec/batch, 1567.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:49,001] [train step61040] D loss: 0.32755 G loss: 2.31882 (0.037 sec/batch, 1712.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:49,390] [train step61050] D loss: 0.32832 G loss: 2.28715 (0.038 sec/batch, 1679.191 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:49,794] [train step61060] D loss: 0.32904 G loss: 2.30345 (0.043 sec/batch, 1496.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:50,197] [train step61070] D loss: 0.32735 G loss: 2.31652 (0.039 sec/batch, 1632.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:50,588] [train step61080] D loss: 0.32769 G loss: 2.29603 (0.038 sec/batch, 1669.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:50,986] [train step61090] D loss: 0.32822 G loss: 2.27010 (0.041 sec/batch, 1578.800 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:51,406] [train step61101] D loss: 0.32819 G loss: 2.25940 (0.038 sec/batch, 1690.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:51,808] [train step61110] D loss: 0.32808 G loss: 2.38755 (0.042 sec/batch, 1517.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:52,192] [train step61120] D loss: 0.32769 G loss: 2.34471 (0.039 sec/batch, 1642.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:52,581] [train step61130] D loss: 0.32758 G loss: 2.32777 (0.040 sec/batch, 1584.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:52,986] [train step61140] D loss: 0.32738 G loss: 2.26456 (0.038 sec/batch, 1692.809 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:53,377] [train step61150] D loss: 0.32752 G loss: 2.32286 (0.038 sec/batch, 1692.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:53,755] [train step61160] D loss: 0.32757 G loss: 2.34105 (0.027 sec/batch, 2366.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:54,151] [train step61170] D loss: 0.32816 G loss: 2.32558 (0.041 sec/batch, 1574.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:54,535] [train step61180] D loss: 0.32789 G loss: 2.28793 (0.041 sec/batch, 1563.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:54,914] [train step61190] D loss: 0.32778 G loss: 2.31755 (0.034 sec/batch, 1888.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:55,308] [train step61200] D loss: 0.32772 G loss: 2.28260 (0.039 sec/batch, 1625.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:55,696] [train step61210] D loss: 0.32760 G loss: 2.30646 (0.040 sec/batch, 1603.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:56,081] [train step61221] D loss: 0.32756 G loss: 2.30402 (0.039 sec/batch, 1645.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:56,464] [train step61230] D loss: 0.32814 G loss: 2.37735 (0.038 sec/batch, 1696.618 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:56,859] [train step61240] D loss: 0.32765 G loss: 2.30712 (0.041 sec/batch, 1578.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:57,257] [train step61250] D loss: 0.32732 G loss: 2.30553 (0.040 sec/batch, 1611.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:57,637] [train step61260] D loss: 0.32764 G loss: 2.24660 (0.033 sec/batch, 1918.643 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:58,027] [train step61270] D loss: 0.32774 G loss: 2.28538 (0.032 sec/batch, 1974.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:58,418] [train step61281] D loss: 0.32799 G loss: 2.30826 (0.039 sec/batch, 1657.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:58,812] [train step61290] D loss: 0.32760 G loss: 2.27143 (0.038 sec/batch, 1689.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:59,206] [train step61301] D loss: 0.32757 G loss: 2.27526 (0.039 sec/batch, 1639.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:59,593] [train step61311] D loss: 0.32830 G loss: 2.43064 (0.040 sec/batch, 1614.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:48:59,991] [train step61320] D loss: 0.32721 G loss: 2.29144 (0.037 sec/batch, 1738.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:00,380] [train step61331] D loss: 0.32727 G loss: 2.31875 (0.042 sec/batch, 1508.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:00,780] [train step61341] D loss: 0.32735 G loss: 2.33925 (0.040 sec/batch, 1587.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:01,183] [train step61350] D loss: 0.32813 G loss: 2.29774 (0.036 sec/batch, 1773.349 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:01,583] [train step61361] D loss: 0.32747 G loss: 2.32117 (0.050 sec/batch, 1289.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:01,979] [train step61371] D loss: 0.32803 G loss: 2.45689 (0.035 sec/batch, 1813.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:02,378] [train step61380] D loss: 0.32789 G loss: 2.39977 (0.035 sec/batch, 1813.643 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:02,764] [train step61390] D loss: 0.32730 G loss: 2.38790 (0.037 sec/batch, 1722.407 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:03,162] [train step61401] D loss: 0.32781 G loss: 2.22557 (0.041 sec/batch, 1577.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:03,552] [train step61410] D loss: 0.32744 G loss: 2.30346 (0.039 sec/batch, 1641.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:03,932] [train step61420] D loss: 0.32679 G loss: 2.27708 (0.039 sec/batch, 1621.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:04,324] [train step61431] D loss: 0.32714 G loss: 2.31184 (0.039 sec/batch, 1646.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:04,703] [train step61440] D loss: 0.33161 G loss: 2.04279 (0.041 sec/batch, 1575.409 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:05,096] [train step61451] D loss: 0.37830 G loss: 3.48057 (0.038 sec/batch, 1694.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:05,486] [train step61461] D loss: 0.51239 G loss: 5.04319 (0.037 sec/batch, 1714.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:05,879] [train step61470] D loss: 0.38968 G loss: 3.62269 (0.039 sec/batch, 1643.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:06,274] [train step61480] D loss: 0.33176 G loss: 2.43400 (0.038 sec/batch, 1706.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:06,659] [train step61491] D loss: 0.33086 G loss: 2.28468 (0.040 sec/batch, 1616.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:07,047] [train step61500] D loss: 0.33962 G loss: 2.01231 (0.040 sec/batch, 1618.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:07,447] [train step61511] D loss: 0.32906 G loss: 2.36736 (0.038 sec/batch, 1684.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:07,827] [train step61521] D loss: 0.34235 G loss: 2.85206 (0.034 sec/batch, 1908.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:08,226] [train step61530] D loss: 0.32994 G loss: 2.28932 (0.035 sec/batch, 1853.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:08,621] [train step61540] D loss: 0.33044 G loss: 2.38427 (0.043 sec/batch, 1487.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:09,001] [train step61550] D loss: 0.33132 G loss: 2.21877 (0.040 sec/batch, 1603.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:09,396] [train step61560] D loss: 0.32948 G loss: 2.29761 (0.037 sec/batch, 1714.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:09,782] [train step61570] D loss: 0.32935 G loss: 2.36548 (0.041 sec/batch, 1570.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:10,177] [train step61580] D loss: 0.33050 G loss: 2.38812 (0.040 sec/batch, 1591.285 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:10,562] [train step61590] D loss: 0.32989 G loss: 2.23703 (0.040 sec/batch, 1618.807 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:10,949] [train step61601] D loss: 0.33024 G loss: 2.25982 (0.036 sec/batch, 1779.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:11,341] [train step61611] D loss: 0.33017 G loss: 2.42128 (0.036 sec/batch, 1790.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:11,724] [train step61620] D loss: 0.32985 G loss: 2.23212 (0.037 sec/batch, 1744.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:12,109] [train step61630] D loss: 0.33103 G loss: 2.28275 (0.035 sec/batch, 1805.312 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:12,529] [train step61640] D loss: 0.33068 G loss: 2.50721 (0.041 sec/batch, 1578.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:12,915] [train step61650] D loss: 0.32896 G loss: 2.35634 (0.039 sec/batch, 1635.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:13,313] [train step61661] D loss: 0.32953 G loss: 2.39516 (0.037 sec/batch, 1731.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:13,699] [train step61671] D loss: 0.32929 G loss: 2.24542 (0.039 sec/batch, 1629.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:14,079] [train step61680] D loss: 0.32905 G loss: 2.42769 (0.028 sec/batch, 2294.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:14,471] [train step61690] D loss: 0.33018 G loss: 2.21432 (0.031 sec/batch, 2056.646 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:14,869] [train step61701] D loss: 0.32900 G loss: 2.27038 (0.037 sec/batch, 1729.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:15,238] [train step61710] D loss: 0.32947 G loss: 2.29592 (0.037 sec/batch, 1731.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:15,628] [train step61720] D loss: 0.32877 G loss: 2.42584 (0.042 sec/batch, 1516.576 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:16,005] [train step61731] D loss: 0.32908 G loss: 2.42638 (0.036 sec/batch, 1787.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:16,388] [train step61740] D loss: 0.32858 G loss: 2.30492 (0.037 sec/batch, 1722.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:16,762] [train step61751] D loss: 0.32919 G loss: 2.45613 (0.034 sec/batch, 1892.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:17,151] [train step61760] D loss: 0.32947 G loss: 2.40313 (0.047 sec/batch, 1372.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:17,553] [train step61770] D loss: 0.32952 G loss: 2.23689 (0.041 sec/batch, 1570.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:17,937] [train step61780] D loss: 0.32843 G loss: 2.29288 (0.039 sec/batch, 1658.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:18,326] [train step61791] D loss: 0.32918 G loss: 2.20861 (0.037 sec/batch, 1717.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:18,728] [train step61800] D loss: 0.32785 G loss: 2.33734 (0.037 sec/batch, 1736.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:19,108] [train step61811] D loss: 0.32922 G loss: 2.21372 (0.035 sec/batch, 1838.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:19,500] [train step61820] D loss: 0.32842 G loss: 2.22648 (0.038 sec/batch, 1688.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:19,879] [train step61830] D loss: 0.32865 G loss: 2.27614 (0.043 sec/batch, 1491.217 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:20,259] [train step61841] D loss: 0.32753 G loss: 2.26128 (0.037 sec/batch, 1736.738 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:20,644] [train step61850] D loss: 0.32863 G loss: 2.41405 (0.037 sec/batch, 1722.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:21,022] [train step61860] D loss: 0.32875 G loss: 2.37180 (0.037 sec/batch, 1735.066 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:21,396] [train step61871] D loss: 0.32711 G loss: 2.28950 (0.037 sec/batch, 1749.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:21,781] [train step61881] D loss: 0.32749 G loss: 2.30822 (0.036 sec/batch, 1801.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:22,157] [train step61890] D loss: 0.32861 G loss: 2.41169 (0.035 sec/batch, 1825.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:22,550] [train step61901] D loss: 0.32692 G loss: 2.34647 (0.038 sec/batch, 1691.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:22,955] [train step61910] D loss: 0.32791 G loss: 2.20258 (0.040 sec/batch, 1593.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:23,334] [train step61920] D loss: 0.32723 G loss: 2.34024 (0.037 sec/batch, 1728.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:23,731] [train step61931] D loss: 0.32788 G loss: 2.18229 (0.038 sec/batch, 1681.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:24,123] [train step61940] D loss: 0.32729 G loss: 2.35240 (0.039 sec/batch, 1658.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:24,507] [train step61950] D loss: 0.32770 G loss: 2.26238 (0.039 sec/batch, 1631.430 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:24,905] [train step61961] D loss: 0.32723 G loss: 2.26920 (0.039 sec/batch, 1661.913 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:25,296] [train step61970] D loss: 0.32752 G loss: 2.38785 (0.038 sec/batch, 1685.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:25,686] [train step61980] D loss: 0.32734 G loss: 2.37374 (0.036 sec/batch, 1755.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:26,063] [train step61991] D loss: 0.32796 G loss: 2.44980 (0.038 sec/batch, 1674.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:26,448] [train step62000] D loss: 0.32707 G loss: 2.30811 (0.042 sec/batch, 1515.138 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:49:26,448] Saved checkpoint at 62000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:27,058] [train step62010] D loss: 0.32691 G loss: 2.32640 (0.038 sec/batch, 1696.521 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:27,439] [train step62021] D loss: 0.32730 G loss: 2.38438 (0.037 sec/batch, 1711.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:27,828] [train step62030] D loss: 0.32700 G loss: 2.33887 (0.039 sec/batch, 1643.848 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:28,217] [train step62040] D loss: 0.32705 G loss: 2.27528 (0.038 sec/batch, 1675.700 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:28,598] [train step62051] D loss: 0.32700 G loss: 2.26891 (0.036 sec/batch, 1760.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:28,999] [train step62061] D loss: 0.32665 G loss: 2.26788 (0.041 sec/batch, 1550.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:29,379] [train step62070] D loss: 0.32818 G loss: 2.17304 (0.037 sec/batch, 1736.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:29,783] [train step62081] D loss: 0.32691 G loss: 2.41234 (0.040 sec/batch, 1598.230 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:30,178] [train step62091] D loss: 0.32732 G loss: 2.37287 (0.035 sec/batch, 1807.208 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:30,563] [train step62100] D loss: 0.32727 G loss: 2.34811 (0.036 sec/batch, 1767.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:30,958] [train step62111] D loss: 0.32730 G loss: 2.25437 (0.034 sec/batch, 1897.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:31,348] [train step62121] D loss: 0.32683 G loss: 2.29428 (0.040 sec/batch, 1591.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:31,727] [train step62130] D loss: 0.32701 G loss: 2.25053 (0.035 sec/batch, 1810.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:32,137] [train step62141] D loss: 0.32670 G loss: 2.28326 (0.038 sec/batch, 1681.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:32,532] [train step62151] D loss: 0.32748 G loss: 2.32775 (0.038 sec/batch, 1689.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:32,934] [train step62160] D loss: 0.32676 G loss: 2.27731 (0.038 sec/batch, 1697.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:33,324] [train step62171] D loss: 0.32642 G loss: 2.28623 (0.039 sec/batch, 1630.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:33,723] [train step62180] D loss: 0.32654 G loss: 2.35100 (0.038 sec/batch, 1688.698 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:34,122] [train step62190] D loss: 0.32639 G loss: 2.30807 (0.038 sec/batch, 1683.667 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:34,508] [train step62200] D loss: 0.32677 G loss: 2.28876 (0.038 sec/batch, 1681.157 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:34,905] [train step62210] D loss: 0.32729 G loss: 2.29181 (0.041 sec/batch, 1572.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:35,291] [train step62220] D loss: 0.32700 G loss: 2.23909 (0.037 sec/batch, 1722.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:35,670] [train step62231] D loss: 0.32654 G loss: 2.27019 (0.038 sec/batch, 1690.314 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:36,066] [train step62241] D loss: 0.32678 G loss: 2.30940 (0.036 sec/batch, 1786.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:36,457] [train step62250] D loss: 0.32686 G loss: 2.28626 (0.040 sec/batch, 1610.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:36,841] [train step62260] D loss: 0.32704 G loss: 2.29631 (0.040 sec/batch, 1619.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:37,239] [train step62270] D loss: 0.32682 G loss: 2.32767 (0.036 sec/batch, 1768.035 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:37,615] [train step62280] D loss: 0.32691 G loss: 2.27604 (0.042 sec/batch, 1513.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:38,003] [train step62291] D loss: 0.32711 G loss: 2.35849 (0.037 sec/batch, 1732.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:38,395] [train step62301] D loss: 0.32706 G loss: 2.40401 (0.037 sec/batch, 1716.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:38,774] [train step62310] D loss: 0.32674 G loss: 2.30563 (0.036 sec/batch, 1770.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:39,208] [train step62320] D loss: 0.32700 G loss: 2.39851 (0.038 sec/batch, 1696.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:39,595] [train step62330] D loss: 0.32673 G loss: 2.33529 (0.037 sec/batch, 1724.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:39,989] [train step62340] D loss: 0.32645 G loss: 2.32576 (0.045 sec/batch, 1410.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:40,383] [train step62350] D loss: 0.32692 G loss: 2.24857 (0.035 sec/batch, 1823.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:40,773] [train step62361] D loss: 0.32670 G loss: 2.36823 (0.040 sec/batch, 1617.656 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:41,175] [train step62370] D loss: 0.32778 G loss: 2.15486 (0.042 sec/batch, 1532.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:41,561] [train step62380] D loss: 0.32679 G loss: 2.32337 (0.034 sec/batch, 1887.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:41,950] [train step62390] D loss: 0.32665 G loss: 2.35405 (0.034 sec/batch, 1870.722 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:42,346] [train step62400] D loss: 0.32644 G loss: 2.29457 (0.036 sec/batch, 1785.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:42,739] [train step62411] D loss: 0.32663 G loss: 2.25215 (0.039 sec/batch, 1627.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:43,131] [train step62420] D loss: 0.32675 G loss: 2.35830 (0.037 sec/batch, 1712.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:43,518] [train step62430] D loss: 0.32666 G loss: 2.30051 (0.038 sec/batch, 1703.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:43,904] [train step62441] D loss: 0.32728 G loss: 2.31601 (0.038 sec/batch, 1682.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:44,319] [train step62451] D loss: 0.32690 G loss: 2.34562 (0.038 sec/batch, 1673.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:44,711] [train step62460] D loss: 0.32673 G loss: 2.36113 (0.039 sec/batch, 1648.107 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:45,093] [train step62471] D loss: 0.32699 G loss: 2.38741 (0.035 sec/batch, 1807.427 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:45,484] [train step62481] D loss: 0.32696 G loss: 2.37507 (0.038 sec/batch, 1679.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:45,852] [train step62490] D loss: 0.32744 G loss: 2.20183 (0.035 sec/batch, 1826.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:46,244] [train step62500] D loss: 0.32679 G loss: 2.37949 (0.038 sec/batch, 1687.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:46,624] [train step62511] D loss: 0.32645 G loss: 2.26585 (0.037 sec/batch, 1728.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:47,002] [train step62520] D loss: 0.32676 G loss: 2.31115 (0.035 sec/batch, 1811.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:47,386] [train step62530] D loss: 0.32701 G loss: 2.24507 (0.039 sec/batch, 1648.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:47,771] [train step62541] D loss: 0.32656 G loss: 2.28165 (0.038 sec/batch, 1698.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:48,149] [train step62550] D loss: 0.32658 G loss: 2.30333 (0.034 sec/batch, 1872.614 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:48,547] [train step62560] D loss: 0.32685 G loss: 2.37487 (0.038 sec/batch, 1691.710 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:48,928] [train step62570] D loss: 0.32633 G loss: 2.26492 (0.036 sec/batch, 1761.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:49,320] [train step62580] D loss: 0.32648 G loss: 2.27428 (0.038 sec/batch, 1675.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:49,701] [train step62591] D loss: 0.32712 G loss: 2.35186 (0.036 sec/batch, 1755.547 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:50,079] [train step62600] D loss: 0.32661 G loss: 2.37701 (0.037 sec/batch, 1745.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:50,469] [train step62610] D loss: 0.32680 G loss: 2.28008 (0.038 sec/batch, 1695.353 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:50,843] [train step62621] D loss: 0.32658 G loss: 2.35206 (0.035 sec/batch, 1803.553 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:51,221] [train step62631] D loss: 0.32718 G loss: 2.20919 (0.039 sec/batch, 1647.035 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:51,611] [train step62640] D loss: 0.32637 G loss: 2.31111 (0.039 sec/batch, 1653.610 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:51,993] [train step62651] D loss: 0.32637 G loss: 2.26413 (0.041 sec/batch, 1573.848 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:52,382] [train step62661] D loss: 0.32677 G loss: 2.35579 (0.037 sec/batch, 1740.759 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:52,754] [train step62670] D loss: 0.32659 G loss: 2.36881 (0.033 sec/batch, 1943.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:53,142] [train step62680] D loss: 0.32670 G loss: 2.34199 (0.034 sec/batch, 1898.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:53,534] [train step62690] D loss: 0.32625 G loss: 2.32466 (0.044 sec/batch, 1464.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:53,911] [train step62700] D loss: 0.32660 G loss: 2.25198 (0.038 sec/batch, 1704.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:54,288] [train step62710] D loss: 0.32655 G loss: 2.29416 (0.034 sec/batch, 1856.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:54,700] [train step62721] D loss: 0.32641 G loss: 2.37032 (0.040 sec/batch, 1600.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:55,084] [train step62730] D loss: 0.32669 G loss: 2.22817 (0.045 sec/batch, 1415.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:55,474] [train step62741] D loss: 0.32657 G loss: 2.34195 (0.037 sec/batch, 1735.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:55,859] [train step62750] D loss: 0.32666 G loss: 2.42177 (0.041 sec/batch, 1579.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:56,241] [train step62760] D loss: 0.32635 G loss: 2.27990 (0.036 sec/batch, 1766.790 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:56,638] [train step62771] D loss: 0.32686 G loss: 2.41958 (0.040 sec/batch, 1585.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:57,030] [train step62781] D loss: 0.32699 G loss: 2.27275 (0.039 sec/batch, 1643.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:57,425] [train step62790] D loss: 0.32712 G loss: 2.43123 (0.035 sec/batch, 1807.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:57,826] [train step62800] D loss: 0.32631 G loss: 2.33928 (0.034 sec/batch, 1874.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:58,207] [train step62811] D loss: 0.32640 G loss: 2.33637 (0.035 sec/batch, 1829.065 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:58,614] [train step62820] D loss: 0.32641 G loss: 2.32576 (0.040 sec/batch, 1616.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:58,998] [train step62831] D loss: 0.32700 G loss: 2.42002 (0.033 sec/batch, 1930.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:59,392] [train step62841] D loss: 0.32664 G loss: 2.36018 (0.049 sec/batch, 1315.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:49:59,794] [train step62850] D loss: 0.32668 G loss: 2.34728 (0.039 sec/batch, 1646.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:00,182] [train step62861] D loss: 0.32680 G loss: 2.29084 (0.039 sec/batch, 1649.464 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:00,577] [train step62871] D loss: 0.32627 G loss: 2.28501 (0.044 sec/batch, 1469.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:00,969] [train step62880] D loss: 0.32640 G loss: 2.34028 (0.039 sec/batch, 1648.036 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:01,365] [train step62891] D loss: 0.32649 G loss: 2.23342 (0.039 sec/batch, 1634.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:01,771] [train step62900] D loss: 0.32627 G loss: 2.28457 (0.039 sec/batch, 1655.394 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:02,159] [train step62910] D loss: 0.32640 G loss: 2.28756 (0.040 sec/batch, 1600.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:02,552] [train step62921] D loss: 0.32635 G loss: 2.28331 (0.041 sec/batch, 1554.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:02,945] [train step62930] D loss: 0.32632 G loss: 2.31254 (0.038 sec/batch, 1681.547 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:03,330] [train step62940] D loss: 0.32634 G loss: 2.29980 (0.040 sec/batch, 1597.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:03,733] [train step62950] D loss: 0.32635 G loss: 2.24425 (0.036 sec/batch, 1775.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:04,123] [train step62960] D loss: 0.32618 G loss: 2.29089 (0.037 sec/batch, 1731.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:04,509] [train step62970] D loss: 0.32684 G loss: 2.37191 (0.038 sec/batch, 1705.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:04,911] [train step62980] D loss: 0.32649 G loss: 2.37787 (0.043 sec/batch, 1478.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:05,298] [train step62990] D loss: 0.32635 G loss: 2.32173 (0.037 sec/batch, 1752.201 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:05,712] [train step63000] D loss: 0.32694 G loss: 2.23806 (0.042 sec/batch, 1530.855 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:50:05,713] Saved checkpoint at 63000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:06,330] [train step63010] D loss: 0.32653 G loss: 2.30597 (0.038 sec/batch, 1687.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:06,722] [train step63020] D loss: 0.32678 G loss: 2.38004 (0.043 sec/batch, 1492.876 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:07,111] [train step63030] D loss: 0.32644 G loss: 2.31081 (0.037 sec/batch, 1720.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:07,503] [train step63041] D loss: 0.32691 G loss: 2.41327 (0.036 sec/batch, 1790.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:07,900] [train step63050] D loss: 0.32627 G loss: 2.27423 (0.042 sec/batch, 1514.053 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:08,293] [train step63060] D loss: 0.32633 G loss: 2.33083 (0.037 sec/batch, 1733.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:08,691] [train step63070] D loss: 0.32703 G loss: 2.44298 (0.040 sec/batch, 1617.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:09,090] [train step63080] D loss: 0.32615 G loss: 2.31630 (0.042 sec/batch, 1532.962 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:09,478] [train step63090] D loss: 0.32649 G loss: 2.35070 (0.043 sec/batch, 1504.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:09,877] [train step63101] D loss: 0.32607 G loss: 2.28285 (0.038 sec/batch, 1694.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:10,258] [train step63111] D loss: 0.32633 G loss: 2.29986 (0.032 sec/batch, 2019.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:10,643] [train step63120] D loss: 0.32696 G loss: 2.20953 (0.039 sec/batch, 1655.313 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:11,031] [train step63130] D loss: 0.32620 G loss: 2.26879 (0.038 sec/batch, 1683.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:11,419] [train step63141] D loss: 0.32650 G loss: 2.30956 (0.038 sec/batch, 1694.904 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:11,810] [train step63150] D loss: 0.32631 G loss: 2.26575 (0.038 sec/batch, 1672.505 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:12,208] [train step63160] D loss: 0.32611 G loss: 2.30425 (0.039 sec/batch, 1638.920 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:12,589] [train step63171] D loss: 0.32751 G loss: 2.46310 (0.039 sec/batch, 1627.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:12,990] [train step63180] D loss: 0.32610 G loss: 2.25768 (0.035 sec/batch, 1822.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:13,370] [train step63191] D loss: 0.32604 G loss: 2.29419 (0.041 sec/batch, 1580.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:13,752] [train step63200] D loss: 0.32652 G loss: 2.23834 (0.036 sec/batch, 1772.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:14,154] [train step63210] D loss: 0.32618 G loss: 2.32282 (0.038 sec/batch, 1683.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:14,531] [train step63220] D loss: 0.32687 G loss: 2.20175 (0.031 sec/batch, 2088.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:14,934] [train step63230] D loss: 0.32631 G loss: 2.39422 (0.031 sec/batch, 2050.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:15,331] [train step63240] D loss: 0.32645 G loss: 2.28833 (0.048 sec/batch, 1331.327 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:15,720] [train step63250] D loss: 0.32636 G loss: 2.39972 (0.039 sec/batch, 1629.716 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:16,122] [train step63261] D loss: 0.32594 G loss: 2.26637 (0.035 sec/batch, 1826.054 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:16,491] [train step63270] D loss: 0.32683 G loss: 2.42084 (0.032 sec/batch, 1972.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:16,877] [train step63281] D loss: 0.32629 G loss: 2.30407 (0.042 sec/batch, 1540.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:17,256] [train step63290] D loss: 0.32638 G loss: 2.27907 (0.040 sec/batch, 1581.302 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:17,631] [train step63300] D loss: 0.32621 G loss: 2.30557 (0.037 sec/batch, 1743.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:18,007] [train step63311] D loss: 0.32625 G loss: 2.33815 (0.038 sec/batch, 1700.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:18,390] [train step63321] D loss: 0.32636 G loss: 2.22714 (0.036 sec/batch, 1794.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:18,769] [train step63330] D loss: 0.32626 G loss: 2.33856 (0.035 sec/batch, 1821.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:19,161] [train step63341] D loss: 0.32667 G loss: 2.43421 (0.036 sec/batch, 1774.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:19,539] [train step63351] D loss: 0.32620 G loss: 2.33558 (0.039 sec/batch, 1651.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:19,916] [train step63360] D loss: 0.32618 G loss: 2.35033 (0.041 sec/batch, 1573.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:20,314] [train step63370] D loss: 0.32606 G loss: 2.27746 (0.040 sec/batch, 1620.155 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:20,690] [train step63380] D loss: 0.32603 G loss: 2.25293 (0.033 sec/batch, 1911.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:21,075] [train step63390] D loss: 0.32663 G loss: 2.22034 (0.037 sec/batch, 1711.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:21,463] [train step63400] D loss: 0.32612 G loss: 2.30788 (0.040 sec/batch, 1607.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:21,836] [train step63411] D loss: 0.32635 G loss: 2.33449 (0.035 sec/batch, 1814.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:22,232] [train step63420] D loss: 0.32636 G loss: 2.36928 (0.036 sec/batch, 1778.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:22,603] [train step63431] D loss: 0.32635 G loss: 2.36471 (0.034 sec/batch, 1859.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:22,973] [train step63441] D loss: 0.32628 G loss: 2.26851 (0.036 sec/batch, 1759.782 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:23,374] [train step63450] D loss: 0.32631 G loss: 2.34778 (0.040 sec/batch, 1586.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:23,759] [train step63460] D loss: 0.32615 G loss: 2.28951 (0.036 sec/batch, 1791.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:24,140] [train step63470] D loss: 0.32638 G loss: 2.31523 (0.035 sec/batch, 1812.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:24,535] [train step63480] D loss: 0.32592 G loss: 2.29809 (0.042 sec/batch, 1521.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:24,922] [train step63490] D loss: 0.32615 G loss: 2.36620 (0.039 sec/batch, 1639.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:25,309] [train step63500] D loss: 0.32628 G loss: 2.25789 (0.035 sec/batch, 1814.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:25,698] [train step63510] D loss: 0.32636 G loss: 2.24686 (0.036 sec/batch, 1754.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:26,076] [train step63521] D loss: 0.32649 G loss: 2.26835 (0.037 sec/batch, 1708.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:26,486] [train step63531] D loss: 0.32619 G loss: 2.35851 (0.041 sec/batch, 1553.122 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:26,874] [train step63540] D loss: 0.32611 G loss: 2.30267 (0.040 sec/batch, 1588.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:27,261] [train step63550] D loss: 0.32590 G loss: 2.27989 (0.041 sec/batch, 1572.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:27,657] [train step63561] D loss: 0.32674 G loss: 2.22593 (0.038 sec/batch, 1670.434 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:28,038] [train step63570] D loss: 0.32616 G loss: 2.32394 (0.041 sec/batch, 1557.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:28,432] [train step63581] D loss: 0.32650 G loss: 2.23899 (0.039 sec/batch, 1646.409 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:28,810] [train step63591] D loss: 0.32631 G loss: 2.30434 (0.037 sec/batch, 1718.394 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:29,205] [train step63600] D loss: 0.32646 G loss: 2.20293 (0.037 sec/batch, 1718.416 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:29,596] [train step63611] D loss: 0.32611 G loss: 2.36696 (0.044 sec/batch, 1467.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:29,984] [train step63621] D loss: 0.32615 G loss: 2.28717 (0.040 sec/batch, 1599.944 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:30,367] [train step63630] D loss: 0.32586 G loss: 2.28716 (0.039 sec/batch, 1660.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:30,773] [train step63640] D loss: 0.32612 G loss: 2.25868 (0.037 sec/batch, 1739.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:31,154] [train step63651] D loss: 0.32603 G loss: 2.29694 (0.039 sec/batch, 1621.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:31,541] [train step63660] D loss: 0.32599 G loss: 2.29698 (0.035 sec/batch, 1827.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:31,931] [train step63670] D loss: 0.32645 G loss: 2.34943 (0.038 sec/batch, 1694.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:32,318] [train step63680] D loss: 0.32606 G loss: 2.29174 (0.037 sec/batch, 1736.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:32,719] [train step63690] D loss: 0.32621 G loss: 2.35653 (0.043 sec/batch, 1505.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:33,118] [train step63701] D loss: 0.32631 G loss: 2.35418 (0.045 sec/batch, 1430.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:33,510] [train step63710] D loss: 0.32607 G loss: 2.29177 (0.037 sec/batch, 1724.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:33,912] [train step63720] D loss: 0.32647 G loss: 2.26724 (0.039 sec/batch, 1625.384 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:34,307] [train step63731] D loss: 0.32588 G loss: 2.29313 (0.040 sec/batch, 1612.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:34,702] [train step63740] D loss: 0.32622 G loss: 2.24217 (0.041 sec/batch, 1574.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:35,078] [train step63750] D loss: 0.32598 G loss: 2.31147 (0.038 sec/batch, 1667.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:35,467] [train step63760] D loss: 0.32576 G loss: 2.32955 (0.041 sec/batch, 1568.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:35,868] [train step63770] D loss: 0.32593 G loss: 2.34781 (0.041 sec/batch, 1577.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:36,261] [train step63780] D loss: 0.32615 G loss: 2.34905 (0.037 sec/batch, 1716.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:36,650] [train step63791] D loss: 0.32616 G loss: 2.30063 (0.037 sec/batch, 1738.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:37,061] [train step63800] D loss: 0.32602 G loss: 2.26740 (0.039 sec/batch, 1637.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:37,446] [train step63810] D loss: 0.32614 G loss: 2.35091 (0.036 sec/batch, 1759.794 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:37,834] [train step63820] D loss: 0.32574 G loss: 2.30645 (0.034 sec/batch, 1902.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:38,230] [train step63831] D loss: 0.32625 G loss: 2.30230 (0.037 sec/batch, 1747.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:38,631] [train step63840] D loss: 0.32632 G loss: 2.27350 (0.042 sec/batch, 1532.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:39,022] [train step63850] D loss: 0.32629 G loss: 2.26157 (0.037 sec/batch, 1726.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:39,413] [train step63860] D loss: 0.32612 G loss: 2.35708 (0.043 sec/batch, 1494.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:39,821] [train step63870] D loss: 0.32628 G loss: 2.26829 (0.036 sec/batch, 1799.105 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:40,216] [train step63881] D loss: 0.32645 G loss: 2.38107 (0.041 sec/batch, 1575.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:40,612] [train step63891] D loss: 0.32691 G loss: 2.44549 (0.037 sec/batch, 1731.719 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:41,005] [train step63900] D loss: 0.32584 G loss: 2.34389 (0.033 sec/batch, 1923.662 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:41,406] [train step63911] D loss: 0.32612 G loss: 2.36258 (0.039 sec/batch, 1645.420 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:41,797] [train step63921] D loss: 0.32595 G loss: 2.26861 (0.040 sec/batch, 1581.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:42,178] [train step63930] D loss: 0.32595 G loss: 2.35687 (0.037 sec/batch, 1710.554 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:42,563] [train step63941] D loss: 0.32579 G loss: 2.28335 (0.038 sec/batch, 1697.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:42,959] [train step63951] D loss: 0.32609 G loss: 2.32977 (0.037 sec/batch, 1732.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:43,342] [train step63960] D loss: 0.32572 G loss: 2.30759 (0.036 sec/batch, 1784.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:43,736] [train step63970] D loss: 0.32634 G loss: 2.37959 (0.043 sec/batch, 1478.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:44,130] [train step63981] D loss: 0.32629 G loss: 2.25514 (0.040 sec/batch, 1586.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:44,517] [train step63990] D loss: 0.32609 G loss: 2.30789 (0.038 sec/batch, 1700.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:44,908] [train step64001] D loss: 0.32604 G loss: 2.27149 (0.036 sec/batch, 1762.498 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:50:44,908] Saved checkpoint at 64000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:45,529] [train step64010] D loss: 0.32635 G loss: 2.38036 (0.038 sec/batch, 1682.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:45,931] [train step64020] D loss: 0.32630 G loss: 2.23428 (0.040 sec/batch, 1582.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:46,324] [train step64030] D loss: 0.32603 G loss: 2.25785 (0.035 sec/batch, 1836.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:46,703] [train step64041] D loss: 0.32589 G loss: 2.30876 (0.040 sec/batch, 1608.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:47,086] [train step64050] D loss: 0.32618 G loss: 2.34332 (0.036 sec/batch, 1769.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:47,463] [train step64061] D loss: 0.32598 G loss: 2.36392 (0.039 sec/batch, 1652.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:47,862] [train step64070] D loss: 0.32609 G loss: 2.24685 (0.047 sec/batch, 1349.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:48,245] [train step64080] D loss: 0.32606 G loss: 2.34935 (0.038 sec/batch, 1687.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:48,624] [train step64091] D loss: 0.32596 G loss: 2.29556 (0.037 sec/batch, 1729.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:48,998] [train step64101] D loss: 0.32597 G loss: 2.36796 (0.035 sec/batch, 1818.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:49,384] [train step64110] D loss: 0.32586 G loss: 2.28763 (0.039 sec/batch, 1654.690 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:49,755] [train step64120] D loss: 0.32611 G loss: 2.26585 (0.037 sec/batch, 1748.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:50,149] [train step64131] D loss: 0.32647 G loss: 2.21311 (0.044 sec/batch, 1469.068 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:50,540] [train step64140] D loss: 0.32584 G loss: 2.35529 (0.049 sec/batch, 1309.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:50,925] [train step64151] D loss: 0.32592 G loss: 2.27644 (0.037 sec/batch, 1741.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:51,306] [train step64160] D loss: 0.32589 G loss: 2.28793 (0.035 sec/batch, 1839.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:51,685] [train step64170] D loss: 0.32578 G loss: 2.28696 (0.033 sec/batch, 1915.959 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:52,073] [train step64180] D loss: 0.32594 G loss: 2.34898 (0.037 sec/batch, 1718.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:52,461] [train step64191] D loss: 0.32593 G loss: 2.35392 (0.040 sec/batch, 1597.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:52,835] [train step64200] D loss: 0.32592 G loss: 2.28864 (0.045 sec/batch, 1418.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:53,229] [train step64210] D loss: 0.32595 G loss: 2.26389 (0.037 sec/batch, 1723.646 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:53,613] [train step64220] D loss: 0.32601 G loss: 2.35387 (0.049 sec/batch, 1317.539 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:53,990] [train step64230] D loss: 0.32585 G loss: 2.35262 (0.040 sec/batch, 1605.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:54,384] [train step64241] D loss: 0.32606 G loss: 2.32641 (0.034 sec/batch, 1870.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:54,767] [train step64251] D loss: 0.32588 G loss: 2.28909 (0.040 sec/batch, 1601.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:55,167] [train step64260] D loss: 0.32590 G loss: 2.31850 (0.036 sec/batch, 1765.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:55,557] [train step64271] D loss: 0.32637 G loss: 2.38472 (0.037 sec/batch, 1743.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:55,935] [train step64280] D loss: 0.32605 G loss: 2.31480 (0.042 sec/batch, 1512.176 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:56,330] [train step64290] D loss: 0.32594 G loss: 2.30214 (0.041 sec/batch, 1559.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:56,711] [train step64300] D loss: 0.32573 G loss: 2.30752 (0.038 sec/batch, 1699.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:57,095] [train step64310] D loss: 0.32599 G loss: 2.33780 (0.036 sec/batch, 1786.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:57,486] [train step64320] D loss: 0.32654 G loss: 2.41411 (0.036 sec/batch, 1764.514 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:57,873] [train step64330] D loss: 0.32602 G loss: 2.34095 (0.040 sec/batch, 1612.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:58,280] [train step64340] D loss: 0.32579 G loss: 2.31875 (0.030 sec/batch, 2106.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:58,671] [train step64350] D loss: 0.32589 G loss: 2.34217 (0.038 sec/batch, 1674.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:59,053] [train step64361] D loss: 0.32597 G loss: 2.25747 (0.036 sec/batch, 1770.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:59,447] [train step64370] D loss: 0.32603 G loss: 2.35209 (0.039 sec/batch, 1658.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:50:59,836] [train step64380] D loss: 0.32593 G loss: 2.29430 (0.036 sec/batch, 1759.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:00,237] [train step64390] D loss: 0.32609 G loss: 2.38364 (0.049 sec/batch, 1296.884 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:00,629] [train step64401] D loss: 0.32591 G loss: 2.27526 (0.040 sec/batch, 1580.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:01,015] [train step64410] D loss: 0.32581 G loss: 2.28348 (0.036 sec/batch, 1773.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:01,412] [train step64420] D loss: 0.32601 G loss: 2.35713 (0.038 sec/batch, 1667.228 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:01,800] [train step64431] D loss: 0.32596 G loss: 2.28256 (0.037 sec/batch, 1742.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:02,189] [train step64440] D loss: 0.32588 G loss: 2.31161 (0.041 sec/batch, 1559.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:02,589] [train step64451] D loss: 0.32585 G loss: 2.32015 (0.037 sec/batch, 1711.382 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:02,983] [train step64460] D loss: 0.32576 G loss: 2.29460 (0.044 sec/batch, 1446.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:03,363] [train step64470] D loss: 0.32584 G loss: 2.33980 (0.030 sec/batch, 2162.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:03,759] [train step64481] D loss: 0.32598 G loss: 2.27283 (0.038 sec/batch, 1690.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:04,155] [train step64491] D loss: 0.32592 G loss: 2.31622 (0.037 sec/batch, 1714.344 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:04,543] [train step64500] D loss: 0.32603 G loss: 2.25856 (0.036 sec/batch, 1769.585 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:04,931] [train step64510] D loss: 0.32585 G loss: 2.35443 (0.042 sec/batch, 1521.036 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:05,329] [train step64521] D loss: 0.32570 G loss: 2.30683 (0.042 sec/batch, 1516.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:05,713] [train step64530] D loss: 0.32581 G loss: 2.29512 (0.039 sec/batch, 1634.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:06,102] [train step64541] D loss: 0.32613 G loss: 2.25819 (0.042 sec/batch, 1538.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:06,491] [train step64551] D loss: 0.32569 G loss: 2.29282 (0.037 sec/batch, 1738.819 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:06,884] [train step64560] D loss: 0.32591 G loss: 2.33010 (0.038 sec/batch, 1672.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:07,271] [train step64571] D loss: 0.32568 G loss: 2.33169 (0.034 sec/batch, 1892.710 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:07,676] [train step64580] D loss: 0.32566 G loss: 2.29713 (0.042 sec/batch, 1521.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:08,066] [train step64590] D loss: 0.32591 G loss: 2.35354 (0.035 sec/batch, 1829.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:08,458] [train step64600] D loss: 0.32594 G loss: 2.28885 (0.040 sec/batch, 1618.690 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:08,850] [train step64611] D loss: 0.32579 G loss: 2.30353 (0.044 sec/batch, 1443.885 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:09,247] [train step64620] D loss: 0.32565 G loss: 2.31911 (0.037 sec/batch, 1740.815 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:09,647] [train step64631] D loss: 0.32574 G loss: 2.29770 (0.038 sec/batch, 1672.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:10,035] [train step64640] D loss: 0.32585 G loss: 2.32379 (0.042 sec/batch, 1525.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:10,428] [train step64650] D loss: 0.32587 G loss: 2.33333 (0.042 sec/batch, 1517.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:10,824] [train step64660] D loss: 0.32581 G loss: 2.31216 (0.041 sec/batch, 1556.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:11,205] [train step64671] D loss: 0.32635 G loss: 2.22783 (0.039 sec/batch, 1654.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:11,599] [train step64680] D loss: 0.32616 G loss: 2.35366 (0.040 sec/batch, 1608.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:11,982] [train step64690] D loss: 0.32589 G loss: 2.29288 (0.039 sec/batch, 1634.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:12,357] [train step64700] D loss: 0.32591 G loss: 2.34009 (0.027 sec/batch, 2330.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:12,753] [train step64710] D loss: 0.32622 G loss: 2.27555 (0.032 sec/batch, 2022.646 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:13,139] [train step64720] D loss: 0.32601 G loss: 2.25456 (0.038 sec/batch, 1699.948 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:13,523] [train step64730] D loss: 0.32585 G loss: 2.36091 (0.038 sec/batch, 1662.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:13,915] [train step64740] D loss: 0.32581 G loss: 2.27456 (0.036 sec/batch, 1784.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:14,300] [train step64750] D loss: 0.32565 G loss: 2.27240 (0.038 sec/batch, 1703.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:14,687] [train step64760] D loss: 0.32577 G loss: 2.29397 (0.040 sec/batch, 1593.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:15,068] [train step64770] D loss: 0.32591 G loss: 2.34742 (0.036 sec/batch, 1760.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:15,453] [train step64780] D loss: 0.32607 G loss: 2.29269 (0.037 sec/batch, 1750.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:15,845] [train step64791] D loss: 0.32577 G loss: 2.30745 (0.035 sec/batch, 1835.030 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:16,231] [train step64800] D loss: 0.32589 G loss: 2.27925 (0.043 sec/batch, 1491.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:16,615] [train step64811] D loss: 0.32582 G loss: 2.32320 (0.038 sec/batch, 1662.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:17,000] [train step64820] D loss: 0.32627 G loss: 2.38014 (0.036 sec/batch, 1767.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:17,391] [train step64830] D loss: 0.32574 G loss: 2.28216 (0.035 sec/batch, 1845.539 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:17,772] [train step64841] D loss: 0.32651 G loss: 2.42123 (0.036 sec/batch, 1794.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:18,152] [train step64850] D loss: 0.32581 G loss: 2.35122 (0.040 sec/batch, 1603.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:18,534] [train step64860] D loss: 0.32569 G loss: 2.30191 (0.036 sec/batch, 1801.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:18,925] [train step64871] D loss: 0.32600 G loss: 2.23109 (0.039 sec/batch, 1631.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:19,315] [train step64881] D loss: 0.32563 G loss: 2.27900 (0.041 sec/batch, 1575.473 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:19,710] [train step64890] D loss: 0.32587 G loss: 2.28016 (0.046 sec/batch, 1399.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:20,092] [train step64901] D loss: 0.32598 G loss: 2.24022 (0.033 sec/batch, 1942.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:20,472] [train step64910] D loss: 0.32570 G loss: 2.30124 (0.038 sec/batch, 1683.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:20,862] [train step64920] D loss: 0.32567 G loss: 2.31050 (0.036 sec/batch, 1800.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:21,247] [train step64931] D loss: 0.32585 G loss: 2.34979 (0.034 sec/batch, 1861.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:21,629] [train step64941] D loss: 0.32561 G loss: 2.31679 (0.044 sec/batch, 1461.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:22,025] [train step64950] D loss: 0.32579 G loss: 2.30030 (0.036 sec/batch, 1784.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:22,409] [train step64961] D loss: 0.32577 G loss: 2.31421 (0.037 sec/batch, 1747.320 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:22,793] [train step64970] D loss: 0.32575 G loss: 2.34943 (0.041 sec/batch, 1577.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:23,186] [train step64980] D loss: 0.32602 G loss: 2.23785 (0.038 sec/batch, 1700.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:23,574] [train step64991] D loss: 0.32598 G loss: 2.36611 (0.041 sec/batch, 1554.940 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:23,954] [train step65000] D loss: 0.32585 G loss: 2.29981 (0.030 sec/batch, 2159.768 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:51:23,955] Saved checkpoint at 65000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:24,556] [train step65010] D loss: 0.32588 G loss: 2.37947 (0.040 sec/batch, 1612.264 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:24,941] [train step65020] D loss: 0.32577 G loss: 2.32115 (0.036 sec/batch, 1785.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:25,333] [train step65030] D loss: 0.32560 G loss: 2.32177 (0.038 sec/batch, 1671.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:25,716] [train step65040] D loss: 0.32567 G loss: 2.27252 (0.041 sec/batch, 1547.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:26,108] [train step65050] D loss: 0.32568 G loss: 2.30815 (0.038 sec/batch, 1680.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:26,503] [train step65061] D loss: 0.32586 G loss: 2.29064 (0.036 sec/batch, 1801.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:26,897] [train step65070] D loss: 0.32610 G loss: 2.39715 (0.042 sec/batch, 1519.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:27,274] [train step65081] D loss: 0.32574 G loss: 2.29734 (0.035 sec/batch, 1850.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:27,657] [train step65090] D loss: 0.32588 G loss: 2.38194 (0.036 sec/batch, 1771.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:28,050] [train step65100] D loss: 0.32580 G loss: 2.29645 (0.039 sec/batch, 1660.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:28,427] [train step65111] D loss: 0.32578 G loss: 2.33876 (0.034 sec/batch, 1866.585 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:28,806] [train step65121] D loss: 0.32640 G loss: 2.19644 (0.036 sec/batch, 1769.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:29,215] [train step65130] D loss: 0.32589 G loss: 2.32286 (0.040 sec/batch, 1610.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:29,592] [train step65140] D loss: 0.32575 G loss: 2.28752 (0.034 sec/batch, 1903.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:29,995] [train step65151] D loss: 0.32565 G loss: 2.28574 (0.044 sec/batch, 1464.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:30,385] [train step65160] D loss: 0.32570 G loss: 2.32622 (0.034 sec/batch, 1874.877 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:30,769] [train step65171] D loss: 0.32575 G loss: 2.32816 (0.036 sec/batch, 1770.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:31,165] [train step65181] D loss: 0.32575 G loss: 2.27675 (0.035 sec/batch, 1841.702 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:31,549] [train step65190] D loss: 0.32608 G loss: 2.37011 (0.045 sec/batch, 1415.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:31,928] [train step65200] D loss: 0.32590 G loss: 2.35805 (0.030 sec/batch, 2124.505 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:32,337] [train step65211] D loss: 0.32585 G loss: 2.26866 (0.037 sec/batch, 1730.089 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:32,721] [train step65220] D loss: 0.32598 G loss: 2.36877 (0.036 sec/batch, 1763.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:33,124] [train step65230] D loss: 0.32590 G loss: 2.31924 (0.040 sec/batch, 1607.080 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:33,503] [train step65240] D loss: 0.32613 G loss: 2.39727 (0.037 sec/batch, 1726.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:33,883] [train step65250] D loss: 0.32592 G loss: 2.26152 (0.039 sec/batch, 1628.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:34,288] [train step65260] D loss: 0.32580 G loss: 2.28469 (0.041 sec/batch, 1543.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:34,672] [train step65271] D loss: 0.32652 G loss: 2.42037 (0.036 sec/batch, 1760.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:35,065] [train step65280] D loss: 0.32594 G loss: 2.24198 (0.042 sec/batch, 1534.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:35,458] [train step65291] D loss: 0.32569 G loss: 2.34717 (0.036 sec/batch, 1792.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:35,836] [train step65300] D loss: 0.32657 G loss: 2.42827 (0.032 sec/batch, 2016.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:36,229] [train step65310] D loss: 0.32572 G loss: 2.28072 (0.038 sec/batch, 1664.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:36,617] [train step65321] D loss: 0.32601 G loss: 2.38727 (0.037 sec/batch, 1752.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:37,002] [train step65330] D loss: 0.32580 G loss: 2.33735 (0.037 sec/batch, 1743.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:37,392] [train step65340] D loss: 0.32577 G loss: 2.30570 (0.035 sec/batch, 1843.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:37,785] [train step65351] D loss: 0.32589 G loss: 2.31184 (0.037 sec/batch, 1734.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:38,172] [train step65360] D loss: 0.32573 G loss: 2.32218 (0.042 sec/batch, 1539.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:38,564] [train step65370] D loss: 0.32583 G loss: 2.29826 (0.036 sec/batch, 1759.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:38,946] [train step65381] D loss: 0.32562 G loss: 2.28333 (0.037 sec/batch, 1706.910 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:39,337] [train step65391] D loss: 0.32575 G loss: 2.29938 (0.040 sec/batch, 1596.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:39,731] [train step65400] D loss: 0.32566 G loss: 2.33812 (0.040 sec/batch, 1618.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:40,135] [train step65410] D loss: 0.32590 G loss: 2.30374 (0.040 sec/batch, 1581.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:40,529] [train step65420] D loss: 0.32578 G loss: 2.27245 (0.034 sec/batch, 1866.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:40,931] [train step65430] D loss: 0.32581 G loss: 2.24896 (0.038 sec/batch, 1694.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:41,327] [train step65440] D loss: 0.32595 G loss: 2.35724 (0.050 sec/batch, 1292.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:41,711] [train step65451] D loss: 0.32574 G loss: 2.31569 (0.036 sec/batch, 1785.095 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:42,097] [train step65460] D loss: 0.32574 G loss: 2.25739 (0.035 sec/batch, 1813.643 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:42,501] [train step65470] D loss: 0.32588 G loss: 2.35443 (0.040 sec/batch, 1602.188 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:42,892] [train step65480] D loss: 0.32569 G loss: 2.29205 (0.042 sec/batch, 1518.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:43,278] [train step65490] D loss: 0.32580 G loss: 2.31943 (0.042 sec/batch, 1520.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:43,661] [train step65500] D loss: 0.32573 G loss: 2.31964 (0.034 sec/batch, 1898.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:44,061] [train step65510] D loss: 0.32606 G loss: 2.23492 (0.039 sec/batch, 1645.904 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:44,455] [train step65520] D loss: 0.32591 G loss: 2.37047 (0.036 sec/batch, 1776.730 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:44,848] [train step65530] D loss: 0.32594 G loss: 2.27850 (0.032 sec/batch, 1981.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:45,236] [train step65540] D loss: 0.32587 G loss: 2.24828 (0.046 sec/batch, 1395.347 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:45,632] [train step65550] D loss: 0.32580 G loss: 2.29105 (0.037 sec/batch, 1729.922 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:46,020] [train step65561] D loss: 0.32589 G loss: 2.27982 (0.041 sec/batch, 1578.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:46,406] [train step65571] D loss: 0.32569 G loss: 2.36203 (0.039 sec/batch, 1658.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:46,800] [train step65580] D loss: 0.32562 G loss: 2.34251 (0.036 sec/batch, 1760.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:47,176] [train step65590] D loss: 0.32585 G loss: 2.36129 (0.037 sec/batch, 1751.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:47,565] [train step65600] D loss: 0.32573 G loss: 2.26789 (0.038 sec/batch, 1664.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:47,938] [train step65610] D loss: 0.32590 G loss: 2.34980 (0.034 sec/batch, 1871.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:48,314] [train step65621] D loss: 0.32568 G loss: 2.26876 (0.036 sec/batch, 1765.036 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:48,695] [train step65631] D loss: 0.32576 G loss: 2.30444 (0.032 sec/batch, 1997.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:49,077] [train step65640] D loss: 0.32578 G loss: 2.31625 (0.036 sec/batch, 1787.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:49,451] [train step65650] D loss: 0.32574 G loss: 2.33797 (0.039 sec/batch, 1636.772 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:49,827] [train step65660] D loss: 0.32569 G loss: 2.30818 (0.030 sec/batch, 2151.201 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:50,211] [train step65670] D loss: 0.32558 G loss: 2.30430 (0.038 sec/batch, 1666.080 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:50,596] [train step65681] D loss: 0.32588 G loss: 2.26221 (0.038 sec/batch, 1669.489 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:50,976] [train step65691] D loss: 0.32576 G loss: 2.31305 (0.040 sec/batch, 1598.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:51,370] [train step65700] D loss: 0.32575 G loss: 2.33702 (0.040 sec/batch, 1586.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:51,762] [train step65710] D loss: 0.32581 G loss: 2.33948 (0.039 sec/batch, 1638.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:52,139] [train step65721] D loss: 0.32641 G loss: 2.20405 (0.033 sec/batch, 1930.871 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:52,526] [train step65730] D loss: 0.32633 G loss: 2.41292 (0.037 sec/batch, 1724.277 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:52,909] [train step65741] D loss: 0.32583 G loss: 2.33805 (0.036 sec/batch, 1794.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:53,287] [train step65750] D loss: 0.32565 G loss: 2.30539 (0.036 sec/batch, 1791.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:53,669] [train step65760] D loss: 0.32562 G loss: 2.32535 (0.036 sec/batch, 1776.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:54,044] [train step65770] D loss: 0.32572 G loss: 2.30417 (0.038 sec/batch, 1687.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:54,416] [train step65781] D loss: 0.32579 G loss: 2.30371 (0.037 sec/batch, 1718.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:54,809] [train step65790] D loss: 0.32585 G loss: 2.34525 (0.040 sec/batch, 1612.370 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:55,185] [train step65801] D loss: 0.32573 G loss: 2.33976 (0.029 sec/batch, 2186.508 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:55,575] [train step65810] D loss: 0.32596 G loss: 2.22763 (0.041 sec/batch, 1570.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:55,966] [train step65820] D loss: 0.32563 G loss: 2.33121 (0.036 sec/batch, 1769.959 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:56,347] [train step65831] D loss: 0.32559 G loss: 2.31195 (0.039 sec/batch, 1630.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:56,732] [train step65841] D loss: 0.32590 G loss: 2.35809 (0.045 sec/batch, 1426.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:57,114] [train step65850] D loss: 0.32600 G loss: 2.21378 (0.039 sec/batch, 1625.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:57,488] [train step65861] D loss: 0.32580 G loss: 2.32676 (0.034 sec/batch, 1893.751 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:57,879] [train step65871] D loss: 0.32554 G loss: 2.29673 (0.037 sec/batch, 1728.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:58,252] [train step65880] D loss: 0.32579 G loss: 2.27563 (0.035 sec/batch, 1823.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:58,626] [train step65891] D loss: 0.32599 G loss: 2.26917 (0.043 sec/batch, 1501.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:59,010] [train step65901] D loss: 0.32592 G loss: 2.23103 (0.034 sec/batch, 1861.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:59,392] [train step65910] D loss: 0.32582 G loss: 2.36216 (0.039 sec/batch, 1647.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:51:59,764] [train step65921] D loss: 0.32563 G loss: 2.29698 (0.034 sec/batch, 1866.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:00,153] [train step65930] D loss: 0.32571 G loss: 2.27054 (0.038 sec/batch, 1698.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:00,542] [train step65940] D loss: 0.32599 G loss: 2.36663 (0.036 sec/batch, 1756.156 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:00,934] [train step65951] D loss: 0.32567 G loss: 2.34308 (0.036 sec/batch, 1761.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:01,316] [train step65960] D loss: 0.32584 G loss: 2.37039 (0.039 sec/batch, 1646.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:01,698] [train step65970] D loss: 0.32582 G loss: 2.23990 (0.037 sec/batch, 1730.089 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:02,100] [train step65981] D loss: 0.32566 G loss: 2.27107 (0.040 sec/batch, 1609.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:02,485] [train step65990] D loss: 0.32593 G loss: 2.32857 (0.039 sec/batch, 1621.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:02,870] [train step66000] D loss: 0.32577 G loss: 2.29929 (0.037 sec/batch, 1743.314 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:52:02,870] Saved checkpoint at 66000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:03,467] [train step66010] D loss: 0.32557 G loss: 2.31768 (0.036 sec/batch, 1777.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:03,846] [train step66020] D loss: 0.32568 G loss: 2.31647 (0.040 sec/batch, 1612.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:04,242] [train step66030] D loss: 0.32557 G loss: 2.31489 (0.039 sec/batch, 1654.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:04,626] [train step66040] D loss: 0.32577 G loss: 2.33030 (0.036 sec/batch, 1765.338 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:05,017] [train step66051] D loss: 0.32566 G loss: 2.27381 (0.037 sec/batch, 1724.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:05,395] [train step66060] D loss: 0.32572 G loss: 2.31556 (0.037 sec/batch, 1751.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:05,772] [train step66070] D loss: 0.32608 G loss: 2.22922 (0.035 sec/batch, 1809.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:06,170] [train step66080] D loss: 0.32584 G loss: 2.33400 (0.042 sec/batch, 1523.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:06,546] [train step66090] D loss: 0.32568 G loss: 2.29716 (0.038 sec/batch, 1703.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:06,925] [train step66101] D loss: 0.32578 G loss: 2.31139 (0.037 sec/batch, 1729.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:07,315] [train step66110] D loss: 0.32566 G loss: 2.30646 (0.038 sec/batch, 1699.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:07,694] [train step66120] D loss: 0.32569 G loss: 2.32134 (0.036 sec/batch, 1793.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:08,078] [train step66131] D loss: 0.32597 G loss: 2.26161 (0.035 sec/batch, 1820.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:08,458] [train step66140] D loss: 0.32588 G loss: 2.25556 (0.039 sec/batch, 1636.263 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:08,841] [train step66150] D loss: 0.32569 G loss: 2.34902 (0.040 sec/batch, 1614.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:09,226] [train step66160] D loss: 0.32614 G loss: 2.40719 (0.039 sec/batch, 1654.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:09,599] [train step66171] D loss: 0.32565 G loss: 2.26317 (0.032 sec/batch, 1977.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:09,981] [train step66180] D loss: 0.32569 G loss: 2.35285 (0.034 sec/batch, 1885.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:10,375] [train step66191] D loss: 0.32554 G loss: 2.32400 (0.039 sec/batch, 1637.960 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:10,751] [train step66201] D loss: 0.32559 G loss: 2.30846 (0.035 sec/batch, 1830.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:11,137] [train step66210] D loss: 0.32556 G loss: 2.28942 (0.042 sec/batch, 1507.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:11,525] [train step66221] D loss: 0.32567 G loss: 2.29641 (0.039 sec/batch, 1659.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:11,904] [train step66230] D loss: 0.32571 G loss: 2.28756 (0.034 sec/batch, 1871.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:12,299] [train step66240] D loss: 0.32582 G loss: 2.36233 (0.036 sec/batch, 1754.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:12,707] [train step66251] D loss: 0.32554 G loss: 2.28519 (0.037 sec/batch, 1719.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:13,095] [train step66261] D loss: 0.32630 G loss: 2.22659 (0.038 sec/batch, 1667.208 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:13,481] [train step66270] D loss: 0.32564 G loss: 2.34334 (0.034 sec/batch, 1858.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:13,861] [train step66281] D loss: 0.32568 G loss: 2.30632 (0.037 sec/batch, 1725.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:14,250] [train step66290] D loss: 0.32569 G loss: 2.34915 (0.045 sec/batch, 1438.114 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:14,627] [train step66300] D loss: 0.32567 G loss: 2.31615 (0.035 sec/batch, 1820.296 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:15,008] [train step66310] D loss: 0.32556 G loss: 2.32054 (0.037 sec/batch, 1721.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:15,404] [train step66320] D loss: 0.32563 G loss: 2.34538 (0.038 sec/batch, 1665.336 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:15,793] [train step66330] D loss: 0.32571 G loss: 2.26667 (0.039 sec/batch, 1643.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:16,177] [train step66340] D loss: 0.32575 G loss: 2.36078 (0.037 sec/batch, 1734.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:16,573] [train step66350] D loss: 0.32568 G loss: 2.35502 (0.039 sec/batch, 1637.371 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:16,950] [train step66360] D loss: 0.32548 G loss: 2.29185 (0.038 sec/batch, 1675.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:17,342] [train step66370] D loss: 0.32591 G loss: 2.37199 (0.042 sec/batch, 1519.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:17,721] [train step66381] D loss: 0.32581 G loss: 2.28244 (0.037 sec/batch, 1736.615 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:18,096] [train step66390] D loss: 0.32582 G loss: 2.26906 (0.037 sec/batch, 1745.138 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:18,472] [train step66400] D loss: 0.32565 G loss: 2.26884 (0.035 sec/batch, 1839.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:18,845] [train step66411] D loss: 0.32554 G loss: 2.32579 (0.037 sec/batch, 1708.137 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:19,209] [train step66420] D loss: 0.32562 G loss: 2.29573 (0.034 sec/batch, 1865.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:19,588] [train step66431] D loss: 0.32616 G loss: 2.40290 (0.035 sec/batch, 1835.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:19,954] [train step66441] D loss: 0.32564 G loss: 2.32057 (0.035 sec/batch, 1818.114 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:20,330] [train step66450] D loss: 0.32576 G loss: 2.35951 (0.034 sec/batch, 1868.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:20,717] [train step66460] D loss: 0.32553 G loss: 2.31768 (0.050 sec/batch, 1284.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:21,091] [train step66471] D loss: 0.32559 G loss: 2.28398 (0.035 sec/batch, 1826.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:21,466] [train step66480] D loss: 0.32556 G loss: 2.28407 (0.035 sec/batch, 1846.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:21,845] [train step66491] D loss: 0.32570 G loss: 2.27669 (0.035 sec/batch, 1840.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:22,229] [train step66501] D loss: 0.32595 G loss: 2.22139 (0.041 sec/batch, 1576.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:22,601] [train step66510] D loss: 0.32568 G loss: 2.33533 (0.033 sec/batch, 1914.634 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:22,998] [train step66520] D loss: 0.32549 G loss: 2.27686 (0.039 sec/batch, 1640.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:23,373] [train step66531] D loss: 0.32551 G loss: 2.28532 (0.037 sec/batch, 1722.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:23,758] [train step66540] D loss: 0.32549 G loss: 2.31302 (0.037 sec/batch, 1748.583 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:24,135] [train step66550] D loss: 0.32565 G loss: 2.26154 (0.040 sec/batch, 1610.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:24,508] [train step66561] D loss: 0.32564 G loss: 2.31337 (0.037 sec/batch, 1720.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:24,891] [train step66570] D loss: 0.32610 G loss: 2.22897 (0.037 sec/batch, 1751.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:25,273] [train step66580] D loss: 0.32561 G loss: 2.29193 (0.037 sec/batch, 1733.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:25,653] [train step66590] D loss: 0.32570 G loss: 2.30857 (0.037 sec/batch, 1716.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:26,039] [train step66600] D loss: 0.32567 G loss: 2.29680 (0.040 sec/batch, 1609.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:26,407] [train step66611] D loss: 0.32561 G loss: 2.33729 (0.035 sec/batch, 1849.162 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:26,795] [train step66621] D loss: 0.32565 G loss: 2.35400 (0.036 sec/batch, 1757.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:27,177] [train step66630] D loss: 0.32569 G loss: 2.34705 (0.040 sec/batch, 1616.409 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:27,557] [train step66641] D loss: 0.32573 G loss: 2.34234 (0.034 sec/batch, 1896.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:27,948] [train step66650] D loss: 0.32608 G loss: 2.21944 (0.037 sec/batch, 1712.430 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:28,331] [train step66660] D loss: 0.32575 G loss: 2.31243 (0.040 sec/batch, 1619.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:28,709] [train step66671] D loss: 0.32569 G loss: 2.26040 (0.034 sec/batch, 1906.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:29,087] [train step66680] D loss: 0.32568 G loss: 2.34933 (0.034 sec/batch, 1891.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:29,502] [train step66690] D loss: 0.32559 G loss: 2.30453 (0.038 sec/batch, 1666.876 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:29,893] [train step66700] D loss: 0.32560 G loss: 2.28753 (0.047 sec/batch, 1359.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:30,281] [train step66711] D loss: 0.32553 G loss: 2.31171 (0.038 sec/batch, 1698.668 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:30,668] [train step66720] D loss: 0.32570 G loss: 2.29502 (0.037 sec/batch, 1752.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:31,055] [train step66731] D loss: 0.32574 G loss: 2.36807 (0.039 sec/batch, 1661.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:31,441] [train step66741] D loss: 0.32554 G loss: 2.33065 (0.037 sec/batch, 1721.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:31,848] [train step66750] D loss: 0.32547 G loss: 2.29070 (0.040 sec/batch, 1585.374 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:32,245] [train step66760] D loss: 0.32553 G loss: 2.29627 (0.045 sec/batch, 1411.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:32,641] [train step66771] D loss: 0.32543 G loss: 2.31490 (0.037 sec/batch, 1716.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:33,045] [train step66780] D loss: 0.32577 G loss: 2.24649 (0.043 sec/batch, 1486.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:33,432] [train step66790] D loss: 0.32575 G loss: 2.29403 (0.041 sec/batch, 1543.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:33,847] [train step66801] D loss: 0.32604 G loss: 2.38904 (0.037 sec/batch, 1734.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:34,235] [train step66810] D loss: 0.32553 G loss: 2.33171 (0.040 sec/batch, 1598.163 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:34,630] [train step66821] D loss: 0.32601 G loss: 2.38984 (0.037 sec/batch, 1717.371 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:35,028] [train step66831] D loss: 0.32565 G loss: 2.33927 (0.037 sec/batch, 1708.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:35,413] [train step66840] D loss: 0.32560 G loss: 2.34398 (0.036 sec/batch, 1766.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:35,795] [train step66851] D loss: 0.32560 G loss: 2.29783 (0.034 sec/batch, 1889.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:36,192] [train step66860] D loss: 0.32555 G loss: 2.30754 (0.044 sec/batch, 1445.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:36,588] [train step66870] D loss: 0.32561 G loss: 2.30396 (0.040 sec/batch, 1601.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:36,986] [train step66881] D loss: 0.32565 G loss: 2.34840 (0.036 sec/batch, 1761.492 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:37,374] [train step66890] D loss: 0.32573 G loss: 2.36807 (0.039 sec/batch, 1627.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:37,767] [train step66900] D loss: 0.32580 G loss: 2.30821 (0.041 sec/batch, 1556.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:38,154] [train step66911] D loss: 0.32571 G loss: 2.27209 (0.033 sec/batch, 1941.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:38,550] [train step66920] D loss: 0.32633 G loss: 2.42302 (0.039 sec/batch, 1656.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:38,955] [train step66930] D loss: 0.32600 G loss: 2.23681 (0.051 sec/batch, 1260.231 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:39,353] [train step66941] D loss: 0.32567 G loss: 2.34408 (0.042 sec/batch, 1540.608 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:39,745] [train step66951] D loss: 0.32574 G loss: 2.33008 (0.038 sec/batch, 1674.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:40,133] [train step66960] D loss: 0.32569 G loss: 2.28750 (0.037 sec/batch, 1739.608 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:40,515] [train step66971] D loss: 0.32565 G loss: 2.28545 (0.038 sec/batch, 1704.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:40,898] [train step66981] D loss: 0.32558 G loss: 2.32349 (0.038 sec/batch, 1670.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:41,297] [train step66990] D loss: 0.32594 G loss: 2.24282 (0.043 sec/batch, 1486.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:41,680] [train step67001] D loss: 0.32544 G loss: 2.28872 (0.039 sec/batch, 1644.130 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:52:41,681] Saved checkpoint at 67000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:42,297] [train step67011] D loss: 0.32560 G loss: 2.29950 (0.038 sec/batch, 1673.392 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:42,683] [train step67020] D loss: 0.32554 G loss: 2.31284 (0.038 sec/batch, 1698.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:43,080] [train step67030] D loss: 0.32590 G loss: 2.39280 (0.050 sec/batch, 1273.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:43,458] [train step67040] D loss: 0.32551 G loss: 2.32767 (0.035 sec/batch, 1825.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:43,843] [train step67050] D loss: 0.32565 G loss: 2.32881 (0.039 sec/batch, 1625.729 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:44,236] [train step67060] D loss: 0.32575 G loss: 2.30773 (0.038 sec/batch, 1683.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:44,630] [train step67071] D loss: 0.32572 G loss: 2.26635 (0.037 sec/batch, 1710.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:45,022] [train step67080] D loss: 0.32554 G loss: 2.31931 (0.041 sec/batch, 1544.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:45,415] [train step67091] D loss: 0.32569 G loss: 2.33196 (0.040 sec/batch, 1595.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:45,818] [train step67100] D loss: 0.32564 G loss: 2.29227 (0.040 sec/batch, 1582.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:46,212] [train step67110] D loss: 0.32552 G loss: 2.30715 (0.037 sec/batch, 1710.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:46,595] [train step67121] D loss: 0.32566 G loss: 2.34567 (0.034 sec/batch, 1859.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:46,982] [train step67130] D loss: 0.32550 G loss: 2.29310 (0.040 sec/batch, 1593.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:47,374] [train step67140] D loss: 0.32583 G loss: 2.34450 (0.048 sec/batch, 1336.105 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:47,759] [train step67150] D loss: 0.32589 G loss: 2.36977 (0.039 sec/batch, 1662.293 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:48,141] [train step67161] D loss: 0.32549 G loss: 2.30339 (0.037 sec/batch, 1751.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:48,524] [train step67170] D loss: 0.32575 G loss: 2.35446 (0.034 sec/batch, 1858.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:48,894] [train step67181] D loss: 0.32583 G loss: 2.24793 (0.039 sec/batch, 1622.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:49,276] [train step67191] D loss: 0.32555 G loss: 2.34224 (0.041 sec/batch, 1558.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:49,640] [train step67200] D loss: 0.32564 G loss: 2.29602 (0.035 sec/batch, 1833.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:50,015] [train step67210] D loss: 0.32569 G loss: 2.34384 (0.033 sec/batch, 1956.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:50,403] [train step67221] D loss: 0.32597 G loss: 2.37374 (0.038 sec/batch, 1664.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:50,759] [train step67230] D loss: 0.32572 G loss: 2.26679 (0.025 sec/batch, 2511.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:51,143] [train step67240] D loss: 0.32563 G loss: 2.31360 (0.036 sec/batch, 1763.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:51,517] [train step67250] D loss: 0.32557 G loss: 2.29686 (0.039 sec/batch, 1644.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:51,896] [train step67260] D loss: 0.32562 G loss: 2.30702 (0.044 sec/batch, 1470.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:52,269] [train step67271] D loss: 0.32569 G loss: 2.34031 (0.039 sec/batch, 1657.234 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:52,655] [train step67281] D loss: 0.32557 G loss: 2.26942 (0.040 sec/batch, 1603.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:53,025] [train step67290] D loss: 0.32559 G loss: 2.32278 (0.037 sec/batch, 1752.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:53,412] [train step67301] D loss: 0.32609 G loss: 2.21978 (0.038 sec/batch, 1674.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:53,810] [train step67310] D loss: 0.32548 G loss: 2.27606 (0.038 sec/batch, 1672.474 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:54,182] [train step67320] D loss: 0.32569 G loss: 2.31327 (0.034 sec/batch, 1906.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:54,571] [train step67330] D loss: 0.32571 G loss: 2.27055 (0.037 sec/batch, 1731.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:54,947] [train step67340] D loss: 0.32616 G loss: 2.20680 (0.037 sec/batch, 1726.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:55,323] [train step67350] D loss: 0.32604 G loss: 2.40351 (0.036 sec/batch, 1798.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:55,709] [train step67361] D loss: 0.32557 G loss: 2.34469 (0.036 sec/batch, 1765.338 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:56,096] [train step67370] D loss: 0.32563 G loss: 2.29176 (0.039 sec/batch, 1629.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:56,492] [train step67380] D loss: 0.32576 G loss: 2.31202 (0.040 sec/batch, 1610.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:56,870] [train step67391] D loss: 0.32557 G loss: 2.27872 (0.033 sec/batch, 1949.068 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:57,255] [train step67400] D loss: 0.32566 G loss: 2.31179 (0.036 sec/batch, 1788.950 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:57,647] [train step67410] D loss: 0.32557 G loss: 2.31610 (0.035 sec/batch, 1820.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:58,027] [train step67420] D loss: 0.32591 G loss: 2.38808 (0.039 sec/batch, 1642.058 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:58,411] [train step67430] D loss: 0.32573 G loss: 2.24780 (0.037 sec/batch, 1732.378 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:58,803] [train step67440] D loss: 0.32562 G loss: 2.34328 (0.038 sec/batch, 1687.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:59,187] [train step67450] D loss: 0.32572 G loss: 2.26319 (0.037 sec/batch, 1721.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:59,578] [train step67461] D loss: 0.32575 G loss: 2.30987 (0.042 sec/batch, 1530.794 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:52:59,954] [train step67470] D loss: 0.32569 G loss: 2.26954 (0.044 sec/batch, 1458.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:00,333] [train step67480] D loss: 0.32584 G loss: 2.21690 (0.039 sec/batch, 1645.289 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:00,728] [train step67490] D loss: 0.32563 G loss: 2.29655 (0.035 sec/batch, 1833.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:01,123] [train step67500] D loss: 0.32560 G loss: 2.34327 (0.045 sec/batch, 1428.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:01,499] [train step67511] D loss: 0.32576 G loss: 2.35809 (0.036 sec/batch, 1783.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:01,886] [train step67520] D loss: 0.32599 G loss: 2.39255 (0.033 sec/batch, 1924.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:02,262] [train step67530] D loss: 0.32560 G loss: 2.32532 (0.036 sec/batch, 1794.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:02,648] [train step67540] D loss: 0.32566 G loss: 2.26283 (0.035 sec/batch, 1826.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:03,020] [train step67551] D loss: 0.32570 G loss: 2.25575 (0.029 sec/batch, 2169.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:03,402] [train step67560] D loss: 0.32552 G loss: 2.32976 (0.038 sec/batch, 1677.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:03,795] [train step67570] D loss: 0.32567 G loss: 2.27694 (0.040 sec/batch, 1593.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:04,170] [train step67580] D loss: 0.32563 G loss: 2.28729 (0.039 sec/batch, 1661.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:04,559] [train step67590] D loss: 0.32560 G loss: 2.34289 (0.038 sec/batch, 1699.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:04,961] [train step67600] D loss: 0.32580 G loss: 2.37536 (0.040 sec/batch, 1585.580 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:05,341] [train step67611] D loss: 0.32561 G loss: 2.33436 (0.036 sec/batch, 1765.268 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:05,723] [train step67620] D loss: 0.32572 G loss: 2.27133 (0.044 sec/batch, 1459.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:06,104] [train step67630] D loss: 0.32548 G loss: 2.28083 (0.037 sec/batch, 1714.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:06,485] [train step67640] D loss: 0.32575 G loss: 2.26555 (0.039 sec/batch, 1633.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:06,873] [train step67650] D loss: 0.32637 G loss: 2.41251 (0.037 sec/batch, 1723.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:07,272] [train step67660] D loss: 0.32597 G loss: 2.38713 (0.039 sec/batch, 1651.433 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:07,655] [train step67671] D loss: 0.32557 G loss: 2.27473 (0.037 sec/batch, 1742.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:08,039] [train step67680] D loss: 0.32551 G loss: 2.32213 (0.031 sec/batch, 2038.838 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:08,430] [train step67691] D loss: 0.32567 G loss: 2.28305 (0.038 sec/batch, 1671.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:08,819] [train step67700] D loss: 0.32552 G loss: 2.32470 (0.045 sec/batch, 1407.433 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:09,204] [train step67710] D loss: 0.32554 G loss: 2.27574 (0.039 sec/batch, 1657.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:09,583] [train step67721] D loss: 0.32556 G loss: 2.29831 (0.040 sec/batch, 1609.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:09,971] [train step67731] D loss: 0.32562 G loss: 2.33496 (0.037 sec/batch, 1730.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:10,350] [train step67740] D loss: 0.32544 G loss: 2.27003 (0.040 sec/batch, 1591.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:10,732] [train step67751] D loss: 0.32547 G loss: 2.30666 (0.044 sec/batch, 1470.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:11,117] [train step67761] D loss: 0.32581 G loss: 2.29910 (0.037 sec/batch, 1748.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:11,498] [train step67770] D loss: 0.32608 G loss: 2.39303 (0.037 sec/batch, 1729.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:11,900] [train step67780] D loss: 0.32545 G loss: 2.30078 (0.048 sec/batch, 1343.871 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:12,283] [train step67790] D loss: 0.32545 G loss: 2.31152 (0.038 sec/batch, 1697.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:12,665] [train step67800] D loss: 0.32562 G loss: 2.26712 (0.041 sec/batch, 1554.832 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:13,057] [train step67810] D loss: 0.32553 G loss: 2.33018 (0.040 sec/batch, 1592.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:13,435] [train step67820] D loss: 0.32574 G loss: 2.36023 (0.039 sec/batch, 1628.224 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:13,815] [train step67830] D loss: 0.32544 G loss: 2.29726 (0.039 sec/batch, 1655.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:14,218] [train step67840] D loss: 0.32569 G loss: 2.34910 (0.038 sec/batch, 1676.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:14,596] [train step67851] D loss: 0.32554 G loss: 2.30823 (0.036 sec/batch, 1775.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:14,986] [train step67860] D loss: 0.32553 G loss: 2.28515 (0.044 sec/batch, 1438.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:15,367] [train step67871] D loss: 0.32571 G loss: 2.23406 (0.036 sec/batch, 1781.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:15,740] [train step67880] D loss: 0.32564 G loss: 2.33508 (0.031 sec/batch, 2094.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:16,147] [train step67890] D loss: 0.32544 G loss: 2.29077 (0.044 sec/batch, 1438.515 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:16,527] [train step67901] D loss: 0.32556 G loss: 2.29978 (0.037 sec/batch, 1729.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:16,908] [train step67910] D loss: 0.32564 G loss: 2.35799 (0.040 sec/batch, 1580.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:17,300] [train step67920] D loss: 0.32564 G loss: 2.27681 (0.039 sec/batch, 1631.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:17,689] [train step67931] D loss: 0.32566 G loss: 2.33673 (0.036 sec/batch, 1757.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:18,076] [train step67940] D loss: 0.32554 G loss: 2.29454 (0.046 sec/batch, 1388.411 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:18,442] [train step67950] D loss: 0.32577 G loss: 2.35655 (0.034 sec/batch, 1887.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:18,806] [train step67960] D loss: 0.32579 G loss: 2.37623 (0.034 sec/batch, 1882.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:19,185] [train step67970] D loss: 0.32559 G loss: 2.33024 (0.033 sec/batch, 1910.940 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:19,552] [train step67980] D loss: 0.32567 G loss: 2.32927 (0.039 sec/batch, 1633.455 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:19,921] [train step67990] D loss: 0.32545 G loss: 2.32331 (0.039 sec/batch, 1628.856 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:20,303] [train step68001] D loss: 0.32551 G loss: 2.30562 (0.035 sec/batch, 1831.137 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:53:20,304] Saved checkpoint at 68000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:20,887] [train step68010] D loss: 0.32580 G loss: 2.25125 (0.041 sec/batch, 1571.351 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:21,259] [train step68020] D loss: 0.32581 G loss: 2.22389 (0.035 sec/batch, 1849.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:21,628] [train step68031] D loss: 0.32546 G loss: 2.28592 (0.032 sec/batch, 2012.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:22,002] [train step68040] D loss: 0.32549 G loss: 2.30870 (0.032 sec/batch, 2017.174 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:22,387] [train step68051] D loss: 0.33387 G loss: 2.71236 (0.037 sec/batch, 1719.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:22,757] [train step68061] D loss: 0.37288 G loss: 3.42739 (0.035 sec/batch, 1844.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:23,128] [train step68070] D loss: 0.32921 G loss: 2.08339 (0.037 sec/batch, 1720.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:23,506] [train step68081] D loss: 0.32967 G loss: 2.58732 (0.036 sec/batch, 1762.810 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:23,882] [train step68090] D loss: 0.33103 G loss: 2.63800 (0.039 sec/batch, 1622.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:24,263] [train step68100] D loss: 0.32648 G loss: 2.19975 (0.039 sec/batch, 1642.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:24,635] [train step68111] D loss: 0.32779 G loss: 2.49468 (0.036 sec/batch, 1768.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:25,007] [train step68121] D loss: 0.32587 G loss: 2.33438 (0.036 sec/batch, 1780.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:25,391] [train step68130] D loss: 0.32617 G loss: 2.37556 (0.040 sec/batch, 1588.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:25,760] [train step68141] D loss: 0.32609 G loss: 2.36113 (0.035 sec/batch, 1824.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:26,134] [train step68150] D loss: 0.32623 G loss: 2.23302 (0.037 sec/batch, 1728.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:26,513] [train step68160] D loss: 0.32633 G loss: 2.40363 (0.037 sec/batch, 1751.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:26,887] [train step68170] D loss: 0.32599 G loss: 2.37404 (0.038 sec/batch, 1692.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:27,261] [train step68181] D loss: 0.32597 G loss: 2.26886 (0.035 sec/batch, 1828.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:27,655] [train step68190] D loss: 0.32601 G loss: 2.30176 (0.040 sec/batch, 1615.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:28,047] [train step68200] D loss: 0.32562 G loss: 2.30711 (0.040 sec/batch, 1595.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:28,450] [train step68211] D loss: 0.32584 G loss: 2.29934 (0.038 sec/batch, 1674.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:28,824] [train step68220] D loss: 0.32584 G loss: 2.33581 (0.038 sec/batch, 1674.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:29,204] [train step68231] D loss: 0.32604 G loss: 2.37001 (0.040 sec/batch, 1615.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:29,596] [train step68241] D loss: 0.32749 G loss: 2.49729 (0.037 sec/batch, 1745.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:29,975] [train step68250] D loss: 0.32648 G loss: 2.18116 (0.036 sec/batch, 1788.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:30,353] [train step68260] D loss: 0.32583 G loss: 2.32456 (0.036 sec/batch, 1784.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:30,749] [train step68270] D loss: 0.32603 G loss: 2.30278 (0.036 sec/batch, 1753.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:31,132] [train step68280] D loss: 0.32581 G loss: 2.34272 (0.036 sec/batch, 1778.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:31,526] [train step68291] D loss: 0.32614 G loss: 2.38644 (0.041 sec/batch, 1544.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:31,912] [train step68300] D loss: 0.32646 G loss: 2.18307 (0.038 sec/batch, 1667.487 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:32,294] [train step68310] D loss: 0.32648 G loss: 2.43212 (0.037 sec/batch, 1752.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:32,686] [train step68321] D loss: 0.32682 G loss: 2.45497 (0.033 sec/batch, 1940.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:33,069] [train step68331] D loss: 0.32599 G loss: 2.35566 (0.037 sec/batch, 1738.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:33,452] [train step68340] D loss: 0.32592 G loss: 2.36059 (0.032 sec/batch, 2025.133 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:33,852] [train step68351] D loss: 0.32712 G loss: 2.46275 (0.043 sec/batch, 1486.370 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:34,236] [train step68360] D loss: 0.32902 G loss: 2.55784 (0.035 sec/batch, 1803.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:34,638] [train step68370] D loss: 0.32598 G loss: 2.24823 (0.043 sec/batch, 1503.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:35,018] [train step68381] D loss: 0.32573 G loss: 2.33201 (0.040 sec/batch, 1613.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:35,398] [train step68391] D loss: 0.32601 G loss: 2.23378 (0.039 sec/batch, 1631.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:35,778] [train step68400] D loss: 0.32590 G loss: 2.32486 (0.034 sec/batch, 1889.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:36,165] [train step68411] D loss: 0.32622 G loss: 2.21703 (0.039 sec/batch, 1640.473 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:36,541] [train step68421] D loss: 0.32744 G loss: 2.14034 (0.037 sec/batch, 1737.863 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:36,928] [train step68430] D loss: 0.32853 G loss: 2.54122 (0.041 sec/batch, 1565.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:37,309] [train step68440] D loss: 0.32578 G loss: 2.33012 (0.039 sec/batch, 1636.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:37,692] [train step68451] D loss: 0.32624 G loss: 2.20150 (0.037 sec/batch, 1729.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:38,074] [train step68460] D loss: 0.32580 G loss: 2.36817 (0.037 sec/batch, 1751.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:38,462] [train step68471] D loss: 0.32571 G loss: 2.31228 (0.039 sec/batch, 1633.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:38,866] [train step68480] D loss: 0.32637 G loss: 2.20589 (0.034 sec/batch, 1899.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:39,250] [train step68490] D loss: 0.32688 G loss: 2.45067 (0.036 sec/batch, 1773.408 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:39,627] [train step68501] D loss: 0.32623 G loss: 2.40014 (0.038 sec/batch, 1694.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:40,022] [train step68511] D loss: 0.32561 G loss: 2.27362 (0.038 sec/batch, 1677.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:40,397] [train step68520] D loss: 0.32583 G loss: 2.35884 (0.037 sec/batch, 1743.359 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:40,784] [train step68530] D loss: 0.32572 G loss: 2.34067 (0.035 sec/batch, 1808.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:41,160] [train step68540] D loss: 0.32582 G loss: 2.25717 (0.037 sec/batch, 1717.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:41,545] [train step68550] D loss: 0.32689 G loss: 2.45266 (0.048 sec/batch, 1341.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:41,934] [train step68560] D loss: 0.32787 G loss: 2.50992 (0.036 sec/batch, 1791.887 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:42,309] [train step68570] D loss: 0.32890 G loss: 2.55736 (0.039 sec/batch, 1625.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:42,681] [train step68580] D loss: 0.32571 G loss: 2.26561 (0.033 sec/batch, 1914.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:43,073] [train step68590] D loss: 0.32711 G loss: 2.47646 (0.036 sec/batch, 1779.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:43,450] [train step68600] D loss: 0.32614 G loss: 2.38443 (0.036 sec/batch, 1773.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:43,842] [train step68610] D loss: 0.32579 G loss: 2.25753 (0.041 sec/batch, 1552.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:44,217] [train step68620] D loss: 0.32595 G loss: 2.37864 (0.040 sec/batch, 1599.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:44,591] [train step68631] D loss: 0.32578 G loss: 2.35380 (0.033 sec/batch, 1953.807 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:44,990] [train step68640] D loss: 0.32561 G loss: 2.33331 (0.038 sec/batch, 1699.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:45,370] [train step68651] D loss: 0.32574 G loss: 2.30176 (0.038 sec/batch, 1672.631 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:45,754] [train step68660] D loss: 0.32581 G loss: 2.24749 (0.043 sec/batch, 1493.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:46,140] [train step68670] D loss: 0.32629 G loss: 2.40654 (0.041 sec/batch, 1578.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:46,521] [train step68680] D loss: 0.32579 G loss: 2.37203 (0.039 sec/batch, 1649.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:46,911] [train step68691] D loss: 0.32564 G loss: 2.32382 (0.045 sec/batch, 1424.514 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:47,302] [train step68700] D loss: 0.32610 G loss: 2.22404 (0.039 sec/batch, 1635.874 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:47,684] [train step68711] D loss: 0.32617 G loss: 2.21813 (0.036 sec/batch, 1768.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:48,074] [train step68720] D loss: 0.32559 G loss: 2.26024 (0.035 sec/batch, 1814.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:48,448] [train step68730] D loss: 0.32564 G loss: 2.34703 (0.041 sec/batch, 1570.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:48,820] [train step68741] D loss: 0.32607 G loss: 2.39267 (0.034 sec/batch, 1906.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:49,220] [train step68750] D loss: 0.32725 G loss: 2.48102 (0.037 sec/batch, 1747.320 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:49,588] [train step68760] D loss: 0.32601 G loss: 2.25289 (0.036 sec/batch, 1801.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:49,972] [train step68771] D loss: 0.32578 G loss: 2.34417 (0.040 sec/batch, 1588.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:50,362] [train step68780] D loss: 0.32582 G loss: 2.25399 (0.047 sec/batch, 1367.483 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:50,729] [train step68790] D loss: 0.32566 G loss: 2.32888 (0.035 sec/batch, 1835.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:51,114] [train step68801] D loss: 0.32571 G loss: 2.27433 (0.040 sec/batch, 1601.166 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:51,491] [train step68810] D loss: 0.32571 G loss: 2.35481 (0.041 sec/batch, 1575.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:51,857] [train step68820] D loss: 0.32559 G loss: 2.30457 (0.041 sec/batch, 1575.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:52,229] [train step68830] D loss: 0.32566 G loss: 2.35132 (0.034 sec/batch, 1879.011 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:52,612] [train step68840] D loss: 0.32562 G loss: 2.29821 (0.040 sec/batch, 1610.822 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:52,990] [train step68850] D loss: 0.32551 G loss: 2.31617 (0.038 sec/batch, 1702.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:53,388] [train step68861] D loss: 0.32559 G loss: 2.29923 (0.035 sec/batch, 1830.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:53,772] [train step68870] D loss: 0.32567 G loss: 2.26180 (0.039 sec/batch, 1641.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:54,161] [train step68880] D loss: 0.32565 G loss: 2.33652 (0.039 sec/batch, 1647.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:54,535] [train step68891] D loss: 0.32561 G loss: 2.34085 (0.035 sec/batch, 1812.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:54,932] [train step68900] D loss: 0.32577 G loss: 2.33290 (0.039 sec/batch, 1636.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:55,321] [train step68910] D loss: 0.32658 G loss: 2.17625 (0.036 sec/batch, 1768.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:55,711] [train step68921] D loss: 0.32581 G loss: 2.25208 (0.036 sec/batch, 1800.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:56,087] [train step68930] D loss: 0.32569 G loss: 2.34636 (0.034 sec/batch, 1908.291 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:56,494] [train step68940] D loss: 0.32557 G loss: 2.27170 (0.042 sec/batch, 1539.547 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:56,878] [train step68950] D loss: 0.32580 G loss: 2.37531 (0.039 sec/batch, 1623.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:57,263] [train step68961] D loss: 0.32688 G loss: 2.46102 (0.034 sec/batch, 1877.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:57,658] [train step68970] D loss: 0.32602 G loss: 2.23125 (0.040 sec/batch, 1613.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:58,041] [train step68980] D loss: 0.32586 G loss: 2.28900 (0.037 sec/batch, 1743.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:58,437] [train step68991] D loss: 0.32800 G loss: 2.51260 (0.037 sec/batch, 1733.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:58,827] [train step69000] D loss: 0.34796 G loss: 1.77526 (0.040 sec/batch, 1591.011 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:53:58,827] Saved checkpoint at 69000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:59,425] [train step69011] D loss: 0.34996 G loss: 3.06025 (0.037 sec/batch, 1728.997 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:53:59,813] [train step69021] D loss: 0.45774 G loss: 4.46786 (0.040 sec/batch, 1589.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:00,193] [train step69030] D loss: 0.43147 G loss: 4.16778 (0.038 sec/batch, 1672.745 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:00,588] [train step69040] D loss: 0.37276 G loss: 1.53826 (0.041 sec/batch, 1572.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:00,973] [train step69051] D loss: 0.34608 G loss: 2.98678 (0.041 sec/batch, 1556.915 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:01,360] [train step69060] D loss: 0.33085 G loss: 2.02510 (0.042 sec/batch, 1526.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:01,741] [train step69071] D loss: 0.32758 G loss: 2.47401 (0.036 sec/batch, 1798.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:02,113] [train step69081] D loss: 0.32677 G loss: 2.19513 (0.037 sec/batch, 1722.308 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:02,502] [train step69090] D loss: 0.32609 G loss: 2.24784 (0.036 sec/batch, 1763.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:02,893] [train step69100] D loss: 0.32596 G loss: 2.27105 (0.038 sec/batch, 1676.642 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:03,271] [train step69110] D loss: 0.32593 G loss: 2.32052 (0.038 sec/batch, 1685.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:03,660] [train step69120] D loss: 0.32588 G loss: 2.34582 (0.040 sec/batch, 1582.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:04,042] [train step69130] D loss: 0.32604 G loss: 2.28459 (0.031 sec/batch, 2089.447 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:04,445] [train step69141] D loss: 0.32597 G loss: 2.31086 (0.050 sec/batch, 1281.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:04,832] [train step69150] D loss: 0.32597 G loss: 2.32491 (0.035 sec/batch, 1806.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:05,217] [train step69160] D loss: 0.32579 G loss: 2.31457 (0.037 sec/batch, 1723.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:05,606] [train step69170] D loss: 0.32578 G loss: 2.28331 (0.034 sec/batch, 1889.858 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:05,988] [train step69180] D loss: 0.32603 G loss: 2.29450 (0.038 sec/batch, 1702.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:06,372] [train step69191] D loss: 0.32590 G loss: 2.32131 (0.038 sec/batch, 1683.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:06,760] [train step69201] D loss: 0.32599 G loss: 2.28526 (0.036 sec/batch, 1770.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:07,146] [train step69210] D loss: 0.32581 G loss: 2.31878 (0.036 sec/batch, 1759.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:07,537] [train step69220] D loss: 0.32596 G loss: 2.27061 (0.049 sec/batch, 1301.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:07,920] [train step69230] D loss: 0.32583 G loss: 2.29619 (0.040 sec/batch, 1607.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:08,305] [train step69240] D loss: 0.32596 G loss: 2.32253 (0.036 sec/batch, 1768.163 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:08,696] [train step69250] D loss: 0.32593 G loss: 2.28379 (0.034 sec/batch, 1895.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:09,074] [train step69261] D loss: 0.32594 G loss: 2.32918 (0.041 sec/batch, 1562.615 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:09,453] [train step69270] D loss: 0.32593 G loss: 2.34758 (0.040 sec/batch, 1612.690 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:09,846] [train step69281] D loss: 0.32696 G loss: 2.44670 (0.036 sec/batch, 1754.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:10,233] [train step69291] D loss: 0.32670 G loss: 2.18927 (0.037 sec/batch, 1749.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:10,635] [train step69300] D loss: 0.32599 G loss: 2.36757 (0.047 sec/batch, 1356.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:11,012] [train step69311] D loss: 0.32608 G loss: 2.24087 (0.036 sec/batch, 1784.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:11,397] [train step69321] D loss: 0.32623 G loss: 2.36585 (0.036 sec/batch, 1765.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:11,783] [train step69330] D loss: 0.32571 G loss: 2.29147 (0.036 sec/batch, 1790.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:12,170] [train step69341] D loss: 0.32596 G loss: 2.30194 (0.039 sec/batch, 1631.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:12,553] [train step69350] D loss: 0.32630 G loss: 2.22231 (0.035 sec/batch, 1837.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:12,946] [train step69360] D loss: 0.32621 G loss: 2.39267 (0.038 sec/batch, 1673.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:13,335] [train step69370] D loss: 0.32595 G loss: 2.25544 (0.035 sec/batch, 1822.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:13,725] [train step69381] D loss: 0.32598 G loss: 2.33597 (0.036 sec/batch, 1790.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:14,119] [train step69390] D loss: 0.32601 G loss: 2.35277 (0.038 sec/batch, 1692.606 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:14,497] [train step69401] D loss: 0.32593 G loss: 2.34785 (0.035 sec/batch, 1829.689 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:14,889] [train step69411] D loss: 0.32587 G loss: 2.30441 (0.035 sec/batch, 1819.174 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:15,268] [train step69420] D loss: 0.32583 G loss: 2.32271 (0.038 sec/batch, 1694.005 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:15,648] [train step69431] D loss: 0.32575 G loss: 2.30836 (0.038 sec/batch, 1677.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:16,033] [train step69441] D loss: 0.32599 G loss: 2.32472 (0.036 sec/batch, 1781.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:16,404] [train step69450] D loss: 0.32599 G loss: 2.34556 (0.036 sec/batch, 1790.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:16,800] [train step69461] D loss: 0.32569 G loss: 2.32428 (0.044 sec/batch, 1456.822 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:17,177] [train step69470] D loss: 0.32612 G loss: 2.23327 (0.035 sec/batch, 1850.296 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:17,559] [train step69480] D loss: 0.32607 G loss: 2.36674 (0.031 sec/batch, 2051.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:17,947] [train step69491] D loss: 0.32647 G loss: 2.22549 (0.037 sec/batch, 1729.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:18,324] [train step69501] D loss: 0.32592 G loss: 2.29443 (0.035 sec/batch, 1810.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:18,694] [train step69510] D loss: 0.32582 G loss: 2.28114 (0.036 sec/batch, 1753.609 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:19,077] [train step69520] D loss: 0.32575 G loss: 2.30144 (0.035 sec/batch, 1813.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:19,443] [train step69530] D loss: 0.32591 G loss: 2.34651 (0.037 sec/batch, 1750.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:19,811] [train step69540] D loss: 0.32579 G loss: 2.34277 (0.037 sec/batch, 1737.323 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:20,201] [train step69550] D loss: 0.32583 G loss: 2.32120 (0.037 sec/batch, 1709.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:20,566] [train step69560] D loss: 0.32600 G loss: 2.30651 (0.039 sec/batch, 1651.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:20,943] [train step69570] D loss: 0.32604 G loss: 2.28129 (0.036 sec/batch, 1755.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:21,331] [train step69581] D loss: 0.32603 G loss: 2.31702 (0.035 sec/batch, 1817.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:21,694] [train step69590] D loss: 0.32609 G loss: 2.35445 (0.030 sec/batch, 2108.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:22,086] [train step69600] D loss: 0.32601 G loss: 2.33838 (0.036 sec/batch, 1769.387 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:22,456] [train step69611] D loss: 0.32630 G loss: 2.34398 (0.037 sec/batch, 1748.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:22,835] [train step69621] D loss: 0.32695 G loss: 2.27501 (0.037 sec/batch, 1749.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:23,214] [train step69630] D loss: 0.33202 G loss: 2.03721 (0.037 sec/batch, 1733.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:23,591] [train step69641] D loss: 0.32843 G loss: 2.17669 (0.037 sec/batch, 1739.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:23,969] [train step69651] D loss: 0.33053 G loss: 2.09274 (0.043 sec/batch, 1503.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:24,339] [train step69660] D loss: 0.32988 G loss: 2.57327 (0.033 sec/batch, 1925.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:24,711] [train step69671] D loss: 0.32681 G loss: 2.31702 (0.035 sec/batch, 1816.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:25,087] [train step69680] D loss: 0.32954 G loss: 2.54176 (0.038 sec/batch, 1677.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:25,466] [train step69690] D loss: 0.32670 G loss: 2.30860 (0.036 sec/batch, 1759.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:25,841] [train step69700] D loss: 0.33055 G loss: 2.58935 (0.034 sec/batch, 1906.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:26,223] [train step69711] D loss: 0.32760 G loss: 2.44505 (0.037 sec/batch, 1744.515 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:26,594] [train step69720] D loss: 0.32743 G loss: 2.24751 (0.038 sec/batch, 1686.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:26,972] [train step69731] D loss: 0.32627 G loss: 2.31023 (0.034 sec/batch, 1910.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:27,363] [train step69741] D loss: 0.32640 G loss: 2.30944 (0.036 sec/batch, 1802.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:27,743] [train step69750] D loss: 0.32659 G loss: 2.25233 (0.038 sec/batch, 1704.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:28,119] [train step69760] D loss: 0.32811 G loss: 2.15547 (0.045 sec/batch, 1422.861 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:28,494] [train step69770] D loss: 0.32730 G loss: 2.25027 (0.035 sec/batch, 1825.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:28,868] [train step69780] D loss: 0.32703 G loss: 2.40922 (0.041 sec/batch, 1578.846 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:29,263] [train step69791] D loss: 0.32692 G loss: 2.43892 (0.038 sec/batch, 1705.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:29,636] [train step69801] D loss: 0.32632 G loss: 2.32034 (0.040 sec/batch, 1616.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:30,004] [train step69810] D loss: 0.32716 G loss: 2.17584 (0.032 sec/batch, 2017.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:30,387] [train step69821] D loss: 0.32705 G loss: 2.21568 (0.033 sec/batch, 1945.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:30,787] [train step69830] D loss: 0.32667 G loss: 2.31515 (0.048 sec/batch, 1325.005 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:31,169] [train step69840] D loss: 0.32696 G loss: 2.38591 (0.039 sec/batch, 1658.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:31,554] [train step69850] D loss: 0.34787 G loss: 3.01213 (0.037 sec/batch, 1711.230 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:31,956] [train step69861] D loss: 0.34478 G loss: 2.95805 (0.037 sec/batch, 1746.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:32,353] [train step69870] D loss: 0.32846 G loss: 2.12895 (0.040 sec/batch, 1595.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:32,732] [train step69881] D loss: 0.32697 G loss: 2.42365 (0.039 sec/batch, 1646.863 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:33,100] [train step69890] D loss: 0.32716 G loss: 2.42007 (0.028 sec/batch, 2297.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:33,498] [train step69900] D loss: 0.32656 G loss: 2.28110 (0.045 sec/batch, 1423.509 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:33,881] [train step69910] D loss: 0.32653 G loss: 2.27866 (0.036 sec/batch, 1780.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:34,276] [train step69920] D loss: 0.32620 G loss: 2.25056 (0.056 sec/batch, 1141.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:34,661] [train step69930] D loss: 0.32681 G loss: 2.34207 (0.042 sec/batch, 1526.129 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:35,045] [train step69940] D loss: 0.32635 G loss: 2.25263 (0.044 sec/batch, 1440.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:35,436] [train step69951] D loss: 0.32619 G loss: 2.30251 (0.034 sec/batch, 1874.562 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:35,800] [train step69960] D loss: 0.32615 G loss: 2.33309 (0.026 sec/batch, 2444.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:36,187] [train step69970] D loss: 0.32603 G loss: 2.31195 (0.037 sec/batch, 1733.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:36,576] [train step69981] D loss: 0.32651 G loss: 2.22931 (0.039 sec/batch, 1630.578 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:36,950] [train step69990] D loss: 0.32721 G loss: 2.44831 (0.038 sec/batch, 1696.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:37,324] [train step70001] D loss: 0.32636 G loss: 2.35397 (0.039 sec/batch, 1647.429 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:54:37,325] Saved checkpoint at 70000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:37,909] [train step70011] D loss: 0.32686 G loss: 2.20437 (0.036 sec/batch, 1777.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:38,283] [train step70020] D loss: 0.32605 G loss: 2.36665 (0.039 sec/batch, 1637.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:38,669] [train step70030] D loss: 0.32644 G loss: 2.35226 (0.038 sec/batch, 1704.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:39,051] [train step70041] D loss: 0.32661 G loss: 2.39885 (0.040 sec/batch, 1589.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:39,438] [train step70050] D loss: 0.32753 G loss: 2.16478 (0.045 sec/batch, 1409.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:39,815] [train step70061] D loss: 0.32749 G loss: 2.17635 (0.040 sec/batch, 1590.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:40,191] [train step70071] D loss: 0.32797 G loss: 2.12554 (0.035 sec/batch, 1827.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:40,580] [train step70080] D loss: 0.32722 G loss: 2.46368 (0.039 sec/batch, 1625.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:40,962] [train step70090] D loss: 0.32627 G loss: 2.28427 (0.040 sec/batch, 1617.685 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:41,335] [train step70100] D loss: 0.32671 G loss: 2.20561 (0.039 sec/batch, 1661.162 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:41,728] [train step70110] D loss: 0.32790 G loss: 2.48914 (0.038 sec/batch, 1685.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:42,117] [train step70121] D loss: 0.32621 G loss: 2.34489 (0.038 sec/batch, 1667.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:42,533] [train step70130] D loss: 0.32607 G loss: 2.30723 (0.039 sec/batch, 1644.815 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:42,909] [train step70140] D loss: 0.32609 G loss: 2.27596 (0.037 sec/batch, 1723.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:43,285] [train step70151] D loss: 0.32601 G loss: 2.27469 (0.033 sec/batch, 1926.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:43,663] [train step70160] D loss: 0.32614 G loss: 2.28745 (0.033 sec/batch, 1963.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:44,043] [train step70170] D loss: 0.32594 G loss: 2.29453 (0.036 sec/batch, 1771.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:44,422] [train step70181] D loss: 0.32627 G loss: 2.28998 (0.033 sec/batch, 1913.828 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:44,813] [train step70191] D loss: 0.32665 G loss: 2.39974 (0.036 sec/batch, 1779.050 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:45,185] [train step70200] D loss: 0.32603 G loss: 2.31639 (0.034 sec/batch, 1895.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:45,572] [train step70210] D loss: 0.32730 G loss: 2.45469 (0.045 sec/batch, 1421.113 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:45,955] [train step70220] D loss: 0.32596 G loss: 2.34438 (0.036 sec/batch, 1794.403 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:46,335] [train step70230] D loss: 0.32595 G loss: 2.33922 (0.039 sec/batch, 1630.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:46,721] [train step70240] D loss: 0.32652 G loss: 2.41069 (0.040 sec/batch, 1604.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:47,103] [train step70251] D loss: 0.32595 G loss: 2.28945 (0.038 sec/batch, 1689.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:47,492] [train step70260] D loss: 0.32587 G loss: 2.28580 (0.036 sec/batch, 1753.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:47,880] [train step70271] D loss: 0.32600 G loss: 2.25324 (0.048 sec/batch, 1332.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:48,260] [train step70280] D loss: 0.32591 G loss: 2.33993 (0.034 sec/batch, 1893.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:48,639] [train step70290] D loss: 0.32612 G loss: 2.23539 (0.037 sec/batch, 1714.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:49,020] [train step70301] D loss: 0.32600 G loss: 2.30970 (0.036 sec/batch, 1762.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:49,387] [train step70310] D loss: 0.32602 G loss: 2.36000 (0.035 sec/batch, 1836.423 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:49,772] [train step70320] D loss: 0.32675 G loss: 2.20110 (0.039 sec/batch, 1655.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:50,137] [train step70331] D loss: 0.32640 G loss: 2.24988 (0.038 sec/batch, 1697.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:50,502] [train step70341] D loss: 0.32642 G loss: 2.40364 (0.036 sec/batch, 1798.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:50,875] [train step70350] D loss: 0.32576 G loss: 2.26074 (0.039 sec/batch, 1627.138 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:51,247] [train step70360] D loss: 0.32586 G loss: 2.33722 (0.038 sec/batch, 1701.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:51,614] [train step70370] D loss: 0.32608 G loss: 2.26037 (0.035 sec/batch, 1811.782 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:51,998] [train step70380] D loss: 0.32607 G loss: 2.23527 (0.036 sec/batch, 1779.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:52,374] [train step70390] D loss: 0.32611 G loss: 2.23732 (0.038 sec/batch, 1675.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:52,747] [train step70400] D loss: 0.32661 G loss: 2.42454 (0.038 sec/batch, 1688.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:53,139] [train step70410] D loss: 0.32590 G loss: 2.29817 (0.037 sec/batch, 1727.173 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:53,505] [train step70420] D loss: 0.32607 G loss: 2.36505 (0.037 sec/batch, 1722.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:53,885] [train step70431] D loss: 0.32588 G loss: 2.28263 (0.037 sec/batch, 1751.447 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:54,256] [train step70440] D loss: 0.32600 G loss: 2.37303 (0.044 sec/batch, 1448.583 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:54,625] [train step70451] D loss: 0.32593 G loss: 2.31692 (0.037 sec/batch, 1732.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:55,002] [train step70460] D loss: 0.32577 G loss: 2.29253 (0.037 sec/batch, 1722.264 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:55,370] [train step70470] D loss: 0.32579 G loss: 2.34942 (0.036 sec/batch, 1787.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:55,744] [train step70480] D loss: 0.32572 G loss: 2.30584 (0.037 sec/batch, 1742.205 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:56,124] [train step70491] D loss: 0.32626 G loss: 2.38505 (0.036 sec/batch, 1758.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:56,487] [train step70500] D loss: 0.32579 G loss: 2.28119 (0.033 sec/batch, 1944.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:56,868] [train step70511] D loss: 0.32582 G loss: 2.34553 (0.039 sec/batch, 1630.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:57,255] [train step70520] D loss: 0.32578 G loss: 2.31491 (0.040 sec/batch, 1605.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:57,614] [train step70530] D loss: 0.32560 G loss: 2.30846 (0.030 sec/batch, 2160.133 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:58,007] [train step70540] D loss: 0.32572 G loss: 2.33954 (0.031 sec/batch, 2041.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:58,395] [train step70551] D loss: 0.32608 G loss: 2.37262 (0.039 sec/batch, 1621.457 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:58,771] [train step70560] D loss: 0.32577 G loss: 2.31914 (0.038 sec/batch, 1675.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:59,150] [train step70571] D loss: 0.32604 G loss: 2.38824 (0.042 sec/batch, 1532.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:59,525] [train step70581] D loss: 0.32577 G loss: 2.26064 (0.035 sec/batch, 1807.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:54:59,901] [train step70590] D loss: 0.32573 G loss: 2.34543 (0.036 sec/batch, 1773.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:00,286] [train step70600] D loss: 0.32588 G loss: 2.25430 (0.035 sec/batch, 1814.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:00,663] [train step70611] D loss: 0.32583 G loss: 2.27689 (0.037 sec/batch, 1743.302 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:01,055] [train step70620] D loss: 0.32569 G loss: 2.30099 (0.049 sec/batch, 1314.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:01,429] [train step70631] D loss: 0.32579 G loss: 2.33454 (0.034 sec/batch, 1888.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:01,811] [train step70641] D loss: 0.32579 G loss: 2.29616 (0.046 sec/batch, 1389.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:02,199] [train step70650] D loss: 0.32592 G loss: 2.35843 (0.035 sec/batch, 1829.140 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:02,575] [train step70661] D loss: 0.32605 G loss: 2.38971 (0.038 sec/batch, 1693.482 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:02,957] [train step70671] D loss: 0.32562 G loss: 2.30886 (0.040 sec/batch, 1591.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:03,346] [train step70680] D loss: 0.32577 G loss: 2.34497 (0.038 sec/batch, 1700.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:03,745] [train step70690] D loss: 0.32575 G loss: 2.31898 (0.039 sec/batch, 1660.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:04,116] [train step70701] D loss: 0.32566 G loss: 2.30828 (0.040 sec/batch, 1610.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:04,502] [train step70710] D loss: 0.32574 G loss: 2.31448 (0.039 sec/batch, 1644.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:04,885] [train step70720] D loss: 0.32584 G loss: 2.26150 (0.037 sec/batch, 1724.776 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:05,276] [train step70730] D loss: 0.32567 G loss: 2.26398 (0.038 sec/batch, 1689.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:05,662] [train step70740] D loss: 0.32563 G loss: 2.31080 (0.040 sec/batch, 1615.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:06,043] [train step70750] D loss: 0.32583 G loss: 2.25246 (0.039 sec/batch, 1661.398 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:06,429] [train step70761] D loss: 0.32589 G loss: 2.25967 (0.039 sec/batch, 1646.954 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:06,835] [train step70770] D loss: 0.32597 G loss: 2.36388 (0.042 sec/batch, 1537.960 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:07,226] [train step70780] D loss: 0.32591 G loss: 2.35696 (0.046 sec/batch, 1392.430 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:07,621] [train step70791] D loss: 0.32568 G loss: 2.28805 (0.038 sec/batch, 1680.884 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:07,997] [train step70800] D loss: 0.32576 G loss: 2.33486 (0.036 sec/batch, 1800.807 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:08,385] [train step70810] D loss: 0.32572 G loss: 2.31039 (0.035 sec/batch, 1810.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:08,768] [train step70821] D loss: 0.32556 G loss: 2.30040 (0.035 sec/batch, 1814.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:09,154] [train step70830] D loss: 0.32573 G loss: 2.33601 (0.039 sec/batch, 1650.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:09,547] [train step70840] D loss: 0.32549 G loss: 2.29916 (0.038 sec/batch, 1687.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:09,924] [train step70850] D loss: 0.32565 G loss: 2.32661 (0.037 sec/batch, 1737.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:10,316] [train step70860] D loss: 0.32578 G loss: 2.26014 (0.040 sec/batch, 1581.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:10,695] [train step70871] D loss: 0.32558 G loss: 2.27980 (0.035 sec/batch, 1825.508 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:11,080] [train step70881] D loss: 0.32574 G loss: 2.26706 (0.038 sec/batch, 1663.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:11,468] [train step70890] D loss: 0.32548 G loss: 2.29654 (0.039 sec/batch, 1647.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:11,847] [train step70900] D loss: 0.32556 G loss: 2.29255 (0.038 sec/batch, 1668.244 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:12,227] [train step70911] D loss: 0.32562 G loss: 2.26099 (0.043 sec/batch, 1472.784 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:12,610] [train step70920] D loss: 0.32560 G loss: 2.33179 (0.040 sec/batch, 1615.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:12,987] [train step70930] D loss: 0.32579 G loss: 2.27969 (0.036 sec/batch, 1783.257 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:13,373] [train step70941] D loss: 0.32568 G loss: 2.33732 (0.044 sec/batch, 1462.696 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:13,760] [train step70950] D loss: 0.32572 G loss: 2.29886 (0.041 sec/batch, 1567.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:14,152] [train step70960] D loss: 0.32581 G loss: 2.35713 (0.040 sec/batch, 1596.471 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:14,539] [train step70971] D loss: 0.32552 G loss: 2.28516 (0.038 sec/batch, 1696.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:14,918] [train step70980] D loss: 0.32583 G loss: 2.36886 (0.045 sec/batch, 1424.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:15,296] [train step70991] D loss: 0.32563 G loss: 2.30860 (0.036 sec/batch, 1789.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:15,681] [train step71001] D loss: 0.32563 G loss: 2.33633 (0.037 sec/batch, 1747.445 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:55:15,682] Saved checkpoint at 71000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:16,274] [train step71010] D loss: 0.32571 G loss: 2.26927 (0.040 sec/batch, 1600.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:16,669] [train step71021] D loss: 0.32560 G loss: 2.26332 (0.039 sec/batch, 1650.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:17,051] [train step71031] D loss: 0.32555 G loss: 2.30222 (0.036 sec/batch, 1758.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:17,431] [train step71040] D loss: 0.32594 G loss: 2.23749 (0.040 sec/batch, 1613.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:17,818] [train step71050] D loss: 0.32578 G loss: 2.26074 (0.038 sec/batch, 1687.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:18,193] [train step71060] D loss: 0.32576 G loss: 2.35928 (0.037 sec/batch, 1708.431 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:18,579] [train step71070] D loss: 0.32563 G loss: 2.31207 (0.034 sec/batch, 1909.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:18,956] [train step71081] D loss: 0.32594 G loss: 2.34150 (0.035 sec/batch, 1820.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:19,333] [train step71090] D loss: 0.32569 G loss: 2.30353 (0.037 sec/batch, 1718.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:19,705] [train step71100] D loss: 0.32561 G loss: 2.32228 (0.037 sec/batch, 1743.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:20,074] [train step71111] D loss: 0.32600 G loss: 2.22258 (0.034 sec/batch, 1873.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:20,450] [train step71121] D loss: 0.32553 G loss: 2.27082 (0.035 sec/batch, 1809.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:20,824] [train step71130] D loss: 0.32576 G loss: 2.35316 (0.035 sec/batch, 1808.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:21,199] [train step71140] D loss: 0.32563 G loss: 2.29693 (0.035 sec/batch, 1851.688 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:21,574] [train step71150] D loss: 0.32569 G loss: 2.27577 (0.042 sec/batch, 1527.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:21,956] [train step71160] D loss: 0.32563 G loss: 2.29696 (0.040 sec/batch, 1618.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:22,334] [train step71171] D loss: 0.32566 G loss: 2.27331 (0.038 sec/batch, 1666.649 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:22,721] [train step71181] D loss: 0.32561 G loss: 2.28449 (0.042 sec/batch, 1539.971 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:23,096] [train step71190] D loss: 0.32569 G loss: 2.27548 (0.033 sec/batch, 1926.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:23,470] [train step71201] D loss: 0.32569 G loss: 2.25747 (0.034 sec/batch, 1889.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:23,852] [train step71211] D loss: 0.32558 G loss: 2.27621 (0.037 sec/batch, 1728.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:24,224] [train step71220] D loss: 0.32570 G loss: 2.32224 (0.035 sec/batch, 1843.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:24,628] [train step71230] D loss: 0.32549 G loss: 2.34000 (0.039 sec/batch, 1656.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:25,024] [train step71240] D loss: 0.32579 G loss: 2.24414 (0.045 sec/batch, 1423.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:25,400] [train step71250] D loss: 0.32596 G loss: 2.39891 (0.037 sec/batch, 1712.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:25,778] [train step71261] D loss: 0.32577 G loss: 2.24761 (0.039 sec/batch, 1625.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:26,156] [train step71271] D loss: 0.32624 G loss: 2.19394 (0.036 sec/batch, 1759.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:26,534] [train step71280] D loss: 0.32578 G loss: 2.36752 (0.035 sec/batch, 1812.577 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:26,923] [train step71290] D loss: 0.32547 G loss: 2.28376 (0.038 sec/batch, 1700.810 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:27,302] [train step71300] D loss: 0.32553 G loss: 2.30741 (0.038 sec/batch, 1681.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:27,675] [train step71310] D loss: 0.32551 G loss: 2.27866 (0.036 sec/batch, 1753.643 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:28,067] [train step71320] D loss: 0.32558 G loss: 2.30578 (0.034 sec/batch, 1877.158 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:28,439] [train step71331] D loss: 0.32553 G loss: 2.26605 (0.038 sec/batch, 1690.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:28,827] [train step71340] D loss: 0.32568 G loss: 2.31043 (0.046 sec/batch, 1399.800 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:29,211] [train step71350] D loss: 0.32542 G loss: 2.30985 (0.039 sec/batch, 1638.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:29,586] [train step71361] D loss: 0.32555 G loss: 2.27856 (0.033 sec/batch, 1937.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:29,969] [train step71370] D loss: 0.32550 G loss: 2.29092 (0.040 sec/batch, 1605.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:30,343] [train step71381] D loss: 0.32559 G loss: 2.33033 (0.035 sec/batch, 1827.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:30,726] [train step71390] D loss: 0.32555 G loss: 2.29120 (0.037 sec/batch, 1707.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:31,109] [train step71400] D loss: 0.32556 G loss: 2.32341 (0.038 sec/batch, 1704.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:31,498] [train step71411] D loss: 0.32548 G loss: 2.31021 (0.036 sec/batch, 1761.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:31,879] [train step71421] D loss: 0.32562 G loss: 2.27422 (0.037 sec/batch, 1731.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:32,269] [train step71430] D loss: 0.32594 G loss: 2.38912 (0.037 sec/batch, 1719.781 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:32,638] [train step71440] D loss: 0.32555 G loss: 2.33844 (0.031 sec/batch, 2072.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:33,029] [train step71451] D loss: 0.32542 G loss: 2.27947 (0.036 sec/batch, 1791.074 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:33,404] [train step71460] D loss: 0.32561 G loss: 2.35052 (0.038 sec/batch, 1695.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:33,785] [train step71471] D loss: 0.32581 G loss: 2.24253 (0.037 sec/batch, 1747.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:34,176] [train step71480] D loss: 0.32554 G loss: 2.27328 (0.043 sec/batch, 1503.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:34,557] [train step71490] D loss: 0.32561 G loss: 2.29991 (0.037 sec/batch, 1745.263 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:34,932] [train step71500] D loss: 0.32545 G loss: 2.28438 (0.033 sec/batch, 1939.142 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:35,336] [train step71510] D loss: 0.32559 G loss: 2.25399 (0.037 sec/batch, 1711.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:35,721] [train step71520] D loss: 0.32543 G loss: 2.33753 (0.036 sec/batch, 1756.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:36,108] [train step71530] D loss: 0.32544 G loss: 2.31700 (0.042 sec/batch, 1536.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:36,491] [train step71541] D loss: 0.32551 G loss: 2.31065 (0.040 sec/batch, 1613.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:36,865] [train step71550] D loss: 0.32547 G loss: 2.32122 (0.038 sec/batch, 1686.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:37,247] [train step71561] D loss: 0.32581 G loss: 2.24405 (0.037 sec/batch, 1729.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:37,639] [train step71571] D loss: 0.32548 G loss: 2.28261 (0.036 sec/batch, 1792.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:38,016] [train step71580] D loss: 0.32555 G loss: 2.30875 (0.037 sec/batch, 1748.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:38,405] [train step71590] D loss: 0.32543 G loss: 2.28580 (0.039 sec/batch, 1628.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:38,782] [train step71600] D loss: 0.32561 G loss: 2.24619 (0.041 sec/batch, 1573.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:39,169] [train step71610] D loss: 0.32550 G loss: 2.29374 (0.041 sec/batch, 1562.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:39,554] [train step71621] D loss: 0.32592 G loss: 2.22021 (0.039 sec/batch, 1643.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:39,931] [train step71630] D loss: 0.32576 G loss: 2.24977 (0.038 sec/batch, 1678.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:40,336] [train step71640] D loss: 0.32546 G loss: 2.30594 (0.040 sec/batch, 1607.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:40,723] [train step71651] D loss: 0.32558 G loss: 2.29085 (0.039 sec/batch, 1649.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:41,101] [train step71661] D loss: 0.32570 G loss: 2.23994 (0.040 sec/batch, 1600.288 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:41,489] [train step71670] D loss: 0.32585 G loss: 2.36919 (0.035 sec/batch, 1846.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:41,870] [train step71680] D loss: 0.32562 G loss: 2.28531 (0.038 sec/batch, 1663.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:42,258] [train step71691] D loss: 0.32548 G loss: 2.29735 (0.040 sec/batch, 1618.495 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:42,664] [train step71700] D loss: 0.32542 G loss: 2.28245 (0.041 sec/batch, 1561.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:43,042] [train step71711] D loss: 0.32546 G loss: 2.26425 (0.035 sec/batch, 1806.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:43,433] [train step71721] D loss: 0.32552 G loss: 2.28049 (0.034 sec/batch, 1857.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:43,816] [train step71730] D loss: 0.32562 G loss: 2.36485 (0.039 sec/batch, 1658.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:44,189] [train step71741] D loss: 0.32548 G loss: 2.31532 (0.032 sec/batch, 1972.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:44,580] [train step71751] D loss: 0.32579 G loss: 2.24000 (0.039 sec/batch, 1642.118 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:44,953] [train step71760] D loss: 0.32553 G loss: 2.35998 (0.036 sec/batch, 1798.972 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:45,346] [train step71771] D loss: 0.32540 G loss: 2.29374 (0.044 sec/batch, 1457.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:45,725] [train step71781] D loss: 0.32564 G loss: 2.34749 (0.038 sec/batch, 1704.049 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:46,117] [train step71790] D loss: 0.32559 G loss: 2.28157 (0.033 sec/batch, 1923.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:46,519] [train step71800] D loss: 0.32571 G loss: 2.25038 (0.041 sec/batch, 1555.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:46,895] [train step71811] D loss: 0.32562 G loss: 2.25946 (0.033 sec/batch, 1949.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:47,277] [train step71820] D loss: 0.32547 G loss: 2.31283 (0.032 sec/batch, 2005.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:47,670] [train step71830] D loss: 0.32560 G loss: 2.25245 (0.036 sec/batch, 1783.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:48,046] [train step71841] D loss: 0.32572 G loss: 2.23909 (0.038 sec/batch, 1700.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:48,423] [train step71850] D loss: 0.32549 G loss: 2.31258 (0.034 sec/batch, 1877.460 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:48,794] [train step71860] D loss: 0.32548 G loss: 2.27014 (0.039 sec/batch, 1640.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:49,161] [train step71871] D loss: 0.32541 G loss: 2.31545 (0.034 sec/batch, 1902.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:49,543] [train step71880] D loss: 0.32557 G loss: 2.36056 (0.040 sec/batch, 1604.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:49,911] [train step71890] D loss: 0.32556 G loss: 2.27338 (0.037 sec/batch, 1721.391 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:50,292] [train step71901] D loss: 0.32564 G loss: 2.24525 (0.034 sec/batch, 1858.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:50,671] [train step71910] D loss: 0.32552 G loss: 2.35189 (0.039 sec/batch, 1660.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:51,052] [train step71920] D loss: 0.32538 G loss: 2.31590 (0.038 sec/batch, 1672.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:51,420] [train step71931] D loss: 0.32596 G loss: 2.22606 (0.037 sec/batch, 1739.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:51,800] [train step71940] D loss: 0.32564 G loss: 2.34548 (0.040 sec/batch, 1592.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:52,169] [train step71951] D loss: 0.32544 G loss: 2.30978 (0.037 sec/batch, 1731.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:52,547] [train step71960] D loss: 0.32544 G loss: 2.34595 (0.037 sec/batch, 1752.521 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:52,918] [train step71970] D loss: 0.32549 G loss: 2.28463 (0.035 sec/batch, 1850.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:53,289] [train step71981] D loss: 0.32554 G loss: 2.29635 (0.036 sec/batch, 1792.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:53,666] [train step71991] D loss: 0.32563 G loss: 2.26910 (0.037 sec/batch, 1707.463 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:54,040] [train step72000] D loss: 0.32547 G loss: 2.30803 (0.035 sec/batch, 1815.213 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:55:54,040] Saved checkpoint at 72000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:54,628] [train step72010] D loss: 0.32550 G loss: 2.31172 (0.037 sec/batch, 1748.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:54,996] [train step72020] D loss: 0.32549 G loss: 2.27573 (0.035 sec/batch, 1832.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:55,373] [train step72030] D loss: 0.32550 G loss: 2.32448 (0.041 sec/batch, 1571.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:55,751] [train step72041] D loss: 0.32543 G loss: 2.27022 (0.037 sec/batch, 1728.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:56,125] [train step72050] D loss: 0.32550 G loss: 2.32063 (0.038 sec/batch, 1693.172 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:56,518] [train step72060] D loss: 0.32548 G loss: 2.33037 (0.037 sec/batch, 1711.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:56,901] [train step72070] D loss: 0.32564 G loss: 2.36894 (0.037 sec/batch, 1716.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:57,279] [train step72080] D loss: 0.32567 G loss: 2.34298 (0.039 sec/batch, 1650.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:57,660] [train step72090] D loss: 0.32556 G loss: 2.27412 (0.045 sec/batch, 1412.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:58,041] [train step72100] D loss: 0.32545 G loss: 2.32075 (0.037 sec/batch, 1717.009 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:58,409] [train step72110] D loss: 0.32556 G loss: 2.31126 (0.034 sec/batch, 1877.631 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:58,800] [train step72120] D loss: 0.32539 G loss: 2.31961 (0.037 sec/batch, 1721.722 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:59,177] [train step72131] D loss: 0.32565 G loss: 2.23747 (0.038 sec/batch, 1701.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:59,549] [train step72141] D loss: 0.32548 G loss: 2.32936 (0.036 sec/batch, 1754.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:55:59,940] [train step72150] D loss: 0.32553 G loss: 2.29718 (0.040 sec/batch, 1618.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:00,314] [train step72160] D loss: 0.32599 G loss: 2.40007 (0.034 sec/batch, 1908.413 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:00,696] [train step72170] D loss: 0.32547 G loss: 2.32796 (0.038 sec/batch, 1669.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:01,088] [train step72180] D loss: 0.32547 G loss: 2.32451 (0.039 sec/batch, 1625.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:01,459] [train step72190] D loss: 0.32550 G loss: 2.33548 (0.039 sec/batch, 1648.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:01,846] [train step72201] D loss: 0.32568 G loss: 2.35055 (0.039 sec/batch, 1625.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:02,226] [train step72210] D loss: 0.32554 G loss: 2.27077 (0.036 sec/batch, 1794.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:02,609] [train step72221] D loss: 0.32557 G loss: 2.27282 (0.041 sec/batch, 1577.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:02,999] [train step72231] D loss: 0.32554 G loss: 2.26532 (0.036 sec/batch, 1802.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:03,380] [train step72240] D loss: 0.32567 G loss: 2.35914 (0.045 sec/batch, 1406.732 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:03,757] [train step72251] D loss: 0.32547 G loss: 2.27587 (0.036 sec/batch, 1773.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:04,145] [train step72261] D loss: 0.32546 G loss: 2.28420 (0.036 sec/batch, 1775.461 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:04,522] [train step72270] D loss: 0.32570 G loss: 2.36399 (0.036 sec/batch, 1789.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:04,902] [train step72281] D loss: 0.32588 G loss: 2.39060 (0.041 sec/batch, 1565.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:05,282] [train step72291] D loss: 0.32548 G loss: 2.33296 (0.035 sec/batch, 1805.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:05,669] [train step72300] D loss: 0.32551 G loss: 2.27867 (0.037 sec/batch, 1710.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:06,067] [train step72310] D loss: 0.32545 G loss: 2.28888 (0.038 sec/batch, 1671.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:06,449] [train step72320] D loss: 0.32550 G loss: 2.26181 (0.042 sec/batch, 1529.224 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:06,822] [train step72330] D loss: 0.32559 G loss: 2.34353 (0.038 sec/batch, 1663.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:07,219] [train step72340] D loss: 0.32553 G loss: 2.28877 (0.040 sec/batch, 1592.465 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:07,607] [train step72350] D loss: 0.32544 G loss: 2.32348 (0.038 sec/batch, 1698.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:08,008] [train step72360] D loss: 0.32557 G loss: 2.28005 (0.052 sec/batch, 1238.205 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:08,394] [train step72370] D loss: 0.32534 G loss: 2.29367 (0.032 sec/batch, 2024.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:08,782] [train step72381] D loss: 0.32564 G loss: 2.36011 (0.043 sec/batch, 1486.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:09,170] [train step72390] D loss: 0.32535 G loss: 2.30591 (0.041 sec/batch, 1563.781 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:09,552] [train step72401] D loss: 0.32561 G loss: 2.36254 (0.038 sec/batch, 1683.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:09,935] [train step72410] D loss: 0.32542 G loss: 2.29466 (0.044 sec/batch, 1463.286 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:10,320] [train step72420] D loss: 0.32546 G loss: 2.29998 (0.037 sec/batch, 1739.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:10,696] [train step72430] D loss: 0.32548 G loss: 2.35773 (0.035 sec/batch, 1815.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:11,091] [train step72440] D loss: 0.32582 G loss: 2.39308 (0.042 sec/batch, 1521.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:11,464] [train step72450] D loss: 0.32546 G loss: 2.26846 (0.036 sec/batch, 1763.888 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:11,849] [train step72460] D loss: 0.32562 G loss: 2.35233 (0.036 sec/batch, 1776.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:12,240] [train step72470] D loss: 0.32540 G loss: 2.31170 (0.037 sec/batch, 1738.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:12,615] [train step72480] D loss: 0.32538 G loss: 2.32043 (0.035 sec/batch, 1853.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:13,007] [train step72491] D loss: 0.32543 G loss: 2.30576 (0.036 sec/batch, 1773.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:13,391] [train step72500] D loss: 0.32559 G loss: 2.35235 (0.035 sec/batch, 1828.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:13,771] [train step72510] D loss: 0.32554 G loss: 2.31133 (0.036 sec/batch, 1757.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:14,159] [train step72520] D loss: 0.32540 G loss: 2.31037 (0.035 sec/batch, 1819.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:14,537] [train step72531] D loss: 0.32564 G loss: 2.25100 (0.040 sec/batch, 1612.729 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:14,917] [train step72540] D loss: 0.32579 G loss: 2.38530 (0.044 sec/batch, 1463.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:15,299] [train step72551] D loss: 0.32566 G loss: 2.35898 (0.034 sec/batch, 1896.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:15,682] [train step72561] D loss: 0.32546 G loss: 2.34121 (0.041 sec/batch, 1566.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:16,060] [train step72570] D loss: 0.32555 G loss: 2.25677 (0.038 sec/batch, 1670.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:16,448] [train step72580] D loss: 0.32553 G loss: 2.27472 (0.033 sec/batch, 1923.386 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:16,825] [train step72590] D loss: 0.32548 G loss: 2.27843 (0.035 sec/batch, 1804.729 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:17,205] [train step72600] D loss: 0.32550 G loss: 2.28085 (0.044 sec/batch, 1471.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:17,584] [train step72611] D loss: 0.32621 G loss: 2.19310 (0.045 sec/batch, 1412.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:17,967] [train step72620] D loss: 0.32545 G loss: 2.27206 (0.033 sec/batch, 1950.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:18,353] [train step72630] D loss: 0.32606 G loss: 2.39857 (0.037 sec/batch, 1750.053 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:18,726] [train step72641] D loss: 0.32554 G loss: 2.32019 (0.044 sec/batch, 1438.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:19,092] [train step72651] D loss: 0.32556 G loss: 2.29864 (0.037 sec/batch, 1734.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:19,469] [train step72660] D loss: 0.32585 G loss: 2.35774 (0.039 sec/batch, 1661.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:19,838] [train step72671] D loss: 0.32568 G loss: 2.37107 (0.036 sec/batch, 1789.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:20,204] [train step72680] D loss: 0.32564 G loss: 2.37204 (0.033 sec/batch, 1965.293 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:20,584] [train step72690] D loss: 0.32559 G loss: 2.30661 (0.034 sec/batch, 1874.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:20,952] [train step72700] D loss: 0.32588 G loss: 2.38650 (0.035 sec/batch, 1812.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:21,322] [train step72710] D loss: 0.32576 G loss: 2.24945 (0.049 sec/batch, 1300.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:21,696] [train step72720] D loss: 0.32574 G loss: 2.36887 (0.035 sec/batch, 1854.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:22,060] [train step72731] D loss: 0.32542 G loss: 2.31222 (0.034 sec/batch, 1873.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:22,438] [train step72741] D loss: 0.32545 G loss: 2.29316 (0.040 sec/batch, 1608.698 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:22,807] [train step72750] D loss: 0.32553 G loss: 2.28639 (0.035 sec/batch, 1807.074 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:23,178] [train step72760] D loss: 0.32550 G loss: 2.28799 (0.032 sec/batch, 1988.985 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:23,557] [train step72771] D loss: 0.32554 G loss: 2.27677 (0.033 sec/batch, 1925.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:23,934] [train step72780] D loss: 0.32557 G loss: 2.35878 (0.040 sec/batch, 1583.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:24,304] [train step72791] D loss: 0.32592 G loss: 2.39754 (0.037 sec/batch, 1718.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:24,689] [train step72800] D loss: 0.32688 G loss: 2.46570 (0.037 sec/batch, 1729.877 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:25,062] [train step72810] D loss: 0.32600 G loss: 2.21180 (0.038 sec/batch, 1670.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:25,446] [train step72820] D loss: 0.32562 G loss: 2.33539 (0.038 sec/batch, 1684.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:25,824] [train step72831] D loss: 0.32559 G loss: 2.34717 (0.035 sec/batch, 1818.434 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:26,197] [train step72840] D loss: 0.32597 G loss: 2.21064 (0.034 sec/batch, 1873.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:26,577] [train step72850] D loss: 0.32538 G loss: 2.30026 (0.034 sec/batch, 1863.514 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:26,955] [train step72861] D loss: 0.32556 G loss: 2.33813 (0.040 sec/batch, 1586.282 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:27,333] [train step72870] D loss: 0.32541 G loss: 2.32999 (0.039 sec/batch, 1634.052 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:27,720] [train step72881] D loss: 0.32574 G loss: 2.38280 (0.036 sec/batch, 1778.425 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:28,096] [train step72891] D loss: 0.32546 G loss: 2.28126 (0.041 sec/batch, 1556.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:28,482] [train step72900] D loss: 0.32543 G loss: 2.33837 (0.043 sec/batch, 1480.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:28,874] [train step72911] D loss: 0.32549 G loss: 2.32492 (0.037 sec/batch, 1731.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:29,247] [train step72921] D loss: 0.32548 G loss: 2.25595 (0.035 sec/batch, 1809.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:29,632] [train step72930] D loss: 0.32564 G loss: 2.36462 (0.036 sec/batch, 1776.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:30,010] [train step72940] D loss: 0.32547 G loss: 2.33772 (0.046 sec/batch, 1393.464 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:30,391] [train step72950] D loss: 0.32538 G loss: 2.30594 (0.037 sec/batch, 1746.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:30,779] [train step72960] D loss: 0.32562 G loss: 2.34061 (0.034 sec/batch, 1865.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:31,161] [train step72971] D loss: 0.32548 G loss: 2.32490 (0.037 sec/batch, 1714.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:31,542] [train step72980] D loss: 0.32548 G loss: 2.27893 (0.034 sec/batch, 1883.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:31,939] [train step72990] D loss: 0.32552 G loss: 2.34256 (0.040 sec/batch, 1614.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:32,322] [train step73001] D loss: 0.32563 G loss: 2.36743 (0.036 sec/batch, 1775.942 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:56:32,322] Saved checkpoint at 73000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:32,914] [train step73011] D loss: 0.32547 G loss: 2.34280 (0.035 sec/batch, 1821.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:33,293] [train step73020] D loss: 0.32553 G loss: 2.27492 (0.037 sec/batch, 1731.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:33,702] [train step73030] D loss: 0.32542 G loss: 2.32911 (0.041 sec/batch, 1563.089 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:34,089] [train step73040] D loss: 0.32549 G loss: 2.34014 (0.041 sec/batch, 1574.494 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:34,468] [train step73050] D loss: 0.32551 G loss: 2.28902 (0.041 sec/batch, 1545.041 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:34,859] [train step73061] D loss: 0.32544 G loss: 2.28050 (0.037 sec/batch, 1721.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:35,234] [train step73071] D loss: 0.32540 G loss: 2.31272 (0.038 sec/batch, 1702.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:35,630] [train step73080] D loss: 0.32563 G loss: 2.23490 (0.041 sec/batch, 1544.472 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:36,021] [train step73090] D loss: 0.32564 G loss: 2.26161 (0.037 sec/batch, 1739.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:36,392] [train step73100] D loss: 0.32537 G loss: 2.30037 (0.037 sec/batch, 1742.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:36,780] [train step73110] D loss: 0.32555 G loss: 2.34838 (0.041 sec/batch, 1548.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:37,162] [train step73121] D loss: 0.32631 G loss: 2.43423 (0.036 sec/batch, 1768.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:37,538] [train step73130] D loss: 0.32607 G loss: 2.41410 (0.042 sec/batch, 1541.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:37,916] [train step73140] D loss: 0.32566 G loss: 2.24479 (0.032 sec/batch, 2023.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:38,301] [train step73150] D loss: 0.32547 G loss: 2.31373 (0.036 sec/batch, 1801.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:38,686] [train step73160] D loss: 0.32569 G loss: 2.37221 (0.037 sec/batch, 1712.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:39,092] [train step73170] D loss: 0.32573 G loss: 2.22865 (0.037 sec/batch, 1708.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:39,464] [train step73181] D loss: 0.32559 G loss: 2.23200 (0.046 sec/batch, 1383.324 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:39,855] [train step73191] D loss: 0.32539 G loss: 2.31781 (0.044 sec/batch, 1441.427 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:40,242] [train step73200] D loss: 0.32566 G loss: 2.23850 (0.037 sec/batch, 1752.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:40,627] [train step73210] D loss: 0.32555 G loss: 2.26761 (0.035 sec/batch, 1824.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:41,023] [train step73220] D loss: 0.32539 G loss: 2.32884 (0.036 sec/batch, 1783.849 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:41,403] [train step73230] D loss: 0.32576 G loss: 2.23488 (0.038 sec/batch, 1677.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:41,796] [train step73240] D loss: 0.32585 G loss: 2.22359 (0.039 sec/batch, 1645.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:42,199] [train step73250] D loss: 0.32559 G loss: 2.34554 (0.046 sec/batch, 1390.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:42,575] [train step73260] D loss: 0.32577 G loss: 2.21854 (0.039 sec/batch, 1656.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:42,969] [train step73270] D loss: 0.32673 G loss: 2.15496 (0.044 sec/batch, 1449.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:43,349] [train step73281] D loss: 0.32565 G loss: 2.26894 (0.037 sec/batch, 1730.323 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:43,728] [train step73290] D loss: 0.32538 G loss: 2.31884 (0.038 sec/batch, 1677.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:44,117] [train step73301] D loss: 0.32542 G loss: 2.29940 (0.040 sec/batch, 1588.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:44,505] [train step73311] D loss: 0.32561 G loss: 2.34434 (0.040 sec/batch, 1617.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:44,881] [train step73320] D loss: 0.32559 G loss: 2.30514 (0.038 sec/batch, 1675.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:45,279] [train step73330] D loss: 0.32570 G loss: 2.37819 (0.040 sec/batch, 1611.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:45,661] [train step73341] D loss: 0.32563 G loss: 2.24824 (0.036 sec/batch, 1756.662 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:46,046] [train step73350] D loss: 0.32544 G loss: 2.34727 (0.041 sec/batch, 1574.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:46,422] [train step73360] D loss: 0.32541 G loss: 2.28640 (0.037 sec/batch, 1735.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:46,807] [train step73370] D loss: 0.32556 G loss: 2.34449 (0.037 sec/batch, 1747.217 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:47,189] [train step73380] D loss: 0.32559 G loss: 2.23492 (0.038 sec/batch, 1701.155 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:47,580] [train step73390] D loss: 0.32569 G loss: 2.24094 (0.041 sec/batch, 1546.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:47,959] [train step73401] D loss: 0.32574 G loss: 2.22653 (0.041 sec/batch, 1572.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:48,344] [train step73410] D loss: 0.32571 G loss: 2.38304 (0.037 sec/batch, 1709.257 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:48,718] [train step73421] D loss: 0.32538 G loss: 2.30709 (0.036 sec/batch, 1801.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:49,094] [train step73431] D loss: 0.32538 G loss: 2.29114 (0.038 sec/batch, 1689.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:49,482] [train step73440] D loss: 0.32545 G loss: 2.32146 (0.040 sec/batch, 1581.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:49,852] [train step73451] D loss: 0.32545 G loss: 2.33706 (0.035 sec/batch, 1839.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:50,237] [train step73461] D loss: 0.32550 G loss: 2.33441 (0.035 sec/batch, 1838.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:50,615] [train step73470] D loss: 0.32573 G loss: 2.23467 (0.041 sec/batch, 1571.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:50,984] [train step73481] D loss: 0.32568 G loss: 2.22960 (0.037 sec/batch, 1737.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:51,363] [train step73490] D loss: 0.32551 G loss: 2.25223 (0.036 sec/batch, 1759.667 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:51,735] [train step73500] D loss: 0.32545 G loss: 2.30252 (0.037 sec/batch, 1749.039 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:52,106] [train step73511] D loss: 0.32570 G loss: 2.23748 (0.037 sec/batch, 1727.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:52,495] [train step73520] D loss: 0.32566 G loss: 2.24807 (0.036 sec/batch, 1790.716 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:52,879] [train step73530] D loss: 0.32558 G loss: 2.34551 (0.037 sec/batch, 1710.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:53,259] [train step73541] D loss: 0.32568 G loss: 2.37643 (0.036 sec/batch, 1773.080 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:53,625] [train step73551] D loss: 0.32547 G loss: 2.33665 (0.034 sec/batch, 1869.471 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:53,997] [train step73560] D loss: 0.32545 G loss: 2.26480 (0.040 sec/batch, 1602.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:54,373] [train step73570] D loss: 0.32543 G loss: 2.28936 (0.036 sec/batch, 1791.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:54,748] [train step73581] D loss: 0.32543 G loss: 2.28254 (0.035 sec/batch, 1807.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:55,124] [train step73590] D loss: 0.32562 G loss: 2.36224 (0.041 sec/batch, 1575.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:55,502] [train step73600] D loss: 0.32571 G loss: 2.38307 (0.039 sec/batch, 1622.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:55,882] [train step73611] D loss: 0.32542 G loss: 2.33261 (0.041 sec/batch, 1555.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:56,254] [train step73620] D loss: 0.32534 G loss: 2.28397 (0.036 sec/batch, 1758.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:56,634] [train step73631] D loss: 0.32546 G loss: 2.28722 (0.034 sec/batch, 1863.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:57,001] [train step73641] D loss: 0.32541 G loss: 2.30644 (0.036 sec/batch, 1771.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:57,388] [train step73650] D loss: 0.32551 G loss: 2.32331 (0.035 sec/batch, 1843.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:57,761] [train step73661] D loss: 0.32544 G loss: 2.32923 (0.036 sec/batch, 1755.249 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:58,125] [train step73670] D loss: 0.32539 G loss: 2.32089 (0.037 sec/batch, 1741.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:58,514] [train step73680] D loss: 0.32539 G loss: 2.29769 (0.036 sec/batch, 1786.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:58,886] [train step73690] D loss: 0.32581 G loss: 2.39778 (0.037 sec/batch, 1724.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:59,259] [train step73700] D loss: 0.32537 G loss: 2.29880 (0.035 sec/batch, 1854.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:56:59,653] [train step73710] D loss: 0.32535 G loss: 2.28947 (0.038 sec/batch, 1700.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:00,046] [train step73721] D loss: 0.32538 G loss: 2.30028 (0.037 sec/batch, 1747.126 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:00,440] [train step73730] D loss: 0.32553 G loss: 2.36039 (0.045 sec/batch, 1434.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:00,817] [train step73740] D loss: 0.32572 G loss: 2.22576 (0.037 sec/batch, 1711.142 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:01,188] [train step73751] D loss: 0.32536 G loss: 2.29062 (0.037 sec/batch, 1724.177 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:01,581] [train step73761] D loss: 0.32533 G loss: 2.30545 (0.041 sec/batch, 1576.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:01,963] [train step73770] D loss: 0.32545 G loss: 2.32837 (0.040 sec/batch, 1592.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:02,342] [train step73781] D loss: 0.32540 G loss: 2.32706 (0.036 sec/batch, 1786.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:02,726] [train step73790] D loss: 0.32582 G loss: 2.39292 (0.036 sec/batch, 1757.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:03,100] [train step73800] D loss: 0.32543 G loss: 2.29021 (0.037 sec/batch, 1716.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:03,481] [train step73811] D loss: 0.32535 G loss: 2.29275 (0.044 sec/batch, 1467.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:03,864] [train step73820] D loss: 0.32543 G loss: 2.26053 (0.035 sec/batch, 1843.688 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:04,240] [train step73830] D loss: 0.32545 G loss: 2.34314 (0.039 sec/batch, 1651.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:04,631] [train step73841] D loss: 0.32547 G loss: 2.34618 (0.035 sec/batch, 1812.076 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:05,013] [train step73850] D loss: 0.32538 G loss: 2.28685 (0.038 sec/batch, 1697.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:05,399] [train step73860] D loss: 0.32535 G loss: 2.31668 (0.033 sec/batch, 1927.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:05,795] [train step73871] D loss: 0.32537 G loss: 2.30819 (0.043 sec/batch, 1492.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:06,175] [train step73880] D loss: 0.32547 G loss: 2.30718 (0.039 sec/batch, 1637.810 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:06,552] [train step73890] D loss: 0.32539 G loss: 2.30960 (0.035 sec/batch, 1822.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:06,936] [train step73901] D loss: 0.32550 G loss: 2.27210 (0.038 sec/batch, 1668.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:07,316] [train step73910] D loss: 0.32539 G loss: 2.26986 (0.039 sec/batch, 1653.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:07,712] [train step73920] D loss: 0.32538 G loss: 2.32722 (0.041 sec/batch, 1545.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:08,091] [train step73930] D loss: 0.32551 G loss: 2.27889 (0.040 sec/batch, 1607.398 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:08,472] [train step73941] D loss: 0.32544 G loss: 2.28224 (0.043 sec/batch, 1483.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:08,855] [train step73950] D loss: 0.32547 G loss: 2.34893 (0.038 sec/batch, 1675.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:09,225] [train step73961] D loss: 0.32536 G loss: 2.30490 (0.038 sec/batch, 1684.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:09,602] [train step73971] D loss: 0.32535 G loss: 2.31048 (0.039 sec/batch, 1638.240 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:09,983] [train step73980] D loss: 0.32540 G loss: 2.29390 (0.035 sec/batch, 1813.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:10,363] [train step73991] D loss: 0.32553 G loss: 2.27377 (0.039 sec/batch, 1633.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:10,759] [train step74001] D loss: 0.32540 G loss: 2.31970 (0.039 sec/batch, 1640.603 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:57:10,760] Saved checkpoint at 74000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:11,346] [train step74010] D loss: 0.32538 G loss: 2.32960 (0.036 sec/batch, 1801.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:11,730] [train step74020] D loss: 0.32548 G loss: 2.35687 (0.037 sec/batch, 1729.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:12,127] [train step74030] D loss: 0.32542 G loss: 2.30447 (0.037 sec/batch, 1710.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:12,500] [train step74040] D loss: 0.32540 G loss: 2.29770 (0.038 sec/batch, 1695.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:12,886] [train step74051] D loss: 0.32545 G loss: 2.29608 (0.037 sec/batch, 1716.240 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:13,267] [train step74060] D loss: 0.32554 G loss: 2.25703 (0.037 sec/batch, 1744.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:13,642] [train step74070] D loss: 0.32571 G loss: 2.38814 (0.036 sec/batch, 1797.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:14,027] [train step74081] D loss: 0.32631 G loss: 2.43143 (0.037 sec/batch, 1712.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:14,401] [train step74091] D loss: 0.32547 G loss: 2.34284 (0.037 sec/batch, 1715.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:14,778] [train step74100] D loss: 0.32535 G loss: 2.28754 (0.038 sec/batch, 1682.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:15,160] [train step74110] D loss: 0.32541 G loss: 2.27844 (0.031 sec/batch, 2066.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:15,548] [train step74121] D loss: 0.32536 G loss: 2.31883 (0.037 sec/batch, 1716.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:15,935] [train step74130] D loss: 0.32545 G loss: 2.26920 (0.039 sec/batch, 1646.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:16,314] [train step74140] D loss: 0.32559 G loss: 2.23962 (0.033 sec/batch, 1937.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:16,696] [train step74151] D loss: 0.32577 G loss: 2.22134 (0.040 sec/batch, 1598.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:17,083] [train step74160] D loss: 0.32556 G loss: 2.34899 (0.039 sec/batch, 1643.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:17,460] [train step74170] D loss: 0.32562 G loss: 2.37431 (0.039 sec/batch, 1626.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:17,834] [train step74181] D loss: 0.32538 G loss: 2.33300 (0.037 sec/batch, 1726.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:18,214] [train step74190] D loss: 0.32541 G loss: 2.28362 (0.036 sec/batch, 1800.312 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:18,589] [train step74201] D loss: 0.32537 G loss: 2.31395 (0.035 sec/batch, 1808.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:18,966] [train step74210] D loss: 0.32574 G loss: 2.22253 (0.039 sec/batch, 1654.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:19,339] [train step74220] D loss: 0.32621 G loss: 2.42356 (0.036 sec/batch, 1761.283 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:19,709] [train step74230] D loss: 0.32542 G loss: 2.32414 (0.038 sec/batch, 1668.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:20,085] [train step74241] D loss: 0.32541 G loss: 2.33962 (0.038 sec/batch, 1701.910 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:20,459] [train step74250] D loss: 0.32545 G loss: 2.25733 (0.037 sec/batch, 1750.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:20,827] [train step74260] D loss: 0.32566 G loss: 2.23213 (0.038 sec/batch, 1683.656 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:21,210] [train step74271] D loss: 0.32536 G loss: 2.28078 (0.039 sec/batch, 1639.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:21,605] [train step74280] D loss: 0.32545 G loss: 2.34699 (0.040 sec/batch, 1587.305 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:21,977] [train step74291] D loss: 0.32546 G loss: 2.33321 (0.035 sec/batch, 1812.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:22,357] [train step74301] D loss: 0.32555 G loss: 2.31646 (0.038 sec/batch, 1662.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:22,723] [train step74310] D loss: 0.32536 G loss: 2.26883 (0.031 sec/batch, 2083.981 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:23,108] [train step74321] D loss: 0.32534 G loss: 2.29816 (0.035 sec/batch, 1805.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:23,484] [train step74331] D loss: 0.32550 G loss: 2.34585 (0.039 sec/batch, 1632.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:23,860] [train step74340] D loss: 0.32535 G loss: 2.28315 (0.037 sec/batch, 1718.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:24,238] [train step74350] D loss: 0.32553 G loss: 2.36364 (0.038 sec/batch, 1704.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:24,615] [train step74361] D loss: 0.32613 G loss: 2.42448 (0.034 sec/batch, 1859.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:25,002] [train step74370] D loss: 0.32594 G loss: 2.20185 (0.039 sec/batch, 1631.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:25,382] [train step74381] D loss: 0.32551 G loss: 2.26045 (0.037 sec/batch, 1730.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:25,758] [train step74390] D loss: 0.32542 G loss: 2.27007 (0.036 sec/batch, 1755.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:26,142] [train step74400] D loss: 0.32549 G loss: 2.35138 (0.040 sec/batch, 1615.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:26,517] [train step74410] D loss: 0.32565 G loss: 2.37907 (0.039 sec/batch, 1655.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:26,893] [train step74421] D loss: 0.32539 G loss: 2.33743 (0.038 sec/batch, 1691.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:27,272] [train step74430] D loss: 0.32534 G loss: 2.29817 (0.034 sec/batch, 1855.797 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:27,644] [train step74441] D loss: 0.32553 G loss: 2.35395 (0.035 sec/batch, 1804.863 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:28,017] [train step74451] D loss: 0.32561 G loss: 2.36846 (0.035 sec/batch, 1803.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:28,407] [train step74460] D loss: 0.32614 G loss: 2.19089 (0.037 sec/batch, 1747.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:28,787] [train step74470] D loss: 0.32618 G loss: 2.18030 (0.035 sec/batch, 1843.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:29,165] [train step74481] D loss: 0.32593 G loss: 2.20605 (0.037 sec/batch, 1738.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:29,555] [train step74490] D loss: 0.32569 G loss: 2.37955 (0.042 sec/batch, 1507.810 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:29,938] [train step74501] D loss: 0.32552 G loss: 2.33736 (0.034 sec/batch, 1856.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:30,329] [train step74511] D loss: 0.32532 G loss: 2.31656 (0.037 sec/batch, 1709.769 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:30,716] [train step74520] D loss: 0.32538 G loss: 2.31376 (0.040 sec/batch, 1608.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:31,102] [train step74531] D loss: 0.32537 G loss: 2.30379 (0.038 sec/batch, 1669.790 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:31,486] [train step74541] D loss: 0.32542 G loss: 2.28834 (0.041 sec/batch, 1550.324 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:31,892] [train step74550] D loss: 0.32533 G loss: 2.31409 (0.040 sec/batch, 1598.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:32,277] [train step74561] D loss: 0.32537 G loss: 2.28856 (0.040 sec/batch, 1592.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:32,673] [train step74571] D loss: 0.32552 G loss: 2.36879 (0.037 sec/batch, 1720.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:33,050] [train step74580] D loss: 0.32551 G loss: 2.25686 (0.036 sec/batch, 1766.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:33,437] [train step74591] D loss: 0.32543 G loss: 2.24869 (0.036 sec/batch, 1782.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:33,826] [train step74600] D loss: 0.32587 G loss: 2.20499 (0.036 sec/batch, 1781.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:34,207] [train step74610] D loss: 0.32540 G loss: 2.34187 (0.030 sec/batch, 2132.810 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:34,611] [train step74621] D loss: 0.32536 G loss: 2.33224 (0.039 sec/batch, 1661.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:34,991] [train step74631] D loss: 0.32535 G loss: 2.28189 (0.035 sec/batch, 1806.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:35,380] [train step74640] D loss: 0.32585 G loss: 2.39872 (0.046 sec/batch, 1380.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:35,754] [train step74651] D loss: 0.32582 G loss: 2.39828 (0.036 sec/batch, 1801.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:36,135] [train step74660] D loss: 0.32544 G loss: 2.25696 (0.045 sec/batch, 1429.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:36,525] [train step74670] D loss: 0.32583 G loss: 2.39173 (0.036 sec/batch, 1779.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:36,905] [train step74681] D loss: 0.32622 G loss: 2.42986 (0.038 sec/batch, 1675.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:37,292] [train step74691] D loss: 0.32614 G loss: 2.42368 (0.036 sec/batch, 1784.015 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:37,687] [train step74700] D loss: 0.32571 G loss: 2.23064 (0.043 sec/batch, 1478.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:38,059] [train step74710] D loss: 0.32542 G loss: 2.32961 (0.031 sec/batch, 2033.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:38,453] [train step74721] D loss: 0.32589 G loss: 2.40285 (0.042 sec/batch, 1518.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:38,828] [train step74730] D loss: 0.32649 G loss: 2.17354 (0.033 sec/batch, 1940.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:39,206] [train step74741] D loss: 0.32722 G loss: 2.16151 (0.036 sec/batch, 1763.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:39,593] [train step74750] D loss: 0.32535 G loss: 2.29243 (0.039 sec/batch, 1643.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:40,007] [train step74760] D loss: 0.32549 G loss: 2.26894 (0.043 sec/batch, 1471.944 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:40,407] [train step74770] D loss: 0.32554 G loss: 2.24053 (0.032 sec/batch, 1981.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:40,807] [train step74780] D loss: 0.32537 G loss: 2.30979 (0.039 sec/batch, 1638.460 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:41,188] [train step74790] D loss: 0.32545 G loss: 2.26305 (0.034 sec/batch, 1877.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:41,585] [train step74801] D loss: 0.32614 G loss: 2.18409 (0.043 sec/batch, 1484.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:41,969] [train step74810] D loss: 0.32584 G loss: 2.21597 (0.040 sec/batch, 1613.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:42,369] [train step74820] D loss: 0.32534 G loss: 2.31969 (0.036 sec/batch, 1777.553 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:42,772] [train step74831] D loss: 0.32556 G loss: 2.24841 (0.038 sec/batch, 1676.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:43,158] [train step74841] D loss: 0.32539 G loss: 2.26823 (0.035 sec/batch, 1850.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:43,548] [train step74850] D loss: 0.32549 G loss: 2.30438 (0.034 sec/batch, 1870.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:43,942] [train step74860] D loss: 0.32540 G loss: 2.27958 (0.031 sec/batch, 2077.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:44,340] [train step74870] D loss: 0.32546 G loss: 2.25597 (0.039 sec/batch, 1658.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:44,730] [train step74880] D loss: 0.32543 G loss: 2.32046 (0.036 sec/batch, 1766.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:45,122] [train step74890] D loss: 0.32537 G loss: 2.32678 (0.039 sec/batch, 1647.682 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:45,510] [train step74901] D loss: 0.32539 G loss: 2.33495 (0.039 sec/batch, 1641.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:45,902] [train step74910] D loss: 0.32540 G loss: 2.28115 (0.039 sec/batch, 1620.312 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:46,291] [train step74921] D loss: 0.32542 G loss: 2.27430 (0.036 sec/batch, 1758.157 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:46,681] [train step74930] D loss: 0.32578 G loss: 2.22272 (0.041 sec/batch, 1544.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:47,079] [train step74940] D loss: 0.32566 G loss: 2.37915 (0.035 sec/batch, 1835.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:47,469] [train step74950] D loss: 0.32570 G loss: 2.38385 (0.039 sec/batch, 1655.997 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:47,865] [train step74960] D loss: 0.32539 G loss: 2.27953 (0.041 sec/batch, 1554.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:48,247] [train step74970] D loss: 0.32536 G loss: 2.31573 (0.036 sec/batch, 1754.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:48,627] [train step74981] D loss: 0.32534 G loss: 2.29243 (0.036 sec/batch, 1760.972 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:49,011] [train step74991] D loss: 0.32543 G loss: 2.34455 (0.036 sec/batch, 1800.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:49,399] [train step75000] D loss: 0.32599 G loss: 2.19787 (0.035 sec/batch, 1804.305 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:57:49,399] Saved checkpoint at 75000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:50,008] [train step75010] D loss: 0.32548 G loss: 2.25105 (0.053 sec/batch, 1207.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:50,391] [train step75021] D loss: 0.32568 G loss: 2.37284 (0.033 sec/batch, 1957.668 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:50,770] [train step75030] D loss: 0.32535 G loss: 2.27488 (0.037 sec/batch, 1716.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:51,147] [train step75040] D loss: 0.32551 G loss: 2.34726 (0.030 sec/batch, 2108.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:51,525] [train step75050] D loss: 0.32557 G loss: 2.35733 (0.039 sec/batch, 1651.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:51,907] [train step75060] D loss: 0.32624 G loss: 2.18622 (0.038 sec/batch, 1665.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:52,274] [train step75071] D loss: 0.32595 G loss: 2.20468 (0.034 sec/batch, 1858.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:52,646] [train step75080] D loss: 0.32531 G loss: 2.30674 (0.034 sec/batch, 1857.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:53,036] [train step75090] D loss: 0.32536 G loss: 2.27191 (0.035 sec/batch, 1815.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:53,410] [train step75100] D loss: 0.32539 G loss: 2.28616 (0.043 sec/batch, 1488.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:53,786] [train step75111] D loss: 0.32595 G loss: 2.20884 (0.038 sec/batch, 1694.690 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:54,166] [train step75120] D loss: 0.32579 G loss: 2.39316 (0.034 sec/batch, 1890.843 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:54,536] [train step75131] D loss: 0.32539 G loss: 2.28846 (0.041 sec/batch, 1577.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:54,907] [train step75141] D loss: 0.32532 G loss: 2.28890 (0.042 sec/batch, 1533.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:55,300] [train step75150] D loss: 0.32543 G loss: 2.33947 (0.044 sec/batch, 1440.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:55,677] [train step75161] D loss: 0.32538 G loss: 2.32561 (0.037 sec/batch, 1727.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:56,075] [train step75170] D loss: 0.32542 G loss: 2.31132 (0.040 sec/batch, 1595.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:56,440] [train step75180] D loss: 0.32554 G loss: 2.24606 (0.036 sec/batch, 1784.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:56,822] [train step75191] D loss: 0.32608 G loss: 2.18928 (0.036 sec/batch, 1802.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:57,206] [train step75200] D loss: 0.32564 G loss: 2.23451 (0.034 sec/batch, 1895.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:57,585] [train step75210] D loss: 0.32587 G loss: 2.40030 (0.038 sec/batch, 1702.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:57,963] [train step75220] D loss: 0.32631 G loss: 2.43866 (0.041 sec/batch, 1553.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:58,363] [train step75230] D loss: 0.32561 G loss: 2.37302 (0.043 sec/batch, 1503.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:58,748] [train step75240] D loss: 0.32538 G loss: 2.26332 (0.041 sec/batch, 1567.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:59,135] [train step75251] D loss: 0.32552 G loss: 2.23791 (0.037 sec/batch, 1729.822 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:59,518] [train step75261] D loss: 0.32615 G loss: 2.18391 (0.037 sec/batch, 1730.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:57:59,903] [train step75270] D loss: 0.32577 G loss: 2.39772 (0.037 sec/batch, 1713.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:00,292] [train step75280] D loss: 0.32558 G loss: 2.23923 (0.039 sec/batch, 1651.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:00,670] [train step75291] D loss: 0.32625 G loss: 2.18325 (0.031 sec/batch, 2046.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:01,059] [train step75300] D loss: 0.32588 G loss: 2.39563 (0.038 sec/batch, 1686.799 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:01,456] [train step75310] D loss: 0.32537 G loss: 2.32071 (0.036 sec/batch, 1766.302 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:01,834] [train step75321] D loss: 0.32539 G loss: 2.30208 (0.036 sec/batch, 1801.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:02,222] [train step75330] D loss: 0.32542 G loss: 2.30924 (0.038 sec/batch, 1665.263 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:02,608] [train step75341] D loss: 0.32536 G loss: 2.31073 (0.037 sec/batch, 1738.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:02,997] [train step75350] D loss: 0.32538 G loss: 2.32084 (0.040 sec/batch, 1581.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:03,406] [train step75360] D loss: 0.32527 G loss: 2.30184 (0.055 sec/batch, 1173.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:03,798] [train step75370] D loss: 0.32605 G loss: 2.41977 (0.039 sec/batch, 1658.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:04,179] [train step75380] D loss: 0.32552 G loss: 2.36108 (0.034 sec/batch, 1874.353 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:04,588] [train step75390] D loss: 0.32546 G loss: 2.24801 (0.039 sec/batch, 1642.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:04,970] [train step75401] D loss: 0.32540 G loss: 2.29037 (0.036 sec/batch, 1754.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:05,356] [train step75411] D loss: 0.32533 G loss: 2.28568 (0.038 sec/batch, 1680.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:05,734] [train step75420] D loss: 0.32535 G loss: 2.31076 (0.036 sec/batch, 1801.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:06,129] [train step75430] D loss: 0.32532 G loss: 2.28901 (0.039 sec/batch, 1657.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:06,526] [train step75441] D loss: 0.32538 G loss: 2.32923 (0.031 sec/batch, 2035.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:06,904] [train step75450] D loss: 0.32531 G loss: 2.30185 (0.037 sec/batch, 1711.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:07,292] [train step75461] D loss: 0.32536 G loss: 2.34020 (0.043 sec/batch, 1501.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:07,674] [train step75470] D loss: 0.32533 G loss: 2.32393 (0.038 sec/batch, 1684.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:08,060] [train step75480] D loss: 0.32533 G loss: 2.31582 (0.037 sec/batch, 1711.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:08,458] [train step75491] D loss: 0.32556 G loss: 2.35054 (0.043 sec/batch, 1502.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:08,837] [train step75500] D loss: 0.32532 G loss: 2.29378 (0.041 sec/batch, 1574.078 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:09,219] [train step75510] D loss: 0.32549 G loss: 2.35272 (0.040 sec/batch, 1617.471 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:09,601] [train step75520] D loss: 0.32584 G loss: 2.39935 (0.041 sec/batch, 1542.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:09,974] [train step75531] D loss: 0.32541 G loss: 2.29157 (0.040 sec/batch, 1615.553 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:10,362] [train step75540] D loss: 0.32537 G loss: 2.33305 (0.048 sec/batch, 1319.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:10,740] [train step75551] D loss: 0.32534 G loss: 2.33719 (0.036 sec/batch, 1779.226 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:11,133] [train step75561] D loss: 0.32533 G loss: 2.30093 (0.034 sec/batch, 1875.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:11,521] [train step75570] D loss: 0.32531 G loss: 2.31496 (0.039 sec/batch, 1638.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:11,903] [train step75580] D loss: 0.32535 G loss: 2.26597 (0.035 sec/batch, 1811.587 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:12,285] [train step75591] D loss: 0.32537 G loss: 2.31823 (0.036 sec/batch, 1763.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:12,675] [train step75600] D loss: 0.32573 G loss: 2.21960 (0.035 sec/batch, 1848.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:13,056] [train step75611] D loss: 0.32596 G loss: 2.20014 (0.039 sec/batch, 1645.521 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:13,438] [train step75621] D loss: 0.32568 G loss: 2.22152 (0.046 sec/batch, 1380.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:13,825] [train step75630] D loss: 0.32569 G loss: 2.38236 (0.040 sec/batch, 1616.429 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:14,220] [train step75640] D loss: 0.32543 G loss: 2.35029 (0.040 sec/batch, 1606.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:14,606] [train step75650] D loss: 0.32538 G loss: 2.33550 (0.040 sec/batch, 1583.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:14,983] [train step75660] D loss: 0.32534 G loss: 2.27620 (0.041 sec/batch, 1557.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:15,361] [train step75670] D loss: 0.32537 G loss: 2.31490 (0.038 sec/batch, 1674.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:15,754] [train step75680] D loss: 0.32536 G loss: 2.29118 (0.040 sec/batch, 1593.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:16,134] [train step75690] D loss: 0.32557 G loss: 2.36276 (0.036 sec/batch, 1754.434 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:16,525] [train step75701] D loss: 0.32557 G loss: 2.37379 (0.041 sec/batch, 1572.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:16,911] [train step75710] D loss: 0.32550 G loss: 2.35048 (0.043 sec/batch, 1494.147 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:17,288] [train step75720] D loss: 0.32537 G loss: 2.31730 (0.038 sec/batch, 1684.766 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:17,677] [train step75730] D loss: 0.32532 G loss: 2.31725 (0.040 sec/batch, 1601.319 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:18,055] [train step75740] D loss: 0.32551 G loss: 2.36243 (0.034 sec/batch, 1874.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:18,445] [train step75750] D loss: 0.32555 G loss: 2.24074 (0.037 sec/batch, 1715.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:18,821] [train step75761] D loss: 0.32531 G loss: 2.31455 (0.038 sec/batch, 1701.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:19,193] [train step75771] D loss: 0.32548 G loss: 2.36413 (0.036 sec/batch, 1778.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:19,569] [train step75780] D loss: 0.32543 G loss: 2.26355 (0.036 sec/batch, 1776.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:19,952] [train step75791] D loss: 0.32526 G loss: 2.29490 (0.038 sec/batch, 1694.711 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:20,321] [train step75800] D loss: 0.32549 G loss: 2.36335 (0.037 sec/batch, 1726.140 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:20,715] [train step75810] D loss: 0.32577 G loss: 2.21496 (0.040 sec/batch, 1595.427 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:21,085] [train step75820] D loss: 0.32570 G loss: 2.21964 (0.036 sec/batch, 1761.295 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:21,455] [train step75830] D loss: 0.32542 G loss: 2.32118 (0.038 sec/batch, 1698.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:21,840] [train step75840] D loss: 0.32548 G loss: 2.25916 (0.032 sec/batch, 2027.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:22,212] [train step75851] D loss: 0.32556 G loss: 2.23740 (0.031 sec/batch, 2076.244 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:22,583] [train step75860] D loss: 0.32541 G loss: 2.26176 (0.030 sec/batch, 2100.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:22,966] [train step75870] D loss: 0.32538 G loss: 2.32823 (0.035 sec/batch, 1820.877 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:23,340] [train step75881] D loss: 0.32545 G loss: 2.25448 (0.028 sec/batch, 2249.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:23,710] [train step75890] D loss: 0.32529 G loss: 2.29879 (0.036 sec/batch, 1760.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:24,088] [train step75900] D loss: 0.32533 G loss: 2.27799 (0.036 sec/batch, 1761.908 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:24,465] [train step75911] D loss: 0.32536 G loss: 2.27383 (0.034 sec/batch, 1855.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:24,860] [train step75921] D loss: 0.32531 G loss: 2.29058 (0.037 sec/batch, 1716.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:25,239] [train step75930] D loss: 0.32548 G loss: 2.35865 (0.038 sec/batch, 1670.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:25,615] [train step75940] D loss: 0.32564 G loss: 2.37612 (0.038 sec/batch, 1680.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:25,995] [train step75951] D loss: 0.32550 G loss: 2.36695 (0.039 sec/batch, 1642.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:26,366] [train step75960] D loss: 0.32545 G loss: 2.24622 (0.039 sec/batch, 1628.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:26,735] [train step75970] D loss: 0.32558 G loss: 2.23478 (0.037 sec/batch, 1750.533 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:27,115] [train step75980] D loss: 0.32531 G loss: 2.28439 (0.033 sec/batch, 1912.710 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:27,489] [train step75990] D loss: 0.32530 G loss: 2.33208 (0.039 sec/batch, 1659.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:27,874] [train step76000] D loss: 0.32531 G loss: 2.29839 (0.047 sec/batch, 1348.183 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:58:27,874] Saved checkpoint at 76000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:28,458] [train step76010] D loss: 0.32538 G loss: 2.32534 (0.041 sec/batch, 1550.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:28,830] [train step76020] D loss: 0.32531 G loss: 2.28293 (0.037 sec/batch, 1713.983 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:29,216] [train step76030] D loss: 0.32549 G loss: 2.24468 (0.040 sec/batch, 1605.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:29,591] [train step76041] D loss: 0.32566 G loss: 2.22057 (0.035 sec/batch, 1820.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:29,974] [train step76050] D loss: 0.32559 G loss: 2.37693 (0.038 sec/batch, 1688.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:30,347] [train step76061] D loss: 0.32537 G loss: 2.25997 (0.038 sec/batch, 1689.580 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:30,719] [train step76070] D loss: 0.32607 G loss: 2.18987 (0.035 sec/batch, 1828.554 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:31,109] [train step76080] D loss: 0.32630 G loss: 2.44007 (0.033 sec/batch, 1920.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:31,486] [train step76091] D loss: 0.32690 G loss: 2.47756 (0.038 sec/batch, 1683.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:31,861] [train step76100] D loss: 0.32689 G loss: 2.47600 (0.041 sec/batch, 1557.249 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:32,243] [train step76110] D loss: 0.32619 G loss: 2.17707 (0.038 sec/batch, 1664.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:32,623] [train step76120] D loss: 0.32597 G loss: 2.19693 (0.038 sec/batch, 1688.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:33,016] [train step76130] D loss: 0.32535 G loss: 2.30032 (0.052 sec/batch, 1235.464 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:33,404] [train step76140] D loss: 0.32549 G loss: 2.28896 (0.036 sec/batch, 1761.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:33,789] [train step76150] D loss: 0.32564 G loss: 2.24708 (0.040 sec/batch, 1612.874 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:34,176] [train step76161] D loss: 0.32575 G loss: 2.38262 (0.034 sec/batch, 1872.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:34,564] [train step76170] D loss: 0.32695 G loss: 2.15246 (0.039 sec/batch, 1630.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:34,945] [train step76180] D loss: 0.32641 G loss: 2.18076 (0.040 sec/batch, 1607.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:35,349] [train step76190] D loss: 0.32563 G loss: 2.25103 (0.040 sec/batch, 1607.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:35,734] [train step76200] D loss: 0.32600 G loss: 2.41223 (0.037 sec/batch, 1732.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:36,118] [train step76210] D loss: 0.32593 G loss: 2.39860 (0.044 sec/batch, 1450.211 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:36,503] [train step76221] D loss: 0.32576 G loss: 2.27552 (0.036 sec/batch, 1781.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:36,885] [train step76230] D loss: 0.32568 G loss: 2.34674 (0.037 sec/batch, 1748.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:37,263] [train step76241] D loss: 0.32591 G loss: 2.28322 (0.036 sec/batch, 1754.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:37,635] [train step76250] D loss: 0.32833 G loss: 2.09269 (0.038 sec/batch, 1663.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:38,011] [train step76260] D loss: 0.33161 G loss: 2.66053 (0.035 sec/batch, 1854.797 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:38,402] [train step76270] D loss: 0.33314 G loss: 2.70458 (0.036 sec/batch, 1758.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:38,783] [train step76280] D loss: 0.33095 G loss: 2.63976 (0.040 sec/batch, 1609.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:39,160] [train step76290] D loss: 0.32640 G loss: 2.25059 (0.045 sec/batch, 1408.201 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:39,537] [train step76301] D loss: 0.32587 G loss: 2.36136 (0.034 sec/batch, 1886.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:39,909] [train step76311] D loss: 0.33988 G loss: 2.86565 (0.036 sec/batch, 1765.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:40,313] [train step76320] D loss: 0.33883 G loss: 1.87047 (0.036 sec/batch, 1799.358 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:40,698] [train step76331] D loss: 0.33239 G loss: 1.98584 (0.037 sec/batch, 1723.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:41,074] [train step76340] D loss: 0.32608 G loss: 2.24251 (0.035 sec/batch, 1818.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:41,464] [train step76350] D loss: 0.32632 G loss: 2.25592 (0.040 sec/batch, 1610.387 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:41,862] [train step76360] D loss: 0.32809 G loss: 2.11538 (0.036 sec/batch, 1764.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:42,244] [train step76370] D loss: 0.32693 G loss: 2.17899 (0.036 sec/batch, 1800.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:42,623] [train step76380] D loss: 0.32687 G loss: 2.45248 (0.036 sec/batch, 1793.815 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:43,003] [train step76391] D loss: 0.32563 G loss: 2.34764 (0.034 sec/batch, 1868.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:43,390] [train step76400] D loss: 0.32556 G loss: 2.31603 (0.034 sec/batch, 1881.750 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:43,769] [train step76410] D loss: 0.32571 G loss: 2.29794 (0.038 sec/batch, 1706.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:44,156] [train step76421] D loss: 0.32642 G loss: 2.20377 (0.041 sec/batch, 1566.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:44,557] [train step76431] D loss: 0.32768 G loss: 2.13797 (0.040 sec/batch, 1605.696 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:44,939] [train step76440] D loss: 0.32743 G loss: 2.49850 (0.038 sec/batch, 1692.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:45,328] [train step76450] D loss: 0.32632 G loss: 2.42374 (0.032 sec/batch, 1972.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:45,720] [train step76461] D loss: 0.32548 G loss: 2.27029 (0.034 sec/batch, 1886.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:46,125] [train step76470] D loss: 0.32614 G loss: 2.40720 (0.037 sec/batch, 1734.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:46,518] [train step76481] D loss: 0.32636 G loss: 2.40814 (0.040 sec/batch, 1619.129 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:46,892] [train step76490] D loss: 0.32650 G loss: 2.42208 (0.038 sec/batch, 1706.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:47,278] [train step76500] D loss: 0.32616 G loss: 2.19411 (0.035 sec/batch, 1826.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:47,665] [train step76511] D loss: 0.32587 G loss: 2.24084 (0.038 sec/batch, 1692.147 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:48,039] [train step76520] D loss: 0.32569 G loss: 2.25443 (0.035 sec/batch, 1823.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:48,426] [train step76530] D loss: 0.32583 G loss: 2.39228 (0.037 sec/batch, 1733.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:48,802] [train step76541] D loss: 0.32595 G loss: 2.40319 (0.035 sec/batch, 1824.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:49,176] [train step76551] D loss: 0.32562 G loss: 2.32371 (0.035 sec/batch, 1808.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:49,550] [train step76560] D loss: 0.32568 G loss: 2.32156 (0.035 sec/batch, 1827.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:49,919] [train step76570] D loss: 0.32554 G loss: 2.28029 (0.036 sec/batch, 1770.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:50,292] [train step76580] D loss: 0.32574 G loss: 2.36089 (0.034 sec/batch, 1867.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:50,671] [train step76590] D loss: 0.32554 G loss: 2.29833 (0.040 sec/batch, 1588.893 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:51,039] [train step76601] D loss: 0.32570 G loss: 2.36693 (0.034 sec/batch, 1876.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:51,420] [train step76610] D loss: 0.32556 G loss: 2.34332 (0.036 sec/batch, 1774.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:51,787] [train step76620] D loss: 0.32548 G loss: 2.31550 (0.028 sec/batch, 2280.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:52,163] [train step76630] D loss: 0.32586 G loss: 2.39289 (0.032 sec/batch, 1969.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:52,536] [train step76641] D loss: 0.32593 G loss: 2.40453 (0.034 sec/batch, 1871.883 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:52,910] [train step76650] D loss: 0.32552 G loss: 2.26281 (0.032 sec/batch, 2018.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:53,286] [train step76661] D loss: 0.32542 G loss: 2.29919 (0.039 sec/batch, 1624.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:53,667] [train step76670] D loss: 0.32559 G loss: 2.29398 (0.037 sec/batch, 1751.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:54,043] [train step76680] D loss: 0.32548 G loss: 2.29946 (0.034 sec/batch, 1869.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:54,413] [train step76691] D loss: 0.32558 G loss: 2.26489 (0.037 sec/batch, 1720.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:54,790] [train step76701] D loss: 0.32548 G loss: 2.28214 (0.036 sec/batch, 1777.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:55,160] [train step76710] D loss: 0.32553 G loss: 2.27395 (0.035 sec/batch, 1839.191 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:55,537] [train step76721] D loss: 0.32551 G loss: 2.25980 (0.035 sec/batch, 1855.066 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:55,923] [train step76730] D loss: 0.32547 G loss: 2.29624 (0.042 sec/batch, 1517.605 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:56,300] [train step76740] D loss: 0.32543 G loss: 2.28176 (0.040 sec/batch, 1610.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:56,696] [train step76750] D loss: 0.32545 G loss: 2.29663 (0.045 sec/batch, 1417.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:57,066] [train step76760] D loss: 0.32542 G loss: 2.26140 (0.040 sec/batch, 1618.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:57,430] [train step76770] D loss: 0.32608 G loss: 2.42122 (0.034 sec/batch, 1894.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:57,817] [train step76780] D loss: 0.32596 G loss: 2.40923 (0.038 sec/batch, 1674.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:58,189] [train step76791] D loss: 0.32546 G loss: 2.28093 (0.037 sec/batch, 1742.442 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:58,582] [train step76800] D loss: 0.32549 G loss: 2.33883 (0.038 sec/batch, 1671.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:58,974] [train step76811] D loss: 0.32571 G loss: 2.37914 (0.037 sec/batch, 1707.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:59,361] [train step76821] D loss: 0.32555 G loss: 2.35676 (0.040 sec/batch, 1616.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:58:59,745] [train step76830] D loss: 0.32576 G loss: 2.22964 (0.037 sec/batch, 1718.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:00,144] [train step76840] D loss: 0.32544 G loss: 2.26767 (0.035 sec/batch, 1840.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:00,539] [train step76850] D loss: 0.32571 G loss: 2.37324 (0.038 sec/batch, 1688.231 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:00,934] [train step76860] D loss: 0.32611 G loss: 2.19842 (0.043 sec/batch, 1481.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:01,316] [train step76871] D loss: 0.32572 G loss: 2.22857 (0.035 sec/batch, 1841.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:01,704] [train step76881] D loss: 0.32571 G loss: 2.36524 (0.037 sec/batch, 1749.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:02,095] [train step76890] D loss: 0.32573 G loss: 2.23848 (0.042 sec/batch, 1534.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:02,477] [train step76900] D loss: 0.32535 G loss: 2.29460 (0.034 sec/batch, 1888.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:02,874] [train step76911] D loss: 0.32545 G loss: 2.30829 (0.043 sec/batch, 1482.782 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:03,264] [train step76920] D loss: 0.32559 G loss: 2.33763 (0.043 sec/batch, 1497.289 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:03,658] [train step76931] D loss: 0.32541 G loss: 2.29724 (0.044 sec/batch, 1440.328 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:04,044] [train step76940] D loss: 0.32558 G loss: 2.25096 (0.037 sec/batch, 1738.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:04,418] [train step76950] D loss: 0.32558 G loss: 2.34503 (0.033 sec/batch, 1950.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:04,796] [train step76960] D loss: 0.32548 G loss: 2.26284 (0.037 sec/batch, 1716.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:05,188] [train step76971] D loss: 0.32547 G loss: 2.28653 (0.040 sec/batch, 1605.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:05,561] [train step76980] D loss: 0.32547 G loss: 2.30622 (0.039 sec/batch, 1638.730 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:05,948] [train step76990] D loss: 0.32548 G loss: 2.31674 (0.043 sec/batch, 1488.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:06,335] [train step77000] D loss: 0.32611 G loss: 2.42020 (0.039 sec/batch, 1640.493 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:59:06,336] Saved checkpoint at 77000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:06,926] [train step77010] D loss: 0.32600 G loss: 2.20870 (0.034 sec/batch, 1905.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:07,331] [train step77021] D loss: 0.32607 G loss: 2.19563 (0.038 sec/batch, 1669.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:07,714] [train step77031] D loss: 0.32581 G loss: 2.22831 (0.042 sec/batch, 1523.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:08,101] [train step77040] D loss: 0.32575 G loss: 2.37245 (0.042 sec/batch, 1537.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:08,476] [train step77050] D loss: 0.32541 G loss: 2.30522 (0.037 sec/batch, 1726.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:08,870] [train step77061] D loss: 0.32552 G loss: 2.25082 (0.040 sec/batch, 1595.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:09,250] [train step77070] D loss: 0.32564 G loss: 2.35925 (0.037 sec/batch, 1745.161 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:09,624] [train step77081] D loss: 0.32558 G loss: 2.35684 (0.034 sec/batch, 1891.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:10,006] [train step77091] D loss: 0.32583 G loss: 2.39006 (0.037 sec/batch, 1744.095 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:10,390] [train step77100] D loss: 0.32545 G loss: 2.26649 (0.038 sec/batch, 1699.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:10,764] [train step77110] D loss: 0.32559 G loss: 2.36377 (0.031 sec/batch, 2078.172 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:11,158] [train step77120] D loss: 0.32535 G loss: 2.31358 (0.040 sec/batch, 1589.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:11,536] [train step77130] D loss: 0.32540 G loss: 2.31964 (0.035 sec/batch, 1814.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:11,910] [train step77140] D loss: 0.32544 G loss: 2.30378 (0.034 sec/batch, 1903.541 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:12,320] [train step77150] D loss: 0.32620 G loss: 2.18691 (0.065 sec/batch, 988.680 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:12,705] [train step77160] D loss: 0.32623 G loss: 2.43014 (0.038 sec/batch, 1676.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:13,086] [train step77170] D loss: 0.32541 G loss: 2.32909 (0.038 sec/batch, 1665.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:13,471] [train step77181] D loss: 0.32551 G loss: 2.25584 (0.041 sec/batch, 1574.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:13,847] [train step77190] D loss: 0.32552 G loss: 2.34429 (0.037 sec/batch, 1751.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:14,236] [train step77200] D loss: 0.32575 G loss: 2.38266 (0.038 sec/batch, 1692.553 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:14,610] [train step77210] D loss: 0.32558 G loss: 2.36687 (0.035 sec/batch, 1809.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:14,981] [train step77220] D loss: 0.32539 G loss: 2.29137 (0.038 sec/batch, 1697.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:15,374] [train step77230] D loss: 0.32569 G loss: 2.37483 (0.041 sec/batch, 1543.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:15,751] [train step77241] D loss: 0.32559 G loss: 2.35318 (0.034 sec/batch, 1910.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:16,134] [train step77250] D loss: 0.32546 G loss: 2.29681 (0.042 sec/batch, 1511.921 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:16,520] [train step77261] D loss: 0.32541 G loss: 2.28624 (0.041 sec/batch, 1571.977 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:16,910] [train step77270] D loss: 0.32552 G loss: 2.32572 (0.038 sec/batch, 1666.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:17,292] [train step77280] D loss: 0.32553 G loss: 2.34020 (0.040 sec/batch, 1588.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:17,672] [train step77291] D loss: 0.32549 G loss: 2.33550 (0.037 sec/batch, 1751.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:18,059] [train step77301] D loss: 0.32550 G loss: 2.31672 (0.039 sec/batch, 1654.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:18,435] [train step77310] D loss: 0.32551 G loss: 2.28744 (0.028 sec/batch, 2259.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:18,812] [train step77321] D loss: 0.32548 G loss: 2.28520 (0.035 sec/batch, 1836.021 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:19,174] [train step77331] D loss: 0.32555 G loss: 2.25531 (0.037 sec/batch, 1730.257 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:19,557] [train step77340] D loss: 0.32598 G loss: 2.20876 (0.036 sec/batch, 1794.127 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:19,923] [train step77350] D loss: 0.32590 G loss: 2.22366 (0.036 sec/batch, 1781.068 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:20,298] [train step77360] D loss: 0.32576 G loss: 2.32398 (0.035 sec/batch, 1847.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:20,682] [train step77370] D loss: 0.32624 G loss: 2.18950 (0.041 sec/batch, 1558.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:21,058] [train step77380] D loss: 0.32793 G loss: 2.10344 (0.038 sec/batch, 1670.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:21,432] [train step77390] D loss: 0.32680 G loss: 2.15463 (0.035 sec/batch, 1817.265 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:21,815] [train step77400] D loss: 0.32833 G loss: 2.54376 (0.043 sec/batch, 1483.954 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:22,187] [train step77410] D loss: 0.33099 G loss: 2.63673 (0.034 sec/batch, 1868.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:22,578] [train step77420] D loss: 0.32767 G loss: 2.50696 (0.037 sec/batch, 1742.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:22,943] [train step77430] D loss: 0.32641 G loss: 2.17289 (0.037 sec/batch, 1742.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:23,313] [train step77441] D loss: 0.32566 G loss: 2.24388 (0.036 sec/batch, 1802.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:23,704] [train step77450] D loss: 0.32594 G loss: 2.38682 (0.038 sec/batch, 1688.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:24,075] [train step77460] D loss: 0.32645 G loss: 2.17625 (0.032 sec/batch, 2004.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:24,458] [train step77470] D loss: 0.32638 G loss: 2.18113 (0.038 sec/batch, 1696.457 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:24,846] [train step77481] D loss: 0.32566 G loss: 2.27027 (0.035 sec/batch, 1811.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:25,228] [train step77490] D loss: 0.32561 G loss: 2.34542 (0.041 sec/batch, 1577.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:25,597] [train step77500] D loss: 0.32552 G loss: 2.28411 (0.029 sec/batch, 2238.398 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:25,975] [train step77511] D loss: 0.32552 G loss: 2.33589 (0.038 sec/batch, 1690.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:26,352] [train step77520] D loss: 0.32565 G loss: 2.25265 (0.037 sec/batch, 1747.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:26,746] [train step77530] D loss: 0.32594 G loss: 2.23941 (0.038 sec/batch, 1697.712 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:27,125] [train step77541] D loss: 0.32579 G loss: 2.23852 (0.037 sec/batch, 1738.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:27,504] [train step77550] D loss: 0.32586 G loss: 2.37879 (0.036 sec/batch, 1753.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:27,887] [train step77561] D loss: 0.32596 G loss: 2.38998 (0.037 sec/batch, 1713.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:28,274] [train step77571] D loss: 0.32700 G loss: 2.47611 (0.038 sec/batch, 1691.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:28,673] [train step77580] D loss: 0.32564 G loss: 2.25804 (0.032 sec/batch, 1984.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:29,060] [train step77590] D loss: 0.32661 G loss: 2.45432 (0.042 sec/batch, 1512.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:29,434] [train step77601] D loss: 0.32753 G loss: 2.50272 (0.038 sec/batch, 1672.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:29,819] [train step77610] D loss: 0.32609 G loss: 2.20520 (0.040 sec/batch, 1598.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:30,195] [train step77621] D loss: 0.32662 G loss: 2.17265 (0.044 sec/batch, 1459.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:30,568] [train step77631] D loss: 0.32654 G loss: 2.17468 (0.038 sec/batch, 1671.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:30,952] [train step77640] D loss: 0.32706 G loss: 2.47867 (0.037 sec/batch, 1716.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:31,328] [train step77650] D loss: 0.32553 G loss: 2.31403 (0.035 sec/batch, 1820.877 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:31,718] [train step77660] D loss: 0.32562 G loss: 2.36328 (0.039 sec/batch, 1630.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:32,096] [train step77670] D loss: 0.32559 G loss: 2.25795 (0.040 sec/batch, 1602.514 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:32,470] [train step77681] D loss: 0.32546 G loss: 2.32849 (0.039 sec/batch, 1633.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:32,865] [train step77690] D loss: 0.32562 G loss: 2.35752 (0.039 sec/batch, 1659.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:33,238] [train step77700] D loss: 0.32562 G loss: 2.23207 (0.037 sec/batch, 1746.308 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:33,615] [train step77711] D loss: 0.32560 G loss: 2.25694 (0.043 sec/batch, 1502.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:34,015] [train step77720] D loss: 0.32540 G loss: 2.30368 (0.039 sec/batch, 1654.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:34,387] [train step77730] D loss: 0.32558 G loss: 2.33579 (0.034 sec/batch, 1859.229 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:34,779] [train step77741] D loss: 0.32577 G loss: 2.23383 (0.043 sec/batch, 1472.582 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:35,174] [train step77751] D loss: 0.32548 G loss: 2.30469 (0.041 sec/batch, 1573.184 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:35,546] [train step77760] D loss: 0.32568 G loss: 2.23251 (0.035 sec/batch, 1813.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:35,928] [train step77770] D loss: 0.32581 G loss: 2.22606 (0.033 sec/batch, 1920.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:36,311] [train step77781] D loss: 0.32607 G loss: 2.19763 (0.038 sec/batch, 1692.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:36,693] [train step77790] D loss: 0.32654 G loss: 2.44834 (0.040 sec/batch, 1601.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:37,087] [train step77801] D loss: 0.32703 G loss: 2.47913 (0.038 sec/batch, 1677.260 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:37,460] [train step77810] D loss: 0.32675 G loss: 2.46177 (0.038 sec/batch, 1704.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:37,831] [train step77820] D loss: 0.32623 G loss: 2.19196 (0.032 sec/batch, 1994.809 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:38,235] [train step77831] D loss: 0.32565 G loss: 2.25208 (0.037 sec/batch, 1749.289 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:38,608] [train step77840] D loss: 0.32558 G loss: 2.29737 (0.035 sec/batch, 1836.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:39,004] [train step77850] D loss: 0.32630 G loss: 2.18543 (0.037 sec/batch, 1734.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:39,379] [train step77861] D loss: 0.32784 G loss: 2.11448 (0.039 sec/batch, 1644.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:39,753] [train step77871] D loss: 0.32625 G loss: 2.19089 (0.039 sec/batch, 1646.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:40,149] [train step77880] D loss: 0.32549 G loss: 2.32622 (0.039 sec/batch, 1650.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:40,529] [train step77890] D loss: 0.32565 G loss: 2.25169 (0.041 sec/batch, 1550.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:40,904] [train step77901] D loss: 0.32542 G loss: 2.30275 (0.036 sec/batch, 1796.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:41,292] [train step77910] D loss: 0.32555 G loss: 2.32937 (0.037 sec/batch, 1739.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:41,681] [train step77921] D loss: 0.32556 G loss: 2.35426 (0.043 sec/batch, 1502.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:42,071] [train step77930] D loss: 0.32548 G loss: 2.29820 (0.031 sec/batch, 2035.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:42,458] [train step77940] D loss: 0.32550 G loss: 2.28580 (0.039 sec/batch, 1636.901 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:42,839] [train step77951] D loss: 0.32550 G loss: 2.27867 (0.035 sec/batch, 1809.108 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:43,229] [train step77961] D loss: 0.32586 G loss: 2.40366 (0.035 sec/batch, 1812.137 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:43,614] [train step77970] D loss: 0.32587 G loss: 2.23376 (0.039 sec/batch, 1642.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:43,989] [train step77981] D loss: 0.32599 G loss: 2.20188 (0.038 sec/batch, 1680.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:44,381] [train step77991] D loss: 0.32599 G loss: 2.21597 (0.036 sec/batch, 1765.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:44,755] [train step78000] D loss: 0.32694 G loss: 2.47298 (0.038 sec/batch, 1689.878 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 04:59:44,756] Saved checkpoint at 78000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:45,366] [train step78010] D loss: 0.32640 G loss: 2.43549 (0.042 sec/batch, 1525.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:45,740] [train step78020] D loss: 0.32639 G loss: 2.43768 (0.036 sec/batch, 1779.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:46,133] [train step78030] D loss: 0.32560 G loss: 2.25238 (0.040 sec/batch, 1616.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:46,515] [train step78041] D loss: 0.32549 G loss: 2.33629 (0.036 sec/batch, 1785.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:46,901] [train step78050] D loss: 0.32571 G loss: 2.37071 (0.039 sec/batch, 1622.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:47,303] [train step78060] D loss: 0.32609 G loss: 2.19150 (0.040 sec/batch, 1614.494 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:47,677] [train step78070] D loss: 0.32623 G loss: 2.18566 (0.035 sec/batch, 1840.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:48,057] [train step78080] D loss: 0.32621 G loss: 2.19150 (0.036 sec/batch, 1766.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:48,452] [train step78090] D loss: 0.32565 G loss: 2.36319 (0.043 sec/batch, 1503.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:48,820] [train step78101] D loss: 0.32543 G loss: 2.29578 (0.031 sec/batch, 2034.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:49,199] [train step78111] D loss: 0.32615 G loss: 2.19171 (0.046 sec/batch, 1386.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:49,571] [train step78120] D loss: 0.32635 G loss: 2.42858 (0.035 sec/batch, 1816.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:49,959] [train step78131] D loss: 0.32568 G loss: 2.37301 (0.038 sec/batch, 1704.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:50,341] [train step78140] D loss: 0.32560 G loss: 2.33623 (0.036 sec/batch, 1789.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:50,712] [train step78150] D loss: 0.32544 G loss: 2.32103 (0.032 sec/batch, 2021.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:51,082] [train step78161] D loss: 0.32544 G loss: 2.32152 (0.034 sec/batch, 1909.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:51,455] [train step78171] D loss: 0.32558 G loss: 2.28834 (0.040 sec/batch, 1583.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:51,822] [train step78180] D loss: 0.32552 G loss: 2.33125 (0.034 sec/batch, 1867.494 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:52,198] [train step78190] D loss: 0.32557 G loss: 2.34339 (0.038 sec/batch, 1662.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:52,574] [train step78200] D loss: 0.32557 G loss: 2.26259 (0.034 sec/batch, 1876.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:52,947] [train step78210] D loss: 0.32552 G loss: 2.35290 (0.035 sec/batch, 1828.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:53,330] [train step78220] D loss: 0.32554 G loss: 2.32948 (0.043 sec/batch, 1479.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:53,713] [train step78230] D loss: 0.32577 G loss: 2.22575 (0.041 sec/batch, 1565.696 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:54,082] [train step78240] D loss: 0.32563 G loss: 2.36978 (0.035 sec/batch, 1840.288 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:54,466] [train step78251] D loss: 0.32562 G loss: 2.34506 (0.038 sec/batch, 1674.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:54,849] [train step78260] D loss: 0.32543 G loss: 2.33919 (0.042 sec/batch, 1508.005 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:55,220] [train step78270] D loss: 0.32551 G loss: 2.25717 (0.035 sec/batch, 1820.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:55,600] [train step78281] D loss: 0.32577 G loss: 2.24339 (0.037 sec/batch, 1723.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:55,971] [train step78291] D loss: 0.32667 G loss: 2.16048 (0.038 sec/batch, 1693.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:56,338] [train step78300] D loss: 0.32659 G loss: 2.45271 (0.038 sec/batch, 1669.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:56,715] [train step78310] D loss: 0.32591 G loss: 2.37751 (0.034 sec/batch, 1863.294 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:57,085] [train step78321] D loss: 0.32551 G loss: 2.26034 (0.036 sec/batch, 1800.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:57,473] [train step78330] D loss: 0.32560 G loss: 2.36236 (0.043 sec/batch, 1486.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:57,849] [train step78340] D loss: 0.32581 G loss: 2.38944 (0.037 sec/batch, 1722.816 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:58,231] [train step78350] D loss: 0.32573 G loss: 2.39157 (0.037 sec/batch, 1706.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:58,620] [train step78360] D loss: 0.32598 G loss: 2.20256 (0.038 sec/batch, 1698.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:58,998] [train step78371] D loss: 0.32582 G loss: 2.22403 (0.037 sec/batch, 1743.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:59,370] [train step78380] D loss: 0.32575 G loss: 2.23854 (0.037 sec/batch, 1752.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 04:59:59,754] [train step78390] D loss: 0.32536 G loss: 2.33724 (0.039 sec/batch, 1657.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:00,148] [train step78400] D loss: 0.32551 G loss: 2.33234 (0.035 sec/batch, 1824.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:00,538] [train step78411] D loss: 0.32539 G loss: 2.33595 (0.039 sec/batch, 1648.492 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:00,914] [train step78420] D loss: 0.32553 G loss: 2.27431 (0.039 sec/batch, 1633.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:01,293] [train step78430] D loss: 0.32544 G loss: 2.31538 (0.037 sec/batch, 1748.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:01,678] [train step78441] D loss: 0.32723 G loss: 2.48982 (0.039 sec/batch, 1631.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:02,059] [train step78450] D loss: 0.32722 G loss: 2.13379 (0.038 sec/batch, 1696.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:02,432] [train step78460] D loss: 0.32664 G loss: 2.16266 (0.030 sec/batch, 2120.126 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:02,825] [train step78470] D loss: 0.32561 G loss: 2.23991 (0.035 sec/batch, 1829.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:03,209] [train step78480] D loss: 0.32561 G loss: 2.35900 (0.040 sec/batch, 1592.956 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:03,588] [train step78490] D loss: 0.32551 G loss: 2.34414 (0.038 sec/batch, 1680.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:03,974] [train step78501] D loss: 0.32552 G loss: 2.34340 (0.036 sec/batch, 1775.954 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:04,352] [train step78510] D loss: 0.32550 G loss: 2.29380 (0.033 sec/batch, 1920.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:04,735] [train step78521] D loss: 0.32554 G loss: 2.32384 (0.036 sec/batch, 1775.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:05,115] [train step78530] D loss: 0.32545 G loss: 2.33674 (0.036 sec/batch, 1760.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:05,499] [train step78540] D loss: 0.32542 G loss: 2.33739 (0.037 sec/batch, 1740.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:05,889] [train step78551] D loss: 0.32693 G loss: 2.47315 (0.037 sec/batch, 1737.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:06,265] [train step78560] D loss: 0.32711 G loss: 2.48060 (0.045 sec/batch, 1434.027 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:06,643] [train step78570] D loss: 0.32650 G loss: 2.16682 (0.038 sec/batch, 1682.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:07,037] [train step78580] D loss: 0.32663 G loss: 2.16564 (0.038 sec/batch, 1676.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:07,417] [train step78591] D loss: 0.32577 G loss: 2.22473 (0.036 sec/batch, 1779.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:07,793] [train step78600] D loss: 0.32545 G loss: 2.32932 (0.032 sec/batch, 1993.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:08,172] [train step78611] D loss: 0.32553 G loss: 2.25887 (0.036 sec/batch, 1767.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:08,552] [train step78620] D loss: 0.32545 G loss: 2.26825 (0.037 sec/batch, 1720.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:08,940] [train step78630] D loss: 0.32565 G loss: 2.37283 (0.035 sec/batch, 1851.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:09,326] [train step78640] D loss: 0.32563 G loss: 2.37652 (0.040 sec/batch, 1613.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:09,704] [train step78650] D loss: 0.32565 G loss: 2.37001 (0.039 sec/batch, 1624.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:10,093] [train step78660] D loss: 0.32561 G loss: 2.24415 (0.036 sec/batch, 1765.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:10,475] [train step78671] D loss: 0.32539 G loss: 2.28688 (0.036 sec/batch, 1755.857 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:10,874] [train step78680] D loss: 0.32540 G loss: 2.30304 (0.039 sec/batch, 1622.094 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:11,252] [train step78690] D loss: 0.32542 G loss: 2.34052 (0.039 sec/batch, 1645.249 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:11,625] [train step78701] D loss: 0.32542 G loss: 2.32230 (0.036 sec/batch, 1785.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:12,019] [train step78711] D loss: 0.32561 G loss: 2.23734 (0.038 sec/batch, 1687.286 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:12,402] [train step78720] D loss: 0.32569 G loss: 2.37737 (0.036 sec/batch, 1757.904 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:12,765] [train step78730] D loss: 0.32533 G loss: 2.28394 (0.026 sec/batch, 2491.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:13,156] [train step78740] D loss: 0.32559 G loss: 2.24184 (0.031 sec/batch, 2073.565 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:13,540] [train step78750] D loss: 0.32539 G loss: 2.31725 (0.037 sec/batch, 1729.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:13,936] [train step78761] D loss: 0.32553 G loss: 2.24944 (0.035 sec/batch, 1808.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:14,315] [train step78771] D loss: 0.32578 G loss: 2.21966 (0.034 sec/batch, 1895.423 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:14,696] [train step78780] D loss: 0.32569 G loss: 2.37824 (0.036 sec/batch, 1778.920 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:15,090] [train step78791] D loss: 0.32573 G loss: 2.38061 (0.038 sec/batch, 1703.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:15,470] [train step78801] D loss: 0.32542 G loss: 2.30908 (0.040 sec/batch, 1598.858 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:15,842] [train step78810] D loss: 0.32548 G loss: 2.34449 (0.037 sec/batch, 1730.937 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:16,226] [train step78821] D loss: 0.32549 G loss: 2.26310 (0.035 sec/batch, 1840.427 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:16,607] [train step78831] D loss: 0.32563 G loss: 2.23099 (0.037 sec/batch, 1741.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:16,988] [train step78840] D loss: 0.32576 G loss: 2.38710 (0.041 sec/batch, 1559.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:17,380] [train step78850] D loss: 0.32557 G loss: 2.35619 (0.037 sec/batch, 1733.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:17,753] [train step78860] D loss: 0.32546 G loss: 2.27878 (0.040 sec/batch, 1610.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:18,141] [train step78870] D loss: 0.32538 G loss: 2.34788 (0.036 sec/batch, 1757.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:18,512] [train step78881] D loss: 0.32534 G loss: 2.30513 (0.036 sec/batch, 1769.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:18,885] [train step78891] D loss: 0.32537 G loss: 2.27660 (0.036 sec/batch, 1769.434 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:19,265] [train step78900] D loss: 0.32542 G loss: 2.34554 (0.039 sec/batch, 1642.038 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:19,629] [train step78910] D loss: 0.32560 G loss: 2.37943 (0.035 sec/batch, 1815.888 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:20,006] [train step78920] D loss: 0.32537 G loss: 2.28684 (0.037 sec/batch, 1735.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:20,384] [train step78930] D loss: 0.32541 G loss: 2.33512 (0.036 sec/batch, 1782.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:20,751] [train step78941] D loss: 0.32537 G loss: 2.32931 (0.036 sec/batch, 1790.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:21,149] [train step78950] D loss: 0.32534 G loss: 2.30991 (0.054 sec/batch, 1181.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:21,515] [train step78960] D loss: 0.32550 G loss: 2.34817 (0.036 sec/batch, 1798.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:21,889] [train step78970] D loss: 0.32555 G loss: 2.36851 (0.033 sec/batch, 1949.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:22,266] [train step78981] D loss: 0.32546 G loss: 2.35472 (0.036 sec/batch, 1776.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:22,645] [train step78990] D loss: 0.32531 G loss: 2.29371 (0.035 sec/batch, 1834.879 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:23,013] [train step79001] D loss: 0.32549 G loss: 2.36376 (0.035 sec/batch, 1841.664 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:00:23,013] Saved checkpoint at 79000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:23,598] [train step79010] D loss: 0.32536 G loss: 2.32022 (0.037 sec/batch, 1730.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:23,962] [train step79020] D loss: 0.32538 G loss: 2.28383 (0.036 sec/batch, 1770.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:24,350] [train step79031] D loss: 0.32535 G loss: 2.27185 (0.038 sec/batch, 1689.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:24,718] [train step79040] D loss: 0.32537 G loss: 2.29728 (0.037 sec/batch, 1713.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:25,099] [train step79050] D loss: 0.32536 G loss: 2.29250 (0.036 sec/batch, 1760.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:25,478] [train step79061] D loss: 0.32534 G loss: 2.30372 (0.040 sec/batch, 1593.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:25,851] [train step79071] D loss: 0.32536 G loss: 2.28386 (0.037 sec/batch, 1738.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:26,236] [train step79080] D loss: 0.32532 G loss: 2.30038 (0.048 sec/batch, 1339.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:26,610] [train step79091] D loss: 0.32546 G loss: 2.26117 (0.035 sec/batch, 1821.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:26,982] [train step79101] D loss: 0.32542 G loss: 2.33165 (0.036 sec/batch, 1797.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:27,359] [train step79110] D loss: 0.32555 G loss: 2.23988 (0.027 sec/batch, 2333.775 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:27,745] [train step79120] D loss: 0.32544 G loss: 2.26292 (0.037 sec/batch, 1708.822 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:28,123] [train step79131] D loss: 0.32574 G loss: 2.22500 (0.037 sec/batch, 1750.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:28,511] [train step79140] D loss: 0.32539 G loss: 2.31599 (0.035 sec/batch, 1814.513 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:28,882] [train step79151] D loss: 0.32548 G loss: 2.28273 (0.038 sec/batch, 1667.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:29,260] [train step79160] D loss: 0.32543 G loss: 2.29065 (0.037 sec/batch, 1745.706 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:29,644] [train step79170] D loss: 0.32566 G loss: 2.37394 (0.036 sec/batch, 1790.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:30,030] [train step79181] D loss: 0.32559 G loss: 2.34592 (0.036 sec/batch, 1768.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:30,426] [train step79191] D loss: 0.32556 G loss: 2.35783 (0.038 sec/batch, 1704.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:30,803] [train step79200] D loss: 0.32539 G loss: 2.30467 (0.038 sec/batch, 1705.608 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:31,179] [train step79210] D loss: 0.32554 G loss: 2.36075 (0.040 sec/batch, 1619.813 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:31,562] [train step79221] D loss: 0.32552 G loss: 2.35348 (0.037 sec/batch, 1752.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:31,938] [train step79230] D loss: 0.32568 G loss: 2.23143 (0.038 sec/batch, 1667.166 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:32,313] [train step79240] D loss: 0.32671 G loss: 2.17918 (0.037 sec/batch, 1735.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:32,718] [train step79251] D loss: 0.33218 G loss: 1.99224 (0.036 sec/batch, 1792.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:33,096] [train step79260] D loss: 0.34667 G loss: 2.97584 (0.038 sec/batch, 1692.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:33,482] [train step79271] D loss: 0.81482 G loss: 8.13787 (0.037 sec/batch, 1743.495 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:33,856] [train step79280] D loss: 0.44535 G loss: 4.30932 (0.034 sec/batch, 1885.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:34,245] [train step79290] D loss: 0.46995 G loss: 1.13096 (0.039 sec/batch, 1655.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:34,644] [train step79300] D loss: 0.40779 G loss: 3.86445 (0.035 sec/batch, 1810.035 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:35,025] [train step79311] D loss: 0.36368 G loss: 1.61519 (0.036 sec/batch, 1790.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:35,420] [train step79320] D loss: 0.33979 G loss: 2.83246 (0.045 sec/batch, 1426.733 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:35,814] [train step79330] D loss: 0.32901 G loss: 2.13378 (0.039 sec/batch, 1638.040 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:36,197] [train step79340] D loss: 0.32735 G loss: 2.41364 (0.038 sec/batch, 1691.689 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:36,590] [train step79350] D loss: 0.32746 G loss: 2.42722 (0.042 sec/batch, 1529.155 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:36,975] [train step79360] D loss: 0.32664 G loss: 2.29931 (0.043 sec/batch, 1472.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:37,370] [train step79370] D loss: 0.32655 G loss: 2.34501 (0.037 sec/batch, 1723.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:37,760] [train step79380] D loss: 0.32666 G loss: 2.27344 (0.038 sec/batch, 1680.284 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:38,142] [train step79391] D loss: 0.32696 G loss: 2.29197 (0.039 sec/batch, 1622.212 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:38,532] [train step79401] D loss: 0.32615 G loss: 2.31966 (0.039 sec/batch, 1648.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:38,931] [train step79410] D loss: 0.32668 G loss: 2.34563 (0.040 sec/batch, 1614.533 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:39,321] [train step79420] D loss: 0.32624 G loss: 2.31482 (0.046 sec/batch, 1388.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:39,722] [train step79431] D loss: 0.32633 G loss: 2.25645 (0.039 sec/batch, 1646.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:40,109] [train step79440] D loss: 0.32621 G loss: 2.29615 (0.045 sec/batch, 1419.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:40,501] [train step79451] D loss: 0.32631 G loss: 2.33258 (0.036 sec/batch, 1780.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:40,890] [train step79460] D loss: 0.32652 G loss: 2.27142 (0.036 sec/batch, 1790.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:41,278] [train step79470] D loss: 0.32628 G loss: 2.32968 (0.039 sec/batch, 1649.606 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:41,664] [train step79481] D loss: 0.32622 G loss: 2.25496 (0.041 sec/batch, 1569.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:42,069] [train step79490] D loss: 0.32650 G loss: 2.28069 (0.040 sec/batch, 1604.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:42,447] [train step79500] D loss: 0.32630 G loss: 2.29993 (0.035 sec/batch, 1819.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:42,846] [train step79511] D loss: 0.32652 G loss: 2.32564 (0.037 sec/batch, 1730.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:43,248] [train step79521] D loss: 0.32642 G loss: 2.24820 (0.038 sec/batch, 1676.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:43,637] [train step79530] D loss: 0.32656 G loss: 2.26521 (0.038 sec/batch, 1688.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:44,034] [train step79540] D loss: 0.32637 G loss: 2.37940 (0.043 sec/batch, 1487.795 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:44,424] [train step79550] D loss: 0.32629 G loss: 2.26555 (0.035 sec/batch, 1837.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:44,838] [train step79560] D loss: 0.32600 G loss: 2.30522 (0.036 sec/batch, 1761.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:45,228] [train step79570] D loss: 0.32588 G loss: 2.32441 (0.041 sec/batch, 1564.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:45,631] [train step79580] D loss: 0.32618 G loss: 2.31553 (0.041 sec/batch, 1555.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:46,035] [train step79590] D loss: 0.32604 G loss: 2.30482 (0.038 sec/batch, 1668.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:46,423] [train step79601] D loss: 0.32578 G loss: 2.30459 (0.038 sec/batch, 1690.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:46,811] [train step79611] D loss: 0.32598 G loss: 2.26874 (0.047 sec/batch, 1372.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:47,198] [train step79620] D loss: 0.32644 G loss: 2.23054 (0.037 sec/batch, 1744.481 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:47,586] [train step79631] D loss: 0.32617 G loss: 2.33379 (0.044 sec/batch, 1452.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:47,984] [train step79641] D loss: 0.32648 G loss: 2.27370 (0.036 sec/batch, 1788.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:48,370] [train step79650] D loss: 0.32652 G loss: 2.37185 (0.036 sec/batch, 1784.490 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:48,764] [train step79661] D loss: 0.32640 G loss: 2.24919 (0.037 sec/batch, 1748.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:49,155] [train step79671] D loss: 0.32649 G loss: 2.32252 (0.038 sec/batch, 1686.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:49,523] [train step79680] D loss: 0.32619 G loss: 2.35033 (0.038 sec/batch, 1679.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:49,899] [train step79690] D loss: 0.32707 G loss: 2.41242 (0.036 sec/batch, 1767.732 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:50,289] [train step79701] D loss: 0.32636 G loss: 2.28335 (0.035 sec/batch, 1821.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:50,664] [train step79710] D loss: 0.32678 G loss: 2.21478 (0.040 sec/batch, 1600.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:51,048] [train step79720] D loss: 0.32768 G loss: 2.30091 (0.038 sec/batch, 1671.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:51,429] [train step79731] D loss: 0.32710 G loss: 2.41718 (0.039 sec/batch, 1646.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:51,808] [train step79740] D loss: 0.34564 G loss: 1.84087 (0.041 sec/batch, 1545.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:52,203] [train step79750] D loss: 0.42755 G loss: 1.36884 (0.042 sec/batch, 1522.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:52,573] [train step79761] D loss: 1.09235 G loss: 10.39526 (0.034 sec/batch, 1896.293 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:52,948] [train step79770] D loss: 0.83775 G loss: 8.15816 (0.036 sec/batch, 1793.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:53,338] [train step79780] D loss: 0.35925 G loss: 1.95246 (0.036 sec/batch, 1758.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:53,735] [train step79791] D loss: 0.35540 G loss: 3.05940 (0.041 sec/batch, 1558.614 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:54,119] [train step79800] D loss: 0.34717 G loss: 2.92373 (0.038 sec/batch, 1681.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:54,504] [train step79810] D loss: 0.33703 G loss: 1.98732 (0.034 sec/batch, 1868.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:54,885] [train step79820] D loss: 0.33129 G loss: 2.47838 (0.037 sec/batch, 1751.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:55,269] [train step79830] D loss: 0.32963 G loss: 2.26355 (0.041 sec/batch, 1568.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:55,646] [train step79841] D loss: 0.33018 G loss: 2.32548 (0.038 sec/batch, 1678.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:56,024] [train step79850] D loss: 0.32958 G loss: 2.33718 (0.036 sec/batch, 1770.122 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:56,421] [train step79860] D loss: 0.32924 G loss: 2.28949 (0.038 sec/batch, 1686.407 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:56,803] [train step79870] D loss: 0.32946 G loss: 2.42124 (0.038 sec/batch, 1684.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:57,194] [train step79880] D loss: 0.32815 G loss: 2.36482 (0.038 sec/batch, 1676.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:57,579] [train step79890] D loss: 0.32876 G loss: 2.31540 (0.045 sec/batch, 1437.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:57,957] [train step79900] D loss: 0.32837 G loss: 2.31454 (0.039 sec/batch, 1640.503 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:58,347] [train step79911] D loss: 0.32829 G loss: 2.24430 (0.036 sec/batch, 1762.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:58,728] [train step79920] D loss: 0.32842 G loss: 2.37501 (0.037 sec/batch, 1730.948 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:59,113] [train step79930] D loss: 0.32765 G loss: 2.32865 (0.036 sec/batch, 1769.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:59,503] [train step79941] D loss: 0.32778 G loss: 2.27554 (0.037 sec/batch, 1732.915 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:00:59,885] [train step79950] D loss: 0.32804 G loss: 2.32566 (0.036 sec/batch, 1785.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:00,280] [train step79961] D loss: 0.32789 G loss: 2.34527 (0.039 sec/batch, 1648.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:00,671] [train step79970] D loss: 0.32880 G loss: 2.40714 (0.034 sec/batch, 1893.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:01,052] [train step79980] D loss: 0.32818 G loss: 2.28963 (0.037 sec/batch, 1736.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:01,441] [train step79990] D loss: 0.32827 G loss: 2.28947 (0.034 sec/batch, 1871.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:01,822] [train step80001] D loss: 0.32809 G loss: 2.31260 (0.039 sec/batch, 1647.642 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:01:01,823] Saved checkpoint at 80000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:02,434] [train step80010] D loss: 0.32798 G loss: 2.26113 (0.042 sec/batch, 1532.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:02,824] [train step80021] D loss: 0.32695 G loss: 2.32014 (0.037 sec/batch, 1711.688 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:03,209] [train step80031] D loss: 0.32795 G loss: 2.37374 (0.042 sec/batch, 1518.008 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:03,599] [train step80040] D loss: 0.32779 G loss: 2.31352 (0.036 sec/batch, 1783.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:03,988] [train step80051] D loss: 0.32759 G loss: 2.31748 (0.037 sec/batch, 1750.122 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:04,395] [train step80060] D loss: 0.32678 G loss: 2.30816 (0.035 sec/batch, 1824.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:04,788] [train step80070] D loss: 0.32747 G loss: 2.36662 (0.037 sec/batch, 1716.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:05,174] [train step80080] D loss: 0.32754 G loss: 2.26204 (0.037 sec/batch, 1717.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:05,564] [train step80090] D loss: 0.32699 G loss: 2.35954 (0.038 sec/batch, 1687.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:05,956] [train step80100] D loss: 0.32725 G loss: 2.35196 (0.039 sec/batch, 1645.914 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:06,340] [train step80111] D loss: 0.32783 G loss: 2.24338 (0.038 sec/batch, 1697.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:06,733] [train step80120] D loss: 0.32770 G loss: 2.37282 (0.034 sec/batch, 1884.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:07,120] [train step80130] D loss: 0.32797 G loss: 2.24259 (0.042 sec/batch, 1527.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:07,524] [train step80141] D loss: 0.32698 G loss: 2.34098 (0.038 sec/batch, 1705.760 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:07,905] [train step80151] D loss: 0.32712 G loss: 2.31579 (0.038 sec/batch, 1673.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:08,290] [train step80160] D loss: 0.32703 G loss: 2.35065 (0.039 sec/batch, 1634.728 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:08,691] [train step80171] D loss: 0.32763 G loss: 2.26655 (0.038 sec/batch, 1692.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:09,076] [train step80181] D loss: 0.32682 G loss: 2.35408 (0.039 sec/batch, 1659.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:09,468] [train step80190] D loss: 0.32744 G loss: 2.42450 (0.044 sec/batch, 1461.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:09,853] [train step80201] D loss: 0.32756 G loss: 2.33838 (0.036 sec/batch, 1801.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:10,244] [train step80210] D loss: 0.32742 G loss: 2.32283 (0.041 sec/batch, 1555.832 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:10,636] [train step80220] D loss: 0.32691 G loss: 2.33430 (0.040 sec/batch, 1598.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:11,018] [train step80231] D loss: 0.32665 G loss: 2.29948 (0.035 sec/batch, 1846.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:11,410] [train step80240] D loss: 0.32643 G loss: 2.25929 (0.043 sec/batch, 1494.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:11,804] [train step80250] D loss: 0.32703 G loss: 2.31459 (0.038 sec/batch, 1668.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:12,188] [train step80261] D loss: 0.32711 G loss: 2.35046 (0.036 sec/batch, 1787.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:12,584] [train step80271] D loss: 0.32703 G loss: 2.29585 (0.046 sec/batch, 1401.313 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:12,970] [train step80280] D loss: 0.32664 G loss: 2.34468 (0.037 sec/batch, 1736.109 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:13,358] [train step80291] D loss: 0.32701 G loss: 2.27694 (0.036 sec/batch, 1758.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:13,746] [train step80301] D loss: 0.32679 G loss: 2.31598 (0.039 sec/batch, 1661.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:14,125] [train step80310] D loss: 0.32654 G loss: 2.32739 (0.035 sec/batch, 1851.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:14,514] [train step80321] D loss: 0.32721 G loss: 2.25601 (0.045 sec/batch, 1425.482 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:14,924] [train step80331] D loss: 0.32725 G loss: 2.32425 (0.037 sec/batch, 1729.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:15,312] [train step80340] D loss: 0.32746 G loss: 2.28144 (0.042 sec/batch, 1516.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:15,702] [train step80351] D loss: 0.32720 G loss: 2.33835 (0.045 sec/batch, 1415.784 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:16,086] [train step80360] D loss: 0.32695 G loss: 2.32013 (0.037 sec/batch, 1735.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:16,476] [train step80370] D loss: 0.32687 G loss: 2.27431 (0.042 sec/batch, 1519.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:16,879] [train step80380] D loss: 0.32667 G loss: 2.31406 (0.038 sec/batch, 1690.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:17,260] [train step80390] D loss: 0.32727 G loss: 2.21292 (0.038 sec/batch, 1689.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:17,643] [train step80400] D loss: 0.32721 G loss: 2.39131 (0.039 sec/batch, 1646.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:18,040] [train step80410] D loss: 0.32735 G loss: 2.22689 (0.034 sec/batch, 1899.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:18,427] [train step80421] D loss: 0.32655 G loss: 2.29994 (0.038 sec/batch, 1705.392 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:18,825] [train step80430] D loss: 0.32802 G loss: 2.18644 (0.039 sec/batch, 1634.320 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:19,212] [train step80440] D loss: 0.32741 G loss: 2.39968 (0.039 sec/batch, 1633.107 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:19,579] [train step80451] D loss: 0.32686 G loss: 2.30371 (0.036 sec/batch, 1801.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:19,958] [train step80460] D loss: 0.32701 G loss: 2.34660 (0.034 sec/batch, 1891.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:20,334] [train step80470] D loss: 0.32654 G loss: 2.32854 (0.038 sec/batch, 1692.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:20,702] [train step80480] D loss: 0.32660 G loss: 2.36338 (0.039 sec/batch, 1658.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:21,077] [train step80490] D loss: 0.32657 G loss: 2.38533 (0.037 sec/batch, 1729.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:21,452] [train step80501] D loss: 0.32650 G loss: 2.30505 (0.035 sec/batch, 1807.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:21,824] [train step80510] D loss: 0.32634 G loss: 2.31878 (0.035 sec/batch, 1836.147 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:22,208] [train step80520] D loss: 0.32634 G loss: 2.28911 (0.039 sec/batch, 1634.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:22,581] [train step80530] D loss: 0.32667 G loss: 2.23561 (0.038 sec/batch, 1671.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:22,960] [train step80540] D loss: 0.32656 G loss: 2.35620 (0.037 sec/batch, 1736.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:23,332] [train step80550] D loss: 0.32661 G loss: 2.34113 (0.036 sec/batch, 1801.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:23,704] [train step80560] D loss: 0.32643 G loss: 2.37837 (0.037 sec/batch, 1736.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:24,086] [train step80570] D loss: 0.32658 G loss: 2.33480 (0.040 sec/batch, 1592.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:24,467] [train step80580] D loss: 0.32706 G loss: 2.26736 (0.036 sec/batch, 1786.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:24,841] [train step80591] D loss: 0.32653 G loss: 2.33384 (0.037 sec/batch, 1731.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:25,246] [train step80601] D loss: 0.32666 G loss: 2.28552 (0.036 sec/batch, 1779.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:25,612] [train step80610] D loss: 0.32643 G loss: 2.33405 (0.038 sec/batch, 1686.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:26,000] [train step80620] D loss: 0.32641 G loss: 2.37898 (0.040 sec/batch, 1592.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:26,373] [train step80631] D loss: 0.32630 G loss: 2.33648 (0.036 sec/batch, 1773.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:26,742] [train step80640] D loss: 0.32618 G loss: 2.31279 (0.041 sec/batch, 1564.637 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:27,129] [train step80651] D loss: 0.32633 G loss: 2.30768 (0.038 sec/batch, 1694.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:27,523] [train step80660] D loss: 0.32631 G loss: 2.28185 (0.036 sec/batch, 1783.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:27,913] [train step80670] D loss: 0.32670 G loss: 2.32416 (0.041 sec/batch, 1559.085 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:28,305] [train step80681] D loss: 0.32641 G loss: 2.27551 (0.037 sec/batch, 1724.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:28,689] [train step80691] D loss: 0.32609 G loss: 2.29203 (0.036 sec/batch, 1795.351 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:29,078] [train step80700] D loss: 0.32649 G loss: 2.29912 (0.050 sec/batch, 1284.116 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:29,458] [train step80710] D loss: 0.32646 G loss: 2.34042 (0.034 sec/batch, 1880.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:29,838] [train step80720] D loss: 0.32663 G loss: 2.28905 (0.042 sec/batch, 1539.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:30,223] [train step80730] D loss: 0.32612 G loss: 2.31304 (0.044 sec/batch, 1457.265 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:30,613] [train step80740] D loss: 0.32721 G loss: 2.32184 (0.038 sec/batch, 1666.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:30,994] [train step80750] D loss: 0.32611 G loss: 2.33883 (0.043 sec/batch, 1473.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:31,389] [train step80760] D loss: 0.32626 G loss: 2.33162 (0.036 sec/batch, 1790.668 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:31,773] [train step80770] D loss: 0.32623 G loss: 2.30778 (0.041 sec/batch, 1559.257 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:32,172] [train step80781] D loss: 0.32595 G loss: 2.33808 (0.037 sec/batch, 1729.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:32,566] [train step80790] D loss: 0.32611 G loss: 2.28378 (0.038 sec/batch, 1676.161 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:32,947] [train step80801] D loss: 0.32665 G loss: 2.31233 (0.037 sec/batch, 1718.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:33,344] [train step80811] D loss: 0.32667 G loss: 2.35456 (0.036 sec/batch, 1766.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:33,730] [train step80820] D loss: 0.32641 G loss: 2.30633 (0.037 sec/batch, 1722.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:34,114] [train step80831] D loss: 0.32606 G loss: 2.32547 (0.034 sec/batch, 1875.610 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:34,505] [train step80841] D loss: 0.32654 G loss: 2.32569 (0.038 sec/batch, 1666.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:34,887] [train step80850] D loss: 0.32645 G loss: 2.26354 (0.036 sec/batch, 1791.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:35,285] [train step80860] D loss: 0.32670 G loss: 2.34540 (0.043 sec/batch, 1486.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:35,688] [train step80870] D loss: 0.32672 G loss: 2.26213 (0.053 sec/batch, 1206.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:36,079] [train step80880] D loss: 0.32707 G loss: 2.23699 (0.038 sec/batch, 1665.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:36,471] [train step80890] D loss: 0.32728 G loss: 2.22290 (0.037 sec/batch, 1712.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:36,856] [train step80901] D loss: 0.32704 G loss: 2.36147 (0.042 sec/batch, 1520.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:37,242] [train step80910] D loss: 0.32720 G loss: 2.28186 (0.035 sec/batch, 1825.495 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:37,652] [train step80921] D loss: 0.32637 G loss: 2.25018 (0.040 sec/batch, 1617.539 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:38,039] [train step80930] D loss: 0.32697 G loss: 2.39770 (0.038 sec/batch, 1679.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:38,427] [train step80940] D loss: 0.32650 G loss: 2.34454 (0.036 sec/batch, 1773.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:38,810] [train step80951] D loss: 0.32766 G loss: 2.17260 (0.040 sec/batch, 1617.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:39,187] [train step80960] D loss: 0.32776 G loss: 2.42426 (0.035 sec/batch, 1807.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:39,587] [train step80970] D loss: 0.32747 G loss: 2.34422 (0.040 sec/batch, 1601.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:39,973] [train step80981] D loss: 0.32807 G loss: 2.18832 (0.037 sec/batch, 1739.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:40,367] [train step80991] D loss: 0.32786 G loss: 2.46231 (0.038 sec/batch, 1696.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:40,767] [train step81000] D loss: 0.32652 G loss: 2.32255 (0.037 sec/batch, 1728.452 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:01:40,768] Saved checkpoint at 81000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:41,385] [train step81010] D loss: 0.32860 G loss: 2.44603 (0.039 sec/batch, 1631.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:41,798] [train step81020] D loss: 0.32754 G loss: 2.34496 (0.039 sec/batch, 1649.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:42,185] [train step81030] D loss: 0.32869 G loss: 2.34743 (0.040 sec/batch, 1613.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:42,572] [train step81040] D loss: 0.32728 G loss: 2.33694 (0.043 sec/batch, 1493.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:42,956] [train step81051] D loss: 0.32793 G loss: 2.20013 (0.042 sec/batch, 1535.927 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:43,342] [train step81060] D loss: 0.33574 G loss: 2.01327 (0.036 sec/batch, 1775.449 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:43,743] [train step81070] D loss: 0.33015 G loss: 2.47194 (0.038 sec/batch, 1663.509 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:44,129] [train step81080] D loss: 0.33816 G loss: 2.78606 (0.039 sec/batch, 1659.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:44,518] [train step81090] D loss: 0.32791 G loss: 2.33599 (0.043 sec/batch, 1477.982 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:44,902] [train step81101] D loss: 0.32777 G loss: 2.37004 (0.036 sec/batch, 1763.505 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:45,288] [train step81111] D loss: 0.32776 G loss: 2.40311 (0.044 sec/batch, 1461.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:45,687] [train step81120] D loss: 0.32690 G loss: 2.38713 (0.042 sec/batch, 1512.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:46,076] [train step81131] D loss: 0.32835 G loss: 2.46962 (0.037 sec/batch, 1730.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:46,483] [train step81140] D loss: 0.32910 G loss: 2.17840 (0.038 sec/batch, 1682.338 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:46,870] [train step81150] D loss: 0.32772 G loss: 2.35209 (0.034 sec/batch, 1857.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:47,247] [train step81161] D loss: 0.32729 G loss: 2.23963 (0.033 sec/batch, 1915.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:47,648] [train step81171] D loss: 0.32823 G loss: 2.38054 (0.045 sec/batch, 1407.167 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:48,041] [train step81180] D loss: 0.32796 G loss: 2.34562 (0.048 sec/batch, 1345.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:48,428] [train step81190] D loss: 0.32739 G loss: 2.26543 (0.044 sec/batch, 1464.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:48,816] [train step81201] D loss: 0.32699 G loss: 2.23908 (0.037 sec/batch, 1706.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:49,206] [train step81210] D loss: 0.32726 G loss: 2.27195 (0.039 sec/batch, 1661.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:49,592] [train step81221] D loss: 0.32708 G loss: 2.31748 (0.039 sec/batch, 1626.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:49,979] [train step81231] D loss: 0.32791 G loss: 2.43453 (0.037 sec/batch, 1708.268 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:50,360] [train step81240] D loss: 0.34210 G loss: 2.87921 (0.038 sec/batch, 1668.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:50,743] [train step81251] D loss: 0.34935 G loss: 1.76662 (0.038 sec/batch, 1689.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:51,108] [train step81260] D loss: 0.33345 G loss: 2.64330 (0.031 sec/batch, 2092.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:51,488] [train step81270] D loss: 0.32835 G loss: 2.28126 (0.036 sec/batch, 1758.249 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:51,882] [train step81281] D loss: 0.32954 G loss: 2.46731 (0.042 sec/batch, 1523.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:52,269] [train step81290] D loss: 0.32764 G loss: 2.39774 (0.047 sec/batch, 1357.394 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:52,646] [train step81300] D loss: 0.32761 G loss: 2.36188 (0.037 sec/batch, 1747.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:53,034] [train step81311] D loss: 0.32701 G loss: 2.34536 (0.036 sec/batch, 1760.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:53,412] [train step81321] D loss: 0.32706 G loss: 2.38207 (0.035 sec/batch, 1820.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:53,803] [train step81330] D loss: 0.32753 G loss: 2.30181 (0.051 sec/batch, 1266.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:54,167] [train step81340] D loss: 0.32778 G loss: 2.43784 (0.027 sec/batch, 2329.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:54,548] [train step81351] D loss: 0.32715 G loss: 2.29848 (0.034 sec/batch, 1890.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:54,945] [train step81360] D loss: 0.32736 G loss: 2.33274 (0.041 sec/batch, 1560.327 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:55,327] [train step81370] D loss: 0.32708 G loss: 2.31239 (0.041 sec/batch, 1550.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:55,708] [train step81381] D loss: 0.32755 G loss: 2.24074 (0.036 sec/batch, 1771.980 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:56,094] [train step81390] D loss: 0.32795 G loss: 2.21007 (0.034 sec/batch, 1860.053 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:56,472] [train step81401] D loss: 0.32672 G loss: 2.26727 (0.036 sec/batch, 1794.511 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:56,880] [train step81411] D loss: 0.32657 G loss: 2.31726 (0.049 sec/batch, 1294.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:57,258] [train step81420] D loss: 0.32699 G loss: 2.36595 (0.041 sec/batch, 1542.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:57,631] [train step81430] D loss: 0.32698 G loss: 2.29420 (0.035 sec/batch, 1817.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:58,021] [train step81441] D loss: 0.32676 G loss: 2.35945 (0.036 sec/batch, 1758.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:58,398] [train step81450] D loss: 0.32694 G loss: 2.32576 (0.035 sec/batch, 1827.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:58,784] [train step81461] D loss: 0.32731 G loss: 2.38251 (0.044 sec/batch, 1463.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:59,172] [train step81471] D loss: 0.32726 G loss: 2.22834 (0.037 sec/batch, 1712.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:59,553] [train step81480] D loss: 0.32688 G loss: 2.34516 (0.037 sec/batch, 1739.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:01:59,933] [train step81490] D loss: 0.32675 G loss: 2.28509 (0.041 sec/batch, 1563.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:00,334] [train step81500] D loss: 0.32681 G loss: 2.28488 (0.037 sec/batch, 1720.101 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:00,738] [train step81510] D loss: 0.32647 G loss: 2.31537 (0.035 sec/batch, 1832.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:01,133] [train step81520] D loss: 0.32674 G loss: 2.29645 (0.036 sec/batch, 1786.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:01,524] [train step81530] D loss: 0.32654 G loss: 2.34775 (0.034 sec/batch, 1862.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:01,904] [train step81540] D loss: 0.32680 G loss: 2.35382 (0.030 sec/batch, 2113.282 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:02,305] [train step81550] D loss: 0.32642 G loss: 2.28072 (0.041 sec/batch, 1557.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:02,694] [train step81560] D loss: 0.32631 G loss: 2.32564 (0.044 sec/batch, 1466.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:03,101] [train step81570] D loss: 0.32630 G loss: 2.26560 (0.049 sec/batch, 1312.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:03,487] [train step81580] D loss: 0.32641 G loss: 2.33379 (0.037 sec/batch, 1730.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:03,876] [train step81590] D loss: 0.32675 G loss: 2.26618 (0.039 sec/batch, 1657.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:04,264] [train step81600] D loss: 0.32651 G loss: 2.31385 (0.034 sec/batch, 1867.455 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:04,643] [train step81610] D loss: 0.32611 G loss: 2.30229 (0.035 sec/batch, 1812.577 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:05,034] [train step81620] D loss: 0.32681 G loss: 2.39455 (0.036 sec/batch, 1771.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:05,436] [train step81630] D loss: 0.32677 G loss: 2.36304 (0.042 sec/batch, 1540.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:05,824] [train step81640] D loss: 0.32652 G loss: 2.23163 (0.040 sec/batch, 1615.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:06,222] [train step81651] D loss: 0.32646 G loss: 2.32331 (0.034 sec/batch, 1895.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:06,613] [train step81660] D loss: 0.32630 G loss: 2.29133 (0.039 sec/batch, 1639.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:07,004] [train step81671] D loss: 0.32669 G loss: 2.38319 (0.044 sec/batch, 1453.288 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:07,413] [train step81681] D loss: 0.32652 G loss: 2.24902 (0.038 sec/batch, 1662.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:07,817] [train step81690] D loss: 0.32667 G loss: 2.21185 (0.043 sec/batch, 1505.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:08,219] [train step81701] D loss: 0.32630 G loss: 2.37481 (0.045 sec/batch, 1427.772 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:08,607] [train step81711] D loss: 0.32615 G loss: 2.31540 (0.035 sec/batch, 1827.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:08,994] [train step81720] D loss: 0.32645 G loss: 2.36582 (0.043 sec/batch, 1498.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:09,384] [train step81730] D loss: 0.32650 G loss: 2.25999 (0.038 sec/batch, 1685.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:09,766] [train step81741] D loss: 0.32610 G loss: 2.33502 (0.040 sec/batch, 1599.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:10,153] [train step81750] D loss: 0.32620 G loss: 2.30284 (0.040 sec/batch, 1617.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:10,550] [train step81760] D loss: 0.32630 G loss: 2.29065 (0.040 sec/batch, 1603.298 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:10,935] [train step81771] D loss: 0.32626 G loss: 2.28923 (0.039 sec/batch, 1646.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:11,338] [train step81780] D loss: 0.32636 G loss: 2.34960 (0.042 sec/batch, 1534.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:11,734] [train step81790] D loss: 0.32681 G loss: 2.20379 (0.038 sec/batch, 1703.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:12,121] [train step81800] D loss: 0.32629 G loss: 2.30856 (0.036 sec/batch, 1790.871 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:12,514] [train step81810] D loss: 0.32669 G loss: 2.27571 (0.037 sec/batch, 1713.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:12,903] [train step81821] D loss: 0.32680 G loss: 2.37884 (0.035 sec/batch, 1825.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:13,280] [train step81831] D loss: 0.32612 G loss: 2.31893 (0.035 sec/batch, 1821.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:13,677] [train step81840] D loss: 0.32626 G loss: 2.33900 (0.037 sec/batch, 1709.791 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:14,055] [train step81851] D loss: 0.32627 G loss: 2.26017 (0.039 sec/batch, 1627.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:14,448] [train step81861] D loss: 0.32649 G loss: 2.37012 (0.038 sec/batch, 1681.621 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:14,833] [train step81870] D loss: 0.37271 G loss: 1.61159 (0.041 sec/batch, 1547.045 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:15,210] [train step81880] D loss: 0.33571 G loss: 2.11264 (0.039 sec/batch, 1629.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:15,602] [train step81891] D loss: 0.33133 G loss: 2.55912 (0.036 sec/batch, 1765.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:15,977] [train step81900] D loss: 0.32905 G loss: 2.33050 (0.037 sec/batch, 1739.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:16,369] [train step81910] D loss: 0.32787 G loss: 2.27020 (0.037 sec/batch, 1712.463 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:16,767] [train step81920] D loss: 0.32779 G loss: 2.32970 (0.040 sec/batch, 1590.700 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:17,143] [train step81930] D loss: 0.32938 G loss: 2.23277 (0.035 sec/batch, 1814.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:17,538] [train step81941] D loss: 0.32789 G loss: 2.36965 (0.039 sec/batch, 1650.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:17,921] [train step81950] D loss: 0.32704 G loss: 2.31664 (0.041 sec/batch, 1573.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:18,318] [train step81960] D loss: 0.32707 G loss: 2.33647 (0.038 sec/batch, 1683.815 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:18,717] [train step81971] D loss: 0.32703 G loss: 2.36389 (0.037 sec/batch, 1733.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:19,093] [train step81980] D loss: 0.32659 G loss: 2.31137 (0.034 sec/batch, 1877.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:19,489] [train step81990] D loss: 0.32685 G loss: 2.30364 (0.043 sec/batch, 1503.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:19,877] [train step82001] D loss: 0.32719 G loss: 2.38052 (0.036 sec/batch, 1766.976 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:02:19,878] Saved checkpoint at 82000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:20,473] [train step82010] D loss: 0.32662 G loss: 2.31895 (0.037 sec/batch, 1708.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:20,858] [train step82020] D loss: 0.32741 G loss: 2.33828 (0.035 sec/batch, 1830.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:21,235] [train step82030] D loss: 0.32758 G loss: 2.42561 (0.039 sec/batch, 1638.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:21,614] [train step82040] D loss: 0.32672 G loss: 2.26266 (0.036 sec/batch, 1773.959 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:22,007] [train step82050] D loss: 0.32673 G loss: 2.36201 (0.037 sec/batch, 1746.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:22,379] [train step82060] D loss: 0.32695 G loss: 2.31268 (0.034 sec/batch, 1858.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:22,762] [train step82070] D loss: 0.32691 G loss: 2.37304 (0.037 sec/batch, 1735.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:23,137] [train step82080] D loss: 0.32678 G loss: 2.32410 (0.039 sec/batch, 1634.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:23,521] [train step82091] D loss: 0.32692 G loss: 2.35857 (0.036 sec/batch, 1792.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:23,903] [train step82100] D loss: 0.32662 G loss: 2.32925 (0.039 sec/batch, 1657.050 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:24,277] [train step82110] D loss: 0.32663 G loss: 2.30019 (0.036 sec/batch, 1790.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:24,658] [train step82120] D loss: 0.32643 G loss: 2.33817 (0.050 sec/batch, 1272.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:25,030] [train step82131] D loss: 0.32645 G loss: 2.29575 (0.036 sec/batch, 1774.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:25,411] [train step82140] D loss: 0.32635 G loss: 2.34130 (0.034 sec/batch, 1895.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:25,807] [train step82150] D loss: 0.32629 G loss: 2.32413 (0.041 sec/batch, 1565.824 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:26,185] [train step82160] D loss: 0.32652 G loss: 2.30544 (0.042 sec/batch, 1535.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:26,565] [train step82170] D loss: 0.32659 G loss: 2.32158 (0.039 sec/batch, 1624.430 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:26,948] [train step82181] D loss: 0.32662 G loss: 2.27004 (0.036 sec/batch, 1799.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:27,328] [train step82190] D loss: 0.32653 G loss: 2.30738 (0.038 sec/batch, 1700.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:27,701] [train step82200] D loss: 0.32618 G loss: 2.31604 (0.034 sec/batch, 1905.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:28,086] [train step82211] D loss: 0.32655 G loss: 2.25714 (0.031 sec/batch, 2068.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:28,474] [train step82220] D loss: 0.32643 G loss: 2.27355 (0.044 sec/batch, 1465.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:28,880] [train step82230] D loss: 0.32624 G loss: 2.27604 (0.039 sec/batch, 1639.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:29,258] [train step82241] D loss: 0.32629 G loss: 2.29178 (0.038 sec/batch, 1667.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:29,634] [train step82251] D loss: 0.32649 G loss: 2.36625 (0.035 sec/batch, 1825.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:30,018] [train step82260] D loss: 0.32649 G loss: 2.29270 (0.036 sec/batch, 1782.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:30,394] [train step82271] D loss: 0.32651 G loss: 2.36950 (0.036 sec/batch, 1783.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:30,779] [train step82281] D loss: 0.32642 G loss: 2.36638 (0.039 sec/batch, 1628.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:31,171] [train step82290] D loss: 0.32622 G loss: 2.33804 (0.042 sec/batch, 1516.610 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:31,549] [train step82300] D loss: 0.32616 G loss: 2.34879 (0.039 sec/batch, 1631.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:31,931] [train step82311] D loss: 0.32631 G loss: 2.27877 (0.033 sec/batch, 1929.511 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:32,320] [train step82320] D loss: 0.32661 G loss: 2.34172 (0.043 sec/batch, 1482.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:32,694] [train step82330] D loss: 0.32592 G loss: 2.29213 (0.035 sec/batch, 1813.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:33,089] [train step82340] D loss: 0.32609 G loss: 2.31703 (0.038 sec/batch, 1702.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:33,485] [train step82350] D loss: 0.32627 G loss: 2.36092 (0.043 sec/batch, 1501.434 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:33,871] [train step82361] D loss: 0.32617 G loss: 2.26983 (0.042 sec/batch, 1521.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:34,275] [train step82370] D loss: 0.32623 G loss: 2.31312 (0.042 sec/batch, 1505.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:34,664] [train step82380] D loss: 0.32624 G loss: 2.29238 (0.040 sec/batch, 1601.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:35,052] [train step82391] D loss: 0.32622 G loss: 2.30230 (0.035 sec/batch, 1812.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:35,441] [train step82400] D loss: 0.32678 G loss: 2.24535 (0.044 sec/batch, 1460.881 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:35,824] [train step82410] D loss: 0.32597 G loss: 2.29433 (0.039 sec/batch, 1625.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:36,224] [train step82420] D loss: 0.32608 G loss: 2.34676 (0.040 sec/batch, 1581.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:36,605] [train step82431] D loss: 0.32601 G loss: 2.29636 (0.037 sec/batch, 1735.716 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:36,993] [train step82440] D loss: 0.32604 G loss: 2.29354 (0.044 sec/batch, 1449.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:37,402] [train step82451] D loss: 0.32690 G loss: 2.22681 (0.041 sec/batch, 1560.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:37,781] [train step82461] D loss: 0.32641 G loss: 2.32557 (0.036 sec/batch, 1755.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:38,190] [train step82470] D loss: 0.32662 G loss: 2.30054 (0.036 sec/batch, 1759.609 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:38,579] [train step82480] D loss: 0.32688 G loss: 2.41596 (0.046 sec/batch, 1377.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:38,969] [train step82490] D loss: 0.32663 G loss: 2.31744 (0.037 sec/batch, 1709.617 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:39,369] [train step82500] D loss: 0.32651 G loss: 2.33010 (0.032 sec/batch, 2020.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:39,752] [train step82510] D loss: 0.32661 G loss: 2.29280 (0.038 sec/batch, 1688.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:40,144] [train step82521] D loss: 0.32648 G loss: 2.34726 (0.035 sec/batch, 1822.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:40,531] [train step82530] D loss: 0.32591 G loss: 2.35204 (0.039 sec/batch, 1624.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:40,913] [train step82540] D loss: 0.32612 G loss: 2.31688 (0.042 sec/batch, 1541.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:41,301] [train step82551] D loss: 0.32622 G loss: 2.30190 (0.037 sec/batch, 1738.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:41,685] [train step82560] D loss: 0.32610 G loss: 2.33143 (0.038 sec/batch, 1681.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:42,075] [train step82570] D loss: 0.32583 G loss: 2.27435 (0.037 sec/batch, 1749.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:42,474] [train step82580] D loss: 0.32631 G loss: 2.31751 (0.039 sec/batch, 1661.923 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:42,859] [train step82590] D loss: 0.32605 G loss: 2.26586 (0.038 sec/batch, 1668.908 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:43,261] [train step82600] D loss: 0.32609 G loss: 2.33347 (0.037 sec/batch, 1715.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:43,646] [train step82610] D loss: 0.32645 G loss: 2.29255 (0.035 sec/batch, 1815.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:44,032] [train step82620] D loss: 0.32623 G loss: 2.27652 (0.039 sec/batch, 1629.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:44,430] [train step82630] D loss: 0.32601 G loss: 2.34288 (0.040 sec/batch, 1594.281 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:44,826] [train step82640] D loss: 0.32614 G loss: 2.33014 (0.041 sec/batch, 1561.725 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:45,209] [train step82650] D loss: 0.32606 G loss: 2.33999 (0.035 sec/batch, 1823.487 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:45,603] [train step82661] D loss: 0.32593 G loss: 2.33029 (0.039 sec/batch, 1631.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:45,988] [train step82670] D loss: 0.32595 G loss: 2.32593 (0.041 sec/batch, 1568.074 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:46,393] [train step82680] D loss: 0.32595 G loss: 2.30290 (0.043 sec/batch, 1496.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:46,782] [train step82690] D loss: 0.32610 G loss: 2.31563 (0.048 sec/batch, 1321.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:47,171] [train step82700] D loss: 0.32607 G loss: 2.26836 (0.039 sec/batch, 1641.887 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:47,562] [train step82710] D loss: 0.32615 G loss: 2.34027 (0.041 sec/batch, 1576.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:47,936] [train step82720] D loss: 0.32585 G loss: 2.28197 (0.042 sec/batch, 1530.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:48,328] [train step82731] D loss: 0.32605 G loss: 2.29473 (0.046 sec/batch, 1396.349 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:48,715] [train step82740] D loss: 0.32624 G loss: 2.23999 (0.034 sec/batch, 1864.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:49,099] [train step82751] D loss: 0.32611 G loss: 2.32775 (0.037 sec/batch, 1752.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:49,492] [train step82761] D loss: 0.32591 G loss: 2.30618 (0.041 sec/batch, 1578.967 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:49,885] [train step82770] D loss: 0.32632 G loss: 2.33411 (0.040 sec/batch, 1618.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:50,267] [train step82780] D loss: 0.32613 G loss: 2.28455 (0.038 sec/batch, 1687.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:50,691] [train step82791] D loss: 0.32612 G loss: 2.33419 (0.042 sec/batch, 1512.696 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:51,086] [train step82800] D loss: 0.32617 G loss: 2.27269 (0.034 sec/batch, 1878.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:51,492] [train step82811] D loss: 0.32586 G loss: 2.32040 (0.048 sec/batch, 1338.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:51,877] [train step82821] D loss: 0.32584 G loss: 2.33879 (0.041 sec/batch, 1575.104 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:52,256] [train step82830] D loss: 0.32609 G loss: 2.29021 (0.028 sec/batch, 2323.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:52,656] [train step82841] D loss: 0.32579 G loss: 2.33914 (0.039 sec/batch, 1639.120 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:53,036] [train step82851] D loss: 0.32603 G loss: 2.26588 (0.036 sec/batch, 1801.919 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:53,416] [train step82860] D loss: 0.32598 G loss: 2.35450 (0.040 sec/batch, 1590.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:53,812] [train step82871] D loss: 0.32576 G loss: 2.28966 (0.036 sec/batch, 1757.317 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:54,196] [train step82880] D loss: 0.32580 G loss: 2.30374 (0.038 sec/batch, 1682.759 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:54,596] [train step82890] D loss: 0.32583 G loss: 2.32167 (0.040 sec/batch, 1607.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:54,978] [train step82901] D loss: 0.32614 G loss: 2.37710 (0.036 sec/batch, 1764.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:55,366] [train step82911] D loss: 0.32587 G loss: 2.31340 (0.037 sec/batch, 1731.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:55,756] [train step82920] D loss: 0.32587 G loss: 2.29391 (0.034 sec/batch, 1905.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:56,152] [train step82930] D loss: 0.32570 G loss: 2.32024 (0.041 sec/batch, 1572.852 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:56,542] [train step82941] D loss: 0.32582 G loss: 2.33453 (0.041 sec/batch, 1549.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:56,938] [train step82950] D loss: 0.32580 G loss: 2.31653 (0.034 sec/batch, 1868.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:57,330] [train step82960] D loss: 0.32569 G loss: 2.31422 (0.038 sec/batch, 1693.108 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:57,717] [train step82970] D loss: 0.32583 G loss: 2.27744 (0.039 sec/batch, 1645.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:58,107] [train step82980] D loss: 0.32575 G loss: 2.30327 (0.038 sec/batch, 1703.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:58,494] [train step82990] D loss: 0.32575 G loss: 2.32481 (0.043 sec/batch, 1478.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:58,898] [train step83000] D loss: 0.32578 G loss: 2.31823 (0.040 sec/batch, 1617.481 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:02:58,898] Saved checkpoint at 83000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:59,504] [train step83010] D loss: 0.32577 G loss: 2.34736 (0.033 sec/batch, 1939.184 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:02:59,898] [train step83021] D loss: 0.32562 G loss: 2.31058 (0.038 sec/batch, 1666.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:00,292] [train step83031] D loss: 0.32579 G loss: 2.33146 (0.036 sec/batch, 1754.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:00,699] [train step83040] D loss: 0.32599 G loss: 2.34782 (0.044 sec/batch, 1470.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:01,103] [train step83050] D loss: 0.32592 G loss: 2.32190 (0.038 sec/batch, 1670.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:01,486] [train step83061] D loss: 0.32617 G loss: 2.26519 (0.040 sec/batch, 1602.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:01,881] [train step83070] D loss: 0.32585 G loss: 2.32353 (0.041 sec/batch, 1567.735 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:02,262] [train step83081] D loss: 0.32583 G loss: 2.31809 (0.042 sec/batch, 1508.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:02,641] [train step83090] D loss: 0.32577 G loss: 2.34419 (0.037 sec/batch, 1731.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:03,033] [train step83100] D loss: 0.32594 G loss: 2.28867 (0.040 sec/batch, 1613.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:03,411] [train step83111] D loss: 0.32597 G loss: 2.29049 (0.035 sec/batch, 1836.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:03,807] [train step83121] D loss: 0.32568 G loss: 2.32249 (0.046 sec/batch, 1385.545 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:04,197] [train step83130] D loss: 0.32580 G loss: 2.25924 (0.043 sec/batch, 1476.616 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:04,589] [train step83141] D loss: 0.32563 G loss: 2.30983 (0.038 sec/batch, 1678.970 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:04,987] [train step83150] D loss: 0.32591 G loss: 2.28840 (0.039 sec/batch, 1627.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:05,377] [train step83160] D loss: 0.32583 G loss: 2.27982 (0.039 sec/batch, 1623.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:05,756] [train step83171] D loss: 0.32595 G loss: 2.33719 (0.035 sec/batch, 1844.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:06,150] [train step83181] D loss: 0.32583 G loss: 2.33431 (0.039 sec/batch, 1660.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:06,536] [train step83190] D loss: 0.32596 G loss: 2.30429 (0.034 sec/batch, 1902.785 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:06,924] [train step83201] D loss: 0.32570 G loss: 2.32047 (0.047 sec/batch, 1368.403 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:07,297] [train step83210] D loss: 0.32582 G loss: 2.31709 (0.027 sec/batch, 2354.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:07,683] [train step83220] D loss: 0.32598 G loss: 2.29735 (0.042 sec/batch, 1538.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:08,095] [train step83231] D loss: 0.32605 G loss: 2.30594 (0.042 sec/batch, 1518.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:08,491] [train step83240] D loss: 0.32656 G loss: 2.40276 (0.038 sec/batch, 1667.974 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:08,872] [train step83250] D loss: 0.32598 G loss: 2.32627 (0.039 sec/batch, 1643.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:09,269] [train step83260] D loss: 0.32606 G loss: 2.37911 (0.041 sec/batch, 1556.861 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:09,653] [train step83271] D loss: 0.32626 G loss: 2.26989 (0.036 sec/batch, 1773.420 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:10,043] [train step83280] D loss: 0.32601 G loss: 2.27443 (0.037 sec/batch, 1733.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:10,425] [train step83291] D loss: 0.32624 G loss: 2.29053 (0.037 sec/batch, 1749.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:10,816] [train step83301] D loss: 0.32587 G loss: 2.32618 (0.034 sec/batch, 1879.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:11,222] [train step83310] D loss: 0.32590 G loss: 2.28189 (0.040 sec/batch, 1602.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:11,610] [train step83321] D loss: 0.32616 G loss: 2.33869 (0.035 sec/batch, 1821.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:11,987] [train step83330] D loss: 0.32581 G loss: 2.31795 (0.039 sec/batch, 1633.455 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:12,373] [train step83340] D loss: 0.32591 G loss: 2.28811 (0.035 sec/batch, 1829.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:12,760] [train step83351] D loss: 0.32629 G loss: 2.19979 (0.039 sec/batch, 1658.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:13,154] [train step83360] D loss: 0.32577 G loss: 2.31999 (0.037 sec/batch, 1737.053 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:13,542] [train step83370] D loss: 0.32622 G loss: 2.22738 (0.040 sec/batch, 1593.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:13,920] [train step83381] D loss: 0.32653 G loss: 2.23569 (0.039 sec/batch, 1653.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:14,317] [train step83391] D loss: 0.32713 G loss: 2.44952 (0.037 sec/batch, 1727.439 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:14,699] [train step83400] D loss: 0.32601 G loss: 2.34173 (0.032 sec/batch, 2018.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:15,090] [train step83411] D loss: 0.32626 G loss: 2.23755 (0.035 sec/batch, 1853.849 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:15,478] [train step83420] D loss: 0.32595 G loss: 2.32373 (0.039 sec/batch, 1638.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:15,863] [train step83430] D loss: 0.32654 G loss: 2.35616 (0.042 sec/batch, 1514.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:16,261] [train step83440] D loss: 0.32631 G loss: 2.36995 (0.038 sec/batch, 1678.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:16,651] [train step83450] D loss: 0.32656 G loss: 2.38917 (0.035 sec/batch, 1834.277 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:17,038] [train step83460] D loss: 0.32624 G loss: 2.31955 (0.039 sec/batch, 1624.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:17,427] [train step83470] D loss: 0.32655 G loss: 2.31572 (0.038 sec/batch, 1704.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:17,819] [train step83481] D loss: 0.32676 G loss: 2.23299 (0.037 sec/batch, 1728.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:18,235] [train step83490] D loss: 0.32701 G loss: 2.27249 (0.044 sec/batch, 1467.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:18,622] [train step83500] D loss: 0.32666 G loss: 2.22034 (0.042 sec/batch, 1509.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:19,007] [train step83510] D loss: 0.32640 G loss: 2.34888 (0.040 sec/batch, 1584.457 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:19,401] [train step83520] D loss: 0.32616 G loss: 2.32279 (0.038 sec/batch, 1689.049 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:19,789] [train step83530] D loss: 0.32654 G loss: 2.29498 (0.043 sec/batch, 1501.611 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:20,171] [train step83541] D loss: 0.32636 G loss: 2.35097 (0.037 sec/batch, 1730.959 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:20,571] [train step83550] D loss: 0.32616 G loss: 2.27520 (0.042 sec/batch, 1521.174 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:20,957] [train step83560] D loss: 0.32691 G loss: 2.37841 (0.037 sec/batch, 1729.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:21,343] [train step83571] D loss: 0.32644 G loss: 2.23795 (0.036 sec/batch, 1774.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:21,718] [train step83580] D loss: 0.32607 G loss: 2.33442 (0.033 sec/batch, 1929.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:22,129] [train step83590] D loss: 0.32648 G loss: 2.29106 (0.036 sec/batch, 1757.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:22,504] [train step83600] D loss: 0.32638 G loss: 2.33748 (0.028 sec/batch, 2310.692 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:22,896] [train step83610] D loss: 0.32629 G loss: 2.28791 (0.038 sec/batch, 1697.680 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:23,263] [train step83620] D loss: 0.32610 G loss: 2.26286 (0.025 sec/batch, 2510.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:23,660] [train step83631] D loss: 0.32618 G loss: 2.29212 (0.042 sec/batch, 1524.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:24,045] [train step83640] D loss: 0.32597 G loss: 2.26499 (0.036 sec/batch, 1761.249 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:24,427] [train step83651] D loss: 0.32627 G loss: 2.25568 (0.033 sec/batch, 1950.867 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:24,810] [train step83660] D loss: 0.32601 G loss: 2.30397 (0.042 sec/batch, 1524.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:25,192] [train step83670] D loss: 0.32647 G loss: 2.38992 (0.039 sec/batch, 1636.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:25,582] [train step83681] D loss: 0.32645 G loss: 2.21973 (0.037 sec/batch, 1709.344 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:25,964] [train step83690] D loss: 0.32594 G loss: 2.33419 (0.041 sec/batch, 1573.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:26,341] [train step83700] D loss: 0.32602 G loss: 2.26123 (0.033 sec/batch, 1936.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:26,732] [train step83710] D loss: 0.32577 G loss: 2.34427 (0.038 sec/batch, 1685.686 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:27,105] [train step83721] D loss: 0.32609 G loss: 2.25743 (0.031 sec/batch, 2094.076 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:27,498] [train step83730] D loss: 0.32649 G loss: 2.41699 (0.046 sec/batch, 1390.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:27,887] [train step83741] D loss: 0.32594 G loss: 2.27524 (0.036 sec/batch, 1761.029 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:28,268] [train step83750] D loss: 0.32579 G loss: 2.31976 (0.036 sec/batch, 1765.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:28,670] [train step83760] D loss: 0.32580 G loss: 2.28565 (0.043 sec/batch, 1492.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:29,045] [train step83771] D loss: 0.32568 G loss: 2.30302 (0.031 sec/batch, 2093.733 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:29,434] [train step83781] D loss: 0.32575 G loss: 2.27827 (0.037 sec/batch, 1728.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:29,829] [train step83790] D loss: 0.32576 G loss: 2.29460 (0.038 sec/batch, 1690.059 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:30,210] [train step83800] D loss: 0.32570 G loss: 2.31068 (0.043 sec/batch, 1476.721 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:30,603] [train step83811] D loss: 0.32576 G loss: 2.28459 (0.038 sec/batch, 1693.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:30,988] [train step83820] D loss: 0.32596 G loss: 2.26767 (0.041 sec/batch, 1574.429 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:31,369] [train step83831] D loss: 0.32586 G loss: 2.35270 (0.036 sec/batch, 1787.044 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:31,759] [train step83841] D loss: 0.32588 G loss: 2.30406 (0.036 sec/batch, 1793.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:32,143] [train step83850] D loss: 0.32569 G loss: 2.29922 (0.036 sec/batch, 1792.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:32,527] [train step83861] D loss: 0.32596 G loss: 2.34238 (0.035 sec/batch, 1817.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:32,934] [train step83870] D loss: 0.32611 G loss: 2.26466 (0.038 sec/batch, 1671.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:33,320] [train step83880] D loss: 0.32573 G loss: 2.31124 (0.040 sec/batch, 1608.400 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:33,714] [train step83891] D loss: 0.32595 G loss: 2.30734 (0.046 sec/batch, 1380.990 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:34,104] [train step83901] D loss: 0.32571 G loss: 2.31392 (0.038 sec/batch, 1702.385 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:34,490] [train step83910] D loss: 0.32565 G loss: 2.29060 (0.038 sec/batch, 1705.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:34,883] [train step83920] D loss: 0.32580 G loss: 2.27855 (0.039 sec/batch, 1661.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:35,274] [train step83930] D loss: 0.32590 G loss: 2.26395 (0.040 sec/batch, 1611.228 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:35,658] [train step83940] D loss: 0.32567 G loss: 2.31552 (0.039 sec/batch, 1661.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:36,055] [train step83950] D loss: 0.32572 G loss: 2.29594 (0.037 sec/batch, 1707.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:36,442] [train step83961] D loss: 0.32553 G loss: 2.32187 (0.038 sec/batch, 1699.410 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:36,833] [train step83970] D loss: 0.32567 G loss: 2.28107 (0.041 sec/batch, 1552.799 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:37,215] [train step83981] D loss: 0.32583 G loss: 2.29214 (0.036 sec/batch, 1802.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:37,598] [train step83990] D loss: 0.32570 G loss: 2.28480 (0.038 sec/batch, 1671.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:37,986] [train step84000] D loss: 0.32562 G loss: 2.28230 (0.039 sec/batch, 1639.691 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:03:37,986] Saved checkpoint at 84000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:38,602] [train step84011] D loss: 0.32588 G loss: 2.26789 (0.036 sec/batch, 1757.398 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:38,993] [train step84020] D loss: 0.32575 G loss: 2.30096 (0.033 sec/batch, 1928.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:39,374] [train step84030] D loss: 0.32606 G loss: 2.26390 (0.037 sec/batch, 1749.050 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:39,765] [train step84041] D loss: 0.32612 G loss: 2.35352 (0.038 sec/batch, 1685.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:40,156] [train step84051] D loss: 0.32599 G loss: 2.30816 (0.033 sec/batch, 1925.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:40,550] [train step84060] D loss: 0.32633 G loss: 2.40338 (0.037 sec/batch, 1744.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:40,939] [train step84070] D loss: 0.32583 G loss: 2.26861 (0.037 sec/batch, 1733.150 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:41,332] [train step84081] D loss: 0.32622 G loss: 2.38696 (0.036 sec/batch, 1756.857 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:41,722] [train step84090] D loss: 0.32603 G loss: 2.27285 (0.033 sec/batch, 1961.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:42,119] [train step84100] D loss: 0.32575 G loss: 2.30223 (0.039 sec/batch, 1631.826 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:42,510] [train step84110] D loss: 0.32579 G loss: 2.30235 (0.037 sec/batch, 1746.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:42,904] [train step84120] D loss: 0.32561 G loss: 2.29560 (0.041 sec/batch, 1554.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:43,306] [train step84130] D loss: 0.32570 G loss: 2.34729 (0.044 sec/batch, 1441.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:43,696] [train step84141] D loss: 0.32566 G loss: 2.31496 (0.035 sec/batch, 1829.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:44,087] [train step84150] D loss: 0.32582 G loss: 2.32087 (0.039 sec/batch, 1652.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:44,468] [train step84160] D loss: 0.32607 G loss: 2.25933 (0.039 sec/batch, 1635.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:44,844] [train step84170] D loss: 0.32580 G loss: 2.33854 (0.032 sec/batch, 1975.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:45,244] [train step84180] D loss: 0.32568 G loss: 2.29097 (0.043 sec/batch, 1486.337 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:45,634] [train step84191] D loss: 0.32592 G loss: 2.30640 (0.036 sec/batch, 1771.828 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:46,037] [train step84200] D loss: 0.32588 G loss: 2.34666 (0.040 sec/batch, 1613.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:46,426] [train step84210] D loss: 0.32587 G loss: 2.26850 (0.037 sec/batch, 1717.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:46,804] [train step84221] D loss: 0.32581 G loss: 2.34696 (0.039 sec/batch, 1660.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:47,220] [train step84231] D loss: 0.32588 G loss: 2.26242 (0.043 sec/batch, 1498.166 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:47,606] [train step84240] D loss: 0.32592 G loss: 2.27572 (0.035 sec/batch, 1815.176 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:47,991] [train step84251] D loss: 0.32603 G loss: 2.36734 (0.039 sec/batch, 1631.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:48,381] [train step84261] D loss: 0.32561 G loss: 2.30069 (0.034 sec/batch, 1891.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:48,764] [train step84270] D loss: 0.32573 G loss: 2.30481 (0.038 sec/batch, 1672.349 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:49,157] [train step84281] D loss: 0.32556 G loss: 2.32861 (0.038 sec/batch, 1674.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:49,531] [train step84291] D loss: 0.32578 G loss: 2.31791 (0.035 sec/batch, 1808.828 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:49,911] [train step84300] D loss: 0.32575 G loss: 2.33411 (0.034 sec/batch, 1891.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:50,315] [train step84311] D loss: 0.32557 G loss: 2.31940 (0.039 sec/batch, 1645.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:50,701] [train step84320] D loss: 0.32583 G loss: 2.29383 (0.033 sec/batch, 1915.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:51,092] [train step84330] D loss: 0.32580 G loss: 2.29171 (0.040 sec/batch, 1615.145 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:51,482] [train step84341] D loss: 0.32590 G loss: 2.36935 (0.043 sec/batch, 1491.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:51,863] [train step84350] D loss: 0.32584 G loss: 2.26985 (0.036 sec/batch, 1753.678 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:52,249] [train step84360] D loss: 0.32571 G loss: 2.33970 (0.038 sec/batch, 1702.784 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:52,620] [train step84371] D loss: 0.32552 G loss: 2.30948 (0.037 sec/batch, 1752.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:53,001] [train step84380] D loss: 0.32575 G loss: 2.32052 (0.035 sec/batch, 1812.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:53,386] [train step84390] D loss: 0.32568 G loss: 2.26062 (0.038 sec/batch, 1667.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:53,769] [train step84400] D loss: 0.32577 G loss: 2.32187 (0.040 sec/batch, 1600.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:54,163] [train step84410] D loss: 0.32574 G loss: 2.29417 (0.038 sec/batch, 1678.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:54,541] [train step84420] D loss: 0.32573 G loss: 2.34628 (0.033 sec/batch, 1937.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:54,923] [train step84430] D loss: 0.32572 G loss: 2.27970 (0.041 sec/batch, 1563.553 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:55,307] [train step84440] D loss: 0.32588 G loss: 2.35819 (0.039 sec/batch, 1638.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:55,682] [train step84450] D loss: 0.32567 G loss: 2.29694 (0.037 sec/batch, 1742.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:56,065] [train step84461] D loss: 0.32563 G loss: 2.32220 (0.042 sec/batch, 1537.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:56,458] [train step84471] D loss: 0.32567 G loss: 2.31588 (0.037 sec/batch, 1723.657 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:56,834] [train step84480] D loss: 0.32580 G loss: 2.32970 (0.036 sec/batch, 1765.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:57,206] [train step84491] D loss: 0.32559 G loss: 2.27843 (0.039 sec/batch, 1659.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:57,591] [train step84501] D loss: 0.32549 G loss: 2.31736 (0.037 sec/batch, 1744.481 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:57,970] [train step84510] D loss: 0.32574 G loss: 2.27593 (0.035 sec/batch, 1832.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:58,352] [train step84520] D loss: 0.32584 G loss: 2.35246 (0.039 sec/batch, 1629.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:58,736] [train step84531] D loss: 0.32567 G loss: 2.35170 (0.035 sec/batch, 1805.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:59,119] [train step84540] D loss: 0.32580 G loss: 2.34287 (0.037 sec/batch, 1712.857 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:59,508] [train step84551] D loss: 0.32570 G loss: 2.27016 (0.036 sec/batch, 1791.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:03:59,895] [train step84561] D loss: 0.32580 G loss: 2.35725 (0.034 sec/batch, 1908.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:00,280] [train step84570] D loss: 0.32571 G loss: 2.27293 (0.042 sec/batch, 1507.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:00,674] [train step84581] D loss: 0.32590 G loss: 2.36344 (0.040 sec/batch, 1586.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:01,056] [train step84591] D loss: 0.32561 G loss: 2.27977 (0.039 sec/batch, 1651.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:01,441] [train step84600] D loss: 0.32571 G loss: 2.31257 (0.041 sec/batch, 1562.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:01,835] [train step84610] D loss: 0.32569 G loss: 2.25835 (0.037 sec/batch, 1728.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:02,211] [train step84620] D loss: 0.32557 G loss: 2.31255 (0.033 sec/batch, 1922.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:02,598] [train step84630] D loss: 0.32569 G loss: 2.31986 (0.038 sec/batch, 1678.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:02,974] [train step84640] D loss: 0.32559 G loss: 2.33009 (0.030 sec/batch, 2145.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:03,360] [train step84651] D loss: 0.32571 G loss: 2.29851 (0.038 sec/batch, 1679.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:03,764] [train step84660] D loss: 0.32573 G loss: 2.32980 (0.045 sec/batch, 1417.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:04,159] [train step84670] D loss: 0.32559 G loss: 2.30058 (0.034 sec/batch, 1859.113 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:04,583] [train step84681] D loss: 0.32557 G loss: 2.31933 (0.037 sec/batch, 1732.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:04,974] [train step84690] D loss: 0.32580 G loss: 2.24781 (0.037 sec/batch, 1752.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:05,373] [train step84700] D loss: 0.32582 G loss: 2.23952 (0.033 sec/batch, 1951.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:05,787] [train step84710] D loss: 0.32704 G loss: 2.21460 (0.038 sec/batch, 1689.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:06,198] [train step84720] D loss: 0.32596 G loss: 2.33471 (0.042 sec/batch, 1517.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:06,594] [train step84731] D loss: 0.32630 G loss: 2.24226 (0.051 sec/batch, 1266.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:06,984] [train step84740] D loss: 0.32635 G loss: 2.39638 (0.034 sec/batch, 1868.703 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:07,389] [train step84750] D loss: 0.32592 G loss: 2.24390 (0.038 sec/batch, 1677.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:07,794] [train step84760] D loss: 0.32582 G loss: 2.35960 (0.038 sec/batch, 1673.006 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:08,190] [train step84770] D loss: 0.32582 G loss: 2.29879 (0.038 sec/batch, 1693.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:08,596] [train step84780] D loss: 0.32571 G loss: 2.30670 (0.034 sec/batch, 1863.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:09,004] [train step84790] D loss: 0.32553 G loss: 2.31262 (0.038 sec/batch, 1682.411 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:09,393] [train step84800] D loss: 0.32560 G loss: 2.28481 (0.040 sec/batch, 1606.225 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:09,789] [train step84810] D loss: 0.32568 G loss: 2.34475 (0.038 sec/batch, 1691.614 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:10,169] [train step84820] D loss: 0.32553 G loss: 2.30668 (0.040 sec/batch, 1583.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:10,566] [train step84830] D loss: 0.32558 G loss: 2.28681 (0.040 sec/batch, 1613.397 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:10,959] [train step84840] D loss: 0.32558 G loss: 2.30505 (0.040 sec/batch, 1584.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:11,351] [train step84851] D loss: 0.32576 G loss: 2.32967 (0.039 sec/batch, 1659.580 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:11,733] [train step84861] D loss: 0.32557 G loss: 2.27733 (0.045 sec/batch, 1434.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:12,122] [train step84870] D loss: 0.32583 G loss: 2.28395 (0.038 sec/batch, 1679.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:12,500] [train step84881] D loss: 0.32558 G loss: 2.32932 (0.038 sec/batch, 1672.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:12,898] [train step84890] D loss: 0.32564 G loss: 2.34362 (0.045 sec/batch, 1420.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:13,294] [train step84900] D loss: 0.32569 G loss: 2.31921 (0.037 sec/batch, 1727.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:13,672] [train step84911] D loss: 0.32574 G loss: 2.30487 (0.041 sec/batch, 1557.818 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:14,072] [train step84920] D loss: 0.32549 G loss: 2.28468 (0.042 sec/batch, 1521.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:14,455] [train step84930] D loss: 0.32551 G loss: 2.30030 (0.037 sec/batch, 1743.733 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:14,854] [train step84940] D loss: 0.32573 G loss: 2.25128 (0.043 sec/batch, 1494.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:15,251] [train step84950] D loss: 0.32563 G loss: 2.30525 (0.048 sec/batch, 1343.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:15,637] [train step84960] D loss: 0.32625 G loss: 2.22058 (0.038 sec/batch, 1684.301 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:16,031] [train step84970] D loss: 0.32612 G loss: 2.26759 (0.038 sec/batch, 1704.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:16,415] [train step84981] D loss: 0.32608 G loss: 2.34998 (0.034 sec/batch, 1857.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:16,808] [train step84990] D loss: 0.32660 G loss: 2.36383 (0.052 sec/batch, 1239.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:17,194] [train step85001] D loss: 0.32608 G loss: 2.34738 (0.037 sec/batch, 1726.728 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:04:17,195] Saved checkpoint at 85000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:17,804] [train step85010] D loss: 0.32672 G loss: 2.41386 (0.033 sec/batch, 1952.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:18,202] [train step85020] D loss: 0.32602 G loss: 2.30075 (0.041 sec/batch, 1573.248 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:18,593] [train step85030] D loss: 0.32574 G loss: 2.32913 (0.040 sec/batch, 1603.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:18,989] [train step85040] D loss: 0.32602 G loss: 2.22934 (0.047 sec/batch, 1354.257 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:19,371] [train step85050] D loss: 0.32597 G loss: 2.26145 (0.038 sec/batch, 1705.413 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:19,774] [train step85060] D loss: 0.32578 G loss: 2.29113 (0.040 sec/batch, 1582.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:20,169] [train step85071] D loss: 0.32602 G loss: 2.36311 (0.037 sec/batch, 1728.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:20,555] [train step85080] D loss: 0.32581 G loss: 2.28350 (0.036 sec/batch, 1794.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:20,939] [train step85090] D loss: 0.32597 G loss: 2.32371 (0.046 sec/batch, 1398.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:21,333] [train step85100] D loss: 0.32597 G loss: 2.30552 (0.037 sec/batch, 1724.587 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:21,710] [train step85110] D loss: 0.32661 G loss: 2.43153 (0.037 sec/batch, 1723.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:22,102] [train step85121] D loss: 0.32730 G loss: 2.21830 (0.037 sec/batch, 1731.216 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:22,480] [train step85130] D loss: 0.32936 G loss: 2.24631 (0.034 sec/batch, 1904.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:22,868] [train step85140] D loss: 0.32681 G loss: 2.33642 (0.038 sec/batch, 1689.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:23,249] [train step85151] D loss: 0.32654 G loss: 2.39172 (0.034 sec/batch, 1875.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:23,624] [train step85161] D loss: 0.32693 G loss: 2.18641 (0.035 sec/batch, 1812.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:24,002] [train step85170] D loss: 0.32727 G loss: 2.31501 (0.036 sec/batch, 1759.137 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:24,382] [train step85181] D loss: 0.36507 G loss: 3.30553 (0.036 sec/batch, 1791.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:24,751] [train step85190] D loss: 0.33717 G loss: 1.89955 (0.038 sec/batch, 1697.068 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:25,134] [train step85200] D loss: 0.32946 G loss: 2.56208 (0.035 sec/batch, 1846.250 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:25,519] [train step85211] D loss: 0.32943 G loss: 2.08189 (0.040 sec/batch, 1593.505 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:25,896] [train step85221] D loss: 0.32639 G loss: 2.41961 (0.035 sec/batch, 1835.280 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:26,305] [train step85230] D loss: 0.32590 G loss: 2.30895 (0.039 sec/batch, 1637.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:26,683] [train step85241] D loss: 0.32603 G loss: 2.26238 (0.038 sec/batch, 1667.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:27,064] [train step85250] D loss: 0.32596 G loss: 2.28286 (0.043 sec/batch, 1490.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:27,445] [train step85260] D loss: 0.32624 G loss: 2.35564 (0.033 sec/batch, 1919.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:27,827] [train step85271] D loss: 0.32672 G loss: 2.20725 (0.038 sec/batch, 1698.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:28,211] [train step85280] D loss: 0.32585 G loss: 2.32631 (0.041 sec/batch, 1562.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:28,589] [train step85290] D loss: 0.32597 G loss: 2.27423 (0.040 sec/batch, 1612.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:28,967] [train step85301] D loss: 0.32574 G loss: 2.32110 (0.037 sec/batch, 1747.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:29,352] [train step85311] D loss: 0.32600 G loss: 2.28476 (0.038 sec/batch, 1678.330 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:29,727] [train step85320] D loss: 0.32601 G loss: 2.24869 (0.039 sec/batch, 1645.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:30,110] [train step85331] D loss: 0.32562 G loss: 2.32999 (0.039 sec/batch, 1643.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:30,499] [train step85340] D loss: 0.32607 G loss: 2.26705 (0.038 sec/batch, 1686.142 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:30,880] [train step85350] D loss: 0.32603 G loss: 2.31796 (0.034 sec/batch, 1887.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:31,264] [train step85361] D loss: 0.32605 G loss: 2.31350 (0.043 sec/batch, 1503.849 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:31,648] [train step85370] D loss: 0.32601 G loss: 2.34936 (0.036 sec/batch, 1761.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:32,034] [train step85380] D loss: 0.32562 G loss: 2.31569 (0.037 sec/batch, 1742.748 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:32,427] [train step85391] D loss: 0.32629 G loss: 2.38919 (0.036 sec/batch, 1798.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:32,799] [train step85401] D loss: 0.32573 G loss: 2.33453 (0.037 sec/batch, 1749.483 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:33,178] [train step85410] D loss: 0.32594 G loss: 2.29841 (0.042 sec/batch, 1506.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:33,572] [train step85420] D loss: 0.32581 G loss: 2.35380 (0.038 sec/batch, 1681.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:33,949] [train step85431] D loss: 0.32569 G loss: 2.30335 (0.036 sec/batch, 1778.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:34,336] [train step85440] D loss: 0.32573 G loss: 2.29086 (0.039 sec/batch, 1652.419 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:34,733] [train step85450] D loss: 0.32605 G loss: 2.25294 (0.041 sec/batch, 1563.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:35,118] [train step85461] D loss: 0.32607 G loss: 2.24308 (0.039 sec/batch, 1650.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:35,513] [train step85470] D loss: 0.32609 G loss: 2.38043 (0.041 sec/batch, 1556.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:35,889] [train step85481] D loss: 0.32580 G loss: 2.29585 (0.036 sec/batch, 1783.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:36,269] [train step85490] D loss: 0.32647 G loss: 2.19823 (0.043 sec/batch, 1481.923 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:36,671] [train step85500] D loss: 0.32638 G loss: 2.22392 (0.037 sec/batch, 1732.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:37,056] [train step85511] D loss: 0.32583 G loss: 2.31396 (0.037 sec/batch, 1724.155 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:37,451] [train step85521] D loss: 0.32580 G loss: 2.29108 (0.048 sec/batch, 1329.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:37,834] [train step85530] D loss: 0.32594 G loss: 2.36571 (0.040 sec/batch, 1584.185 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:38,219] [train step85540] D loss: 0.32574 G loss: 2.29193 (0.039 sec/batch, 1650.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:38,626] [train step85551] D loss: 0.32562 G loss: 2.29459 (0.040 sec/batch, 1614.698 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:39,019] [train step85560] D loss: 0.32587 G loss: 2.28314 (0.041 sec/batch, 1568.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:39,403] [train step85570] D loss: 0.32671 G loss: 2.17165 (0.039 sec/batch, 1645.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:39,803] [train step85580] D loss: 0.32591 G loss: 2.28428 (0.035 sec/batch, 1831.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:40,187] [train step85590] D loss: 0.32609 G loss: 2.21202 (0.036 sec/batch, 1775.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:40,575] [train step85601] D loss: 0.32631 G loss: 2.36219 (0.037 sec/batch, 1729.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:40,971] [train step85611] D loss: 0.32612 G loss: 2.25286 (0.041 sec/batch, 1546.492 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:41,356] [train step85620] D loss: 0.32608 G loss: 2.27467 (0.035 sec/batch, 1805.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:41,763] [train step85631] D loss: 0.32572 G loss: 2.25937 (0.036 sec/batch, 1785.416 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:42,151] [train step85640] D loss: 0.32635 G loss: 2.40472 (0.038 sec/batch, 1687.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:42,536] [train step85650] D loss: 0.32590 G loss: 2.28556 (0.033 sec/batch, 1913.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:42,926] [train step85660] D loss: 0.32577 G loss: 2.28581 (0.027 sec/batch, 2361.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:43,327] [train step85670] D loss: 0.32576 G loss: 2.25877 (0.040 sec/batch, 1617.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:43,723] [train step85680] D loss: 0.32587 G loss: 2.36343 (0.039 sec/batch, 1621.398 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:44,111] [train step85691] D loss: 0.32564 G loss: 2.27350 (0.038 sec/batch, 1701.155 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:44,493] [train step85701] D loss: 0.32565 G loss: 2.30421 (0.045 sec/batch, 1429.034 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:44,891] [train step85710] D loss: 0.32574 G loss: 2.31148 (0.042 sec/batch, 1534.522 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:45,280] [train step85720] D loss: 0.32637 G loss: 2.20327 (0.041 sec/batch, 1572.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:45,683] [train step85731] D loss: 0.32595 G loss: 2.31213 (0.048 sec/batch, 1338.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:46,073] [train step85740] D loss: 0.32572 G loss: 2.32227 (0.040 sec/batch, 1609.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:46,463] [train step85751] D loss: 0.32568 G loss: 2.32202 (0.041 sec/batch, 1555.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:46,862] [train step85760] D loss: 0.32573 G loss: 2.24449 (0.045 sec/batch, 1421.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:47,261] [train step85770] D loss: 0.32557 G loss: 2.31550 (0.050 sec/batch, 1274.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:47,668] [train step85781] D loss: 0.32561 G loss: 2.31141 (0.037 sec/batch, 1750.065 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:48,068] [train step85790] D loss: 0.32614 G loss: 2.40212 (0.049 sec/batch, 1308.484 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:48,450] [train step85800] D loss: 0.32567 G loss: 2.28940 (0.035 sec/batch, 1813.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:48,847] [train step85811] D loss: 0.32607 G loss: 2.21357 (0.037 sec/batch, 1721.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:49,236] [train step85821] D loss: 0.32587 G loss: 2.32608 (0.037 sec/batch, 1747.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:49,616] [train step85830] D loss: 0.32565 G loss: 2.27213 (0.036 sec/batch, 1796.600 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:50,009] [train step85841] D loss: 0.32572 G loss: 2.34080 (0.040 sec/batch, 1602.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:50,405] [train step85851] D loss: 0.32558 G loss: 2.27102 (0.037 sec/batch, 1736.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:50,799] [train step85860] D loss: 0.32585 G loss: 2.36132 (0.042 sec/batch, 1532.866 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:51,186] [train step85871] D loss: 0.32570 G loss: 2.29242 (0.036 sec/batch, 1801.133 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:51,567] [train step85881] D loss: 0.32555 G loss: 2.30565 (0.036 sec/batch, 1754.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:51,961] [train step85890] D loss: 0.32560 G loss: 2.29747 (0.042 sec/batch, 1530.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:52,345] [train step85900] D loss: 0.32583 G loss: 2.24018 (0.038 sec/batch, 1684.703 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:52,726] [train step85911] D loss: 0.32571 G loss: 2.33549 (0.040 sec/batch, 1615.582 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:53,112] [train step85920] D loss: 0.32563 G loss: 2.31196 (0.036 sec/batch, 1777.848 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:53,479] [train step85931] D loss: 0.32554 G loss: 2.31963 (0.034 sec/batch, 1869.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:53,859] [train step85940] D loss: 0.32556 G loss: 2.28496 (0.040 sec/batch, 1580.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:54,234] [train step85950] D loss: 0.32563 G loss: 2.32233 (0.035 sec/batch, 1848.716 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:54,619] [train step85961] D loss: 0.32558 G loss: 2.33492 (0.037 sec/batch, 1711.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:54,997] [train step85971] D loss: 0.32603 G loss: 2.33100 (0.035 sec/batch, 1809.254 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:55,377] [train step85980] D loss: 0.32581 G loss: 2.25509 (0.037 sec/batch, 1722.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:55,759] [train step85991] D loss: 0.32551 G loss: 2.28992 (0.037 sec/batch, 1709.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:56,143] [train step86001] D loss: 0.32571 G loss: 2.33455 (0.033 sec/batch, 1921.582 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:04:56,143] Saved checkpoint at 86000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:56,740] [train step86010] D loss: 0.32562 G loss: 2.31723 (0.042 sec/batch, 1515.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:57,117] [train step86021] D loss: 0.32575 G loss: 2.37432 (0.035 sec/batch, 1836.034 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:57,505] [train step86030] D loss: 0.32546 G loss: 2.30817 (0.041 sec/batch, 1567.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:57,897] [train step86040] D loss: 0.32557 G loss: 2.32907 (0.036 sec/batch, 1753.724 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:58,282] [train step86051] D loss: 0.32561 G loss: 2.26984 (0.034 sec/batch, 1876.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:58,662] [train step86060] D loss: 0.32562 G loss: 2.31559 (0.037 sec/batch, 1711.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:59,030] [train step86070] D loss: 0.32562 G loss: 2.32472 (0.035 sec/batch, 1848.653 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:59,422] [train step86081] D loss: 0.32557 G loss: 2.32961 (0.035 sec/batch, 1854.745 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:04:59,807] [train step86091] D loss: 0.32580 G loss: 2.26763 (0.042 sec/batch, 1535.373 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:00,200] [train step86100] D loss: 0.32538 G loss: 2.29720 (0.035 sec/batch, 1833.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:00,580] [train step86111] D loss: 0.32552 G loss: 2.32414 (0.045 sec/batch, 1417.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:00,951] [train step86120] D loss: 0.32558 G loss: 2.33940 (0.035 sec/batch, 1839.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:01,337] [train step86130] D loss: 0.32571 G loss: 2.26839 (0.040 sec/batch, 1589.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:01,720] [train step86141] D loss: 0.32546 G loss: 2.28501 (0.039 sec/batch, 1646.056 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:02,101] [train step86150] D loss: 0.32556 G loss: 2.28993 (0.039 sec/batch, 1626.941 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:02,482] [train step86160] D loss: 0.32553 G loss: 2.28264 (0.036 sec/batch, 1756.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:02,865] [train step86171] D loss: 0.32566 G loss: 2.24789 (0.041 sec/batch, 1545.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:03,250] [train step86181] D loss: 0.32566 G loss: 2.35210 (0.034 sec/batch, 1900.751 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:03,632] [train step86190] D loss: 0.32555 G loss: 2.29762 (0.041 sec/batch, 1543.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:04,006] [train step86200] D loss: 0.32567 G loss: 2.33826 (0.032 sec/batch, 1975.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:04,406] [train step86210] D loss: 0.32564 G loss: 2.26199 (0.040 sec/batch, 1586.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:04,783] [train step86220] D loss: 0.32558 G loss: 2.33211 (0.039 sec/batch, 1636.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:05,157] [train step86230] D loss: 0.32544 G loss: 2.30795 (0.028 sec/batch, 2324.499 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:05,558] [train step86241] D loss: 0.32558 G loss: 2.30449 (0.038 sec/batch, 1678.550 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:05,933] [train step86250] D loss: 0.32570 G loss: 2.25613 (0.036 sec/batch, 1777.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:06,323] [train step86261] D loss: 0.32548 G loss: 2.30334 (0.039 sec/batch, 1630.994 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:06,702] [train step86270] D loss: 0.32551 G loss: 2.30680 (0.039 sec/batch, 1628.787 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:07,085] [train step86280] D loss: 0.32569 G loss: 2.36065 (0.038 sec/batch, 1679.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:07,480] [train step86290] D loss: 0.32608 G loss: 2.39466 (0.039 sec/batch, 1627.513 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:07,874] [train step86300] D loss: 0.32577 G loss: 2.27687 (0.040 sec/batch, 1600.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:08,252] [train step86310] D loss: 0.32628 G loss: 2.20963 (0.037 sec/batch, 1728.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:08,670] [train step86320] D loss: 0.32607 G loss: 2.39142 (0.033 sec/batch, 1928.693 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:09,062] [train step86330] D loss: 0.32569 G loss: 2.24758 (0.040 sec/batch, 1610.832 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:09,463] [train step86340] D loss: 0.32569 G loss: 2.34365 (0.039 sec/batch, 1643.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:09,852] [train step86350] D loss: 0.32569 G loss: 2.30537 (0.042 sec/batch, 1513.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:10,228] [train step86360] D loss: 0.32578 G loss: 2.24376 (0.036 sec/batch, 1776.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:10,624] [train step86370] D loss: 0.32599 G loss: 2.39060 (0.035 sec/batch, 1852.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:11,018] [train step86380] D loss: 0.32549 G loss: 2.28915 (0.038 sec/batch, 1663.633 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:11,400] [train step86391] D loss: 0.32562 G loss: 2.33972 (0.047 sec/batch, 1365.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:11,779] [train step86400] D loss: 0.32551 G loss: 2.32699 (0.040 sec/batch, 1605.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:12,157] [train step86411] D loss: 0.32558 G loss: 2.27337 (0.038 sec/batch, 1694.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:12,547] [train step86421] D loss: 0.32566 G loss: 2.30285 (0.040 sec/batch, 1611.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:12,920] [train step86430] D loss: 0.32600 G loss: 2.21958 (0.034 sec/batch, 1875.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:13,317] [train step86440] D loss: 0.32564 G loss: 2.34357 (0.041 sec/batch, 1570.680 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:13,716] [train step86450] D loss: 0.32549 G loss: 2.31088 (0.044 sec/batch, 1457.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:14,100] [train step86460] D loss: 0.32570 G loss: 2.36363 (0.040 sec/batch, 1588.780 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:14,491] [train step86470] D loss: 0.32559 G loss: 2.29024 (0.047 sec/batch, 1355.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:14,874] [train step86481] D loss: 0.32563 G loss: 2.34168 (0.037 sec/batch, 1740.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:15,258] [train step86490] D loss: 0.32564 G loss: 2.30253 (0.036 sec/batch, 1771.512 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:15,645] [train step86500] D loss: 0.32557 G loss: 2.30143 (0.040 sec/batch, 1602.389 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:16,033] [train step86510] D loss: 0.32603 G loss: 2.21653 (0.039 sec/batch, 1653.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:16,409] [train step86520] D loss: 0.32636 G loss: 2.41779 (0.039 sec/batch, 1651.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:16,804] [train step86531] D loss: 0.32573 G loss: 2.34414 (0.039 sec/batch, 1627.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:17,191] [train step86540] D loss: 0.32557 G loss: 2.33724 (0.042 sec/batch, 1524.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:17,578] [train step86550] D loss: 0.32563 G loss: 2.31859 (0.042 sec/batch, 1533.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:17,956] [train step86560] D loss: 0.32592 G loss: 2.21800 (0.038 sec/batch, 1693.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:18,335] [train step86570] D loss: 0.32591 G loss: 2.36518 (0.034 sec/batch, 1877.132 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:18,732] [train step86580] D loss: 0.32719 G loss: 2.13442 (0.037 sec/batch, 1708.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:19,128] [train step86590] D loss: 0.32592 G loss: 2.36756 (0.050 sec/batch, 1272.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:19,515] [train step86600] D loss: 0.32593 G loss: 2.36691 (0.037 sec/batch, 1707.409 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:19,902] [train step86610] D loss: 0.32674 G loss: 2.44661 (0.036 sec/batch, 1764.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:20,287] [train step86621] D loss: 0.32624 G loss: 2.41716 (0.039 sec/batch, 1650.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:20,684] [train step86630] D loss: 0.32616 G loss: 2.20917 (0.035 sec/batch, 1846.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:21,078] [train step86640] D loss: 0.32567 G loss: 2.34016 (0.037 sec/batch, 1742.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:21,460] [train step86651] D loss: 0.32579 G loss: 2.26825 (0.041 sec/batch, 1578.419 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:21,855] [train step86660] D loss: 0.32560 G loss: 2.34490 (0.035 sec/batch, 1813.949 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:22,246] [train step86670] D loss: 0.32573 G loss: 2.24604 (0.036 sec/batch, 1778.154 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:22,629] [train step86680] D loss: 0.32557 G loss: 2.30128 (0.035 sec/batch, 1843.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:23,023] [train step86690] D loss: 0.32590 G loss: 2.26120 (0.037 sec/batch, 1716.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:23,393] [train step86700] D loss: 0.32554 G loss: 2.31803 (0.040 sec/batch, 1618.553 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:23,777] [train step86711] D loss: 0.32573 G loss: 2.34367 (0.035 sec/batch, 1815.176 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:24,150] [train step86721] D loss: 0.32577 G loss: 2.26744 (0.039 sec/batch, 1645.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:24,529] [train step86730] D loss: 0.32565 G loss: 2.34064 (0.036 sec/batch, 1766.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:24,920] [train step86740] D loss: 0.32602 G loss: 2.22455 (0.037 sec/batch, 1742.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:25,294] [train step86750] D loss: 0.32568 G loss: 2.36311 (0.034 sec/batch, 1905.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:25,668] [train step86760] D loss: 0.32571 G loss: 2.25325 (0.036 sec/batch, 1774.991 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:26,055] [train step86770] D loss: 0.32723 G loss: 2.47450 (0.037 sec/batch, 1748.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:26,429] [train step86781] D loss: 0.32721 G loss: 2.46417 (0.037 sec/batch, 1747.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:26,817] [train step86790] D loss: 0.32591 G loss: 2.23139 (0.049 sec/batch, 1319.242 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:27,191] [train step86801] D loss: 0.32586 G loss: 2.33687 (0.035 sec/batch, 1843.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:27,583] [train step86811] D loss: 0.32592 G loss: 2.33450 (0.039 sec/batch, 1633.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:27,963] [train step86820] D loss: 0.32587 G loss: 2.28966 (0.036 sec/batch, 1780.820 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:28,339] [train step86831] D loss: 0.32582 G loss: 2.29875 (0.038 sec/batch, 1682.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:28,714] [train step86841] D loss: 0.32579 G loss: 2.36955 (0.038 sec/batch, 1673.392 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:29,100] [train step86850] D loss: 0.32584 G loss: 2.34388 (0.039 sec/batch, 1640.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:29,471] [train step86861] D loss: 0.32739 G loss: 2.45880 (0.037 sec/batch, 1717.097 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:29,866] [train step86871] D loss: 0.32616 G loss: 2.21200 (0.033 sec/batch, 1960.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:30,256] [train step86880] D loss: 0.32568 G loss: 2.29189 (0.037 sec/batch, 1738.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:30,650] [train step86890] D loss: 0.32567 G loss: 2.26591 (0.036 sec/batch, 1784.454 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:31,045] [train step86901] D loss: 0.32605 G loss: 2.37308 (0.040 sec/batch, 1592.210 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:31,419] [train step86910] D loss: 0.32562 G loss: 2.30446 (0.037 sec/batch, 1713.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:31,805] [train step86920] D loss: 0.32554 G loss: 2.30826 (0.040 sec/batch, 1603.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:32,193] [train step86930] D loss: 0.32559 G loss: 2.29752 (0.043 sec/batch, 1489.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:32,577] [train step86940] D loss: 0.32590 G loss: 2.25480 (0.039 sec/batch, 1645.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:32,953] [train step86951] D loss: 0.32549 G loss: 2.32773 (0.040 sec/batch, 1590.766 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:33,344] [train step86960] D loss: 0.32554 G loss: 2.33736 (0.037 sec/batch, 1723.657 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:33,719] [train step86970] D loss: 0.32552 G loss: 2.31128 (0.036 sec/batch, 1767.558 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:34,111] [train step86980] D loss: 0.32570 G loss: 2.36277 (0.042 sec/batch, 1538.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:34,503] [train step86990] D loss: 0.32603 G loss: 2.22245 (0.038 sec/batch, 1684.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:34,884] [train step87000] D loss: 0.32553 G loss: 2.32474 (0.037 sec/batch, 1714.279 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:05:34,884] Saved checkpoint at 87000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:35,481] [train step87010] D loss: 0.32562 G loss: 2.25480 (0.035 sec/batch, 1810.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:35,865] [train step87020] D loss: 0.32555 G loss: 2.32286 (0.036 sec/batch, 1757.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:36,262] [train step87030] D loss: 0.32561 G loss: 2.33044 (0.045 sec/batch, 1426.407 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:36,641] [train step87041] D loss: 0.32556 G loss: 2.25966 (0.039 sec/batch, 1622.153 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:37,026] [train step87051] D loss: 0.32589 G loss: 2.21519 (0.037 sec/batch, 1740.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:37,417] [train step87060] D loss: 0.32548 G loss: 2.31437 (0.038 sec/batch, 1689.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:37,798] [train step87071] D loss: 0.32557 G loss: 2.27232 (0.035 sec/batch, 1854.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:38,198] [train step87080] D loss: 0.32561 G loss: 2.31977 (0.039 sec/batch, 1620.703 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:38,585] [train step87090] D loss: 0.32558 G loss: 2.29533 (0.040 sec/batch, 1601.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:38,982] [train step87101] D loss: 0.32546 G loss: 2.28850 (0.040 sec/batch, 1604.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:39,374] [train step87111] D loss: 0.32554 G loss: 2.32447 (0.036 sec/batch, 1773.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:39,761] [train step87120] D loss: 0.32552 G loss: 2.32195 (0.043 sec/batch, 1488.662 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:40,158] [train step87130] D loss: 0.32549 G loss: 2.30248 (0.045 sec/batch, 1437.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:40,560] [train step87140] D loss: 0.32556 G loss: 2.31839 (0.036 sec/batch, 1765.674 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:40,949] [train step87150] D loss: 0.32560 G loss: 2.24738 (0.036 sec/batch, 1784.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:41,339] [train step87161] D loss: 0.32548 G loss: 2.33066 (0.034 sec/batch, 1884.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:41,735] [train step87171] D loss: 0.32543 G loss: 2.31732 (0.039 sec/batch, 1644.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:42,111] [train step87180] D loss: 0.32556 G loss: 2.31896 (0.033 sec/batch, 1946.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:42,512] [train step87191] D loss: 0.32556 G loss: 2.34396 (0.041 sec/batch, 1571.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:42,890] [train step87201] D loss: 0.32549 G loss: 2.33138 (0.038 sec/batch, 1692.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:43,276] [train step87210] D loss: 0.32544 G loss: 2.29066 (0.040 sec/batch, 1586.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:43,666] [train step87221] D loss: 0.32544 G loss: 2.27779 (0.040 sec/batch, 1613.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:44,049] [train step87230] D loss: 0.32558 G loss: 2.35771 (0.035 sec/batch, 1813.055 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:44,451] [train step87240] D loss: 0.32572 G loss: 2.27296 (0.033 sec/batch, 1911.253 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:44,838] [train step87250] D loss: 0.32568 G loss: 2.36230 (0.037 sec/batch, 1707.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:45,225] [train step87261] D loss: 0.32552 G loss: 2.33708 (0.037 sec/batch, 1748.298 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:45,612] [train step87270] D loss: 0.32557 G loss: 2.27604 (0.039 sec/batch, 1629.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:46,003] [train step87280] D loss: 0.32560 G loss: 2.34968 (0.037 sec/batch, 1728.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:46,399] [train step87290] D loss: 0.32546 G loss: 2.30384 (0.042 sec/batch, 1531.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:46,784] [train step87300] D loss: 0.32549 G loss: 2.34878 (0.032 sec/batch, 1974.472 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:47,167] [train step87310] D loss: 0.32561 G loss: 2.33449 (0.033 sec/batch, 1924.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:47,571] [train step87320] D loss: 0.32549 G loss: 2.29498 (0.040 sec/batch, 1602.715 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:47,972] [train step87330] D loss: 0.32560 G loss: 2.31321 (0.038 sec/batch, 1695.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:48,356] [train step87341] D loss: 0.32555 G loss: 2.35165 (0.042 sec/batch, 1517.219 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:48,754] [train step87350] D loss: 0.32547 G loss: 2.29445 (0.041 sec/batch, 1560.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:49,141] [train step87360] D loss: 0.32553 G loss: 2.35505 (0.039 sec/batch, 1630.419 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:49,529] [train step87371] D loss: 0.32556 G loss: 2.30773 (0.036 sec/batch, 1791.277 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:49,919] [train step87381] D loss: 0.32556 G loss: 2.33796 (0.039 sec/batch, 1649.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:50,301] [train step87390] D loss: 0.32530 G loss: 2.29643 (0.036 sec/batch, 1782.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:50,698] [train step87400] D loss: 0.32549 G loss: 2.32069 (0.036 sec/batch, 1791.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:51,092] [train step87411] D loss: 0.32553 G loss: 2.28744 (0.040 sec/batch, 1594.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:51,491] [train step87420] D loss: 0.32544 G loss: 2.29417 (0.048 sec/batch, 1335.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:51,903] [train step87430] D loss: 0.32544 G loss: 2.28924 (0.036 sec/batch, 1782.689 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:52,299] [train step87441] D loss: 0.32544 G loss: 2.28149 (0.036 sec/batch, 1788.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:52,700] [train step87450] D loss: 0.32546 G loss: 2.31096 (0.041 sec/batch, 1542.192 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:53,080] [train step87461] D loss: 0.32543 G loss: 2.30429 (0.035 sec/batch, 1842.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:53,468] [train step87470] D loss: 0.32548 G loss: 2.34289 (0.038 sec/batch, 1682.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:53,853] [train step87480] D loss: 0.32541 G loss: 2.32631 (0.034 sec/batch, 1907.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:54,235] [train step87491] D loss: 0.32553 G loss: 2.29552 (0.035 sec/batch, 1842.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:54,620] [train step87501] D loss: 0.32544 G loss: 2.32319 (0.041 sec/batch, 1542.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:54,999] [train step87510] D loss: 0.32542 G loss: 2.33164 (0.035 sec/batch, 1813.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:55,378] [train step87521] D loss: 0.32539 G loss: 2.28901 (0.037 sec/batch, 1753.357 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:55,763] [train step87530] D loss: 0.32548 G loss: 2.30346 (0.037 sec/batch, 1752.304 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:56,143] [train step87540] D loss: 0.32545 G loss: 2.31365 (0.032 sec/batch, 1970.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:56,525] [train step87550] D loss: 0.32557 G loss: 2.24102 (0.035 sec/batch, 1843.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:56,908] [train step87560] D loss: 0.32565 G loss: 2.31971 (0.037 sec/batch, 1736.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:57,290] [train step87570] D loss: 0.32547 G loss: 2.28843 (0.036 sec/batch, 1791.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:57,664] [train step87580] D loss: 0.32545 G loss: 2.28815 (0.038 sec/batch, 1688.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:58,044] [train step87591] D loss: 0.32543 G loss: 2.28335 (0.033 sec/batch, 1913.855 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:58,433] [train step87600] D loss: 0.32556 G loss: 2.26878 (0.037 sec/batch, 1738.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:58,823] [train step87610] D loss: 0.32561 G loss: 2.37160 (0.035 sec/batch, 1837.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:59,199] [train step87620] D loss: 0.32545 G loss: 2.27922 (0.042 sec/batch, 1513.762 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:59,586] [train step87630] D loss: 0.32547 G loss: 2.33521 (0.037 sec/batch, 1735.357 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:05:59,973] [train step87640] D loss: 0.32556 G loss: 2.25559 (0.037 sec/batch, 1726.528 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:00,361] [train step87650] D loss: 0.32548 G loss: 2.33992 (0.039 sec/batch, 1650.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:00,741] [train step87660] D loss: 0.32561 G loss: 2.35521 (0.036 sec/batch, 1760.775 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:01,120] [train step87671] D loss: 0.32561 G loss: 2.27080 (0.035 sec/batch, 1810.279 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:01,510] [train step87680] D loss: 0.32538 G loss: 2.29915 (0.035 sec/batch, 1808.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:01,909] [train step87690] D loss: 0.32555 G loss: 2.25667 (0.034 sec/batch, 1885.822 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:02,291] [train step87701] D loss: 0.32541 G loss: 2.28243 (0.032 sec/batch, 1978.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:02,675] [train step87710] D loss: 0.32573 G loss: 2.36706 (0.045 sec/batch, 1429.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:03,065] [train step87720] D loss: 0.32549 G loss: 2.30134 (0.033 sec/batch, 1921.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:03,461] [train step87730] D loss: 0.32553 G loss: 2.34797 (0.038 sec/batch, 1670.736 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:03,857] [train step87740] D loss: 0.32550 G loss: 2.28655 (0.045 sec/batch, 1406.607 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:04,236] [train step87750] D loss: 0.32550 G loss: 2.32515 (0.037 sec/batch, 1716.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:04,624] [train step87761] D loss: 0.32529 G loss: 2.30108 (0.040 sec/batch, 1603.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:05,012] [train step87771] D loss: 0.32557 G loss: 2.25641 (0.036 sec/batch, 1801.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:05,406] [train step87780] D loss: 0.32544 G loss: 2.31122 (0.043 sec/batch, 1493.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:05,789] [train step87791] D loss: 0.32543 G loss: 2.31062 (0.040 sec/batch, 1583.513 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:06,189] [train step87801] D loss: 0.32552 G loss: 2.26582 (0.037 sec/batch, 1741.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:06,572] [train step87810] D loss: 0.32542 G loss: 2.32412 (0.040 sec/batch, 1580.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:06,971] [train step87821] D loss: 0.32549 G loss: 2.27074 (0.041 sec/batch, 1571.783 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:07,359] [train step87830] D loss: 0.32539 G loss: 2.31582 (0.038 sec/batch, 1685.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:07,754] [train step87840] D loss: 0.32547 G loss: 2.29764 (0.038 sec/batch, 1706.530 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:08,150] [train step87851] D loss: 0.32547 G loss: 2.29617 (0.037 sec/batch, 1721.645 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:08,543] [train step87861] D loss: 0.32543 G loss: 2.32011 (0.037 sec/batch, 1733.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:08,938] [train step87870] D loss: 0.32575 G loss: 2.23340 (0.031 sec/batch, 2057.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:09,335] [train step87880] D loss: 0.32546 G loss: 2.33408 (0.039 sec/batch, 1636.273 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:09,717] [train step87890] D loss: 0.32541 G loss: 2.27756 (0.028 sec/batch, 2249.183 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:10,122] [train step87900] D loss: 0.32550 G loss: 2.26997 (0.039 sec/batch, 1653.732 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:10,515] [train step87911] D loss: 0.32537 G loss: 2.29867 (0.037 sec/batch, 1731.898 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:10,901] [train step87920] D loss: 0.32550 G loss: 2.30746 (0.038 sec/batch, 1689.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:11,301] [train step87930] D loss: 0.32566 G loss: 2.25408 (0.042 sec/batch, 1511.163 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:11,686] [train step87941] D loss: 0.32583 G loss: 2.23565 (0.037 sec/batch, 1733.822 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:12,095] [train step87950] D loss: 0.32616 G loss: 2.39393 (0.050 sec/batch, 1270.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:12,487] [train step87960] D loss: 0.32602 G loss: 2.24528 (0.041 sec/batch, 1574.014 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:12,870] [train step87970] D loss: 0.33201 G loss: 2.66069 (0.032 sec/batch, 1997.778 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:13,267] [train step87981] D loss: 0.32690 G loss: 2.14949 (0.044 sec/batch, 1452.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:13,657] [train step87990] D loss: 0.32686 G loss: 2.45751 (0.041 sec/batch, 1549.438 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:14,041] [train step88000] D loss: 0.32618 G loss: 2.19047 (0.039 sec/batch, 1647.136 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:06:14,041] Saved checkpoint at 88000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:14,658] [train step88011] D loss: 0.32558 G loss: 2.34889 (0.042 sec/batch, 1527.093 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:15,040] [train step88020] D loss: 0.32562 G loss: 2.27732 (0.039 sec/batch, 1644.543 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:15,433] [train step88030] D loss: 0.32551 G loss: 2.30513 (0.043 sec/batch, 1473.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:15,816] [train step88040] D loss: 0.32538 G loss: 2.28655 (0.038 sec/batch, 1685.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:16,203] [train step88050] D loss: 0.32586 G loss: 2.38440 (0.047 sec/batch, 1368.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:16,593] [train step88060] D loss: 0.32546 G loss: 2.30585 (0.037 sec/batch, 1723.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:16,974] [train step88070] D loss: 0.32580 G loss: 2.21791 (0.042 sec/batch, 1512.679 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:17,365] [train step88080] D loss: 0.32548 G loss: 2.31369 (0.037 sec/batch, 1731.708 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:17,750] [train step88091] D loss: 0.32546 G loss: 2.29261 (0.038 sec/batch, 1701.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:18,128] [train step88101] D loss: 0.32548 G loss: 2.30919 (0.036 sec/batch, 1769.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:18,525] [train step88110] D loss: 0.32567 G loss: 2.31569 (0.038 sec/batch, 1686.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:18,910] [train step88120] D loss: 0.32573 G loss: 2.36206 (0.037 sec/batch, 1721.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:19,297] [train step88131] D loss: 0.32554 G loss: 2.27180 (0.045 sec/batch, 1429.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:19,694] [train step88140] D loss: 0.32544 G loss: 2.30638 (0.041 sec/batch, 1562.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:20,073] [train step88151] D loss: 0.32567 G loss: 2.35693 (0.038 sec/batch, 1697.916 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:20,461] [train step88160] D loss: 0.32605 G loss: 2.20549 (0.034 sec/batch, 1910.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:20,845] [train step88170] D loss: 0.32576 G loss: 2.36456 (0.037 sec/batch, 1713.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:21,221] [train step88181] D loss: 0.32541 G loss: 2.33469 (0.035 sec/batch, 1849.978 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:21,623] [train step88191] D loss: 0.32588 G loss: 2.22980 (0.037 sec/batch, 1720.090 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:22,006] [train step88200] D loss: 0.32586 G loss: 2.38830 (0.038 sec/batch, 1662.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:22,411] [train step88211] D loss: 0.32567 G loss: 2.25035 (0.056 sec/batch, 1138.765 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:22,812] [train step88220] D loss: 0.32541 G loss: 2.30595 (0.039 sec/batch, 1641.325 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:23,195] [train step88230] D loss: 0.32613 G loss: 2.19806 (0.039 sec/batch, 1641.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:23,583] [train step88240] D loss: 0.32557 G loss: 2.31534 (0.037 sec/batch, 1713.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:23,966] [train step88250] D loss: 0.32552 G loss: 2.31601 (0.036 sec/batch, 1761.492 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:24,345] [train step88260] D loss: 0.32553 G loss: 2.33112 (0.041 sec/batch, 1559.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:24,724] [train step88271] D loss: 0.32546 G loss: 2.31994 (0.035 sec/batch, 1844.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:25,101] [train step88280] D loss: 0.32554 G loss: 2.26618 (0.033 sec/batch, 1917.697 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:25,496] [train step88290] D loss: 0.32575 G loss: 2.37934 (0.036 sec/batch, 1772.401 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:25,871] [train step88300] D loss: 0.32596 G loss: 2.22909 (0.032 sec/batch, 2006.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:26,262] [train step88311] D loss: 0.32577 G loss: 2.36036 (0.036 sec/batch, 1790.357 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:26,645] [train step88320] D loss: 0.32579 G loss: 2.22888 (0.034 sec/batch, 1883.863 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:27,030] [train step88331] D loss: 0.32562 G loss: 2.37268 (0.048 sec/batch, 1334.802 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:27,400] [train step88340] D loss: 0.32547 G loss: 2.30576 (0.034 sec/batch, 1872.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:27,784] [train step88350] D loss: 0.32567 G loss: 2.36314 (0.035 sec/batch, 1839.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:28,168] [train step88361] D loss: 0.32540 G loss: 2.31666 (0.038 sec/batch, 1689.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:28,559] [train step88370] D loss: 0.32585 G loss: 2.21888 (0.048 sec/batch, 1337.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:28,947] [train step88380] D loss: 0.32561 G loss: 2.36195 (0.037 sec/batch, 1730.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:29,319] [train step88391] D loss: 0.32556 G loss: 2.29643 (0.034 sec/batch, 1907.003 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:29,708] [train step88400] D loss: 0.32544 G loss: 2.31480 (0.038 sec/batch, 1664.313 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:30,089] [train step88410] D loss: 0.32560 G loss: 2.25343 (0.039 sec/batch, 1626.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:30,461] [train step88420] D loss: 0.32544 G loss: 2.29804 (0.034 sec/batch, 1874.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:30,845] [train step88431] D loss: 0.32544 G loss: 2.32369 (0.035 sec/batch, 1821.593 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:31,225] [train step88440] D loss: 0.32590 G loss: 2.26424 (0.040 sec/batch, 1610.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:31,614] [train step88451] D loss: 0.32575 G loss: 2.26148 (0.044 sec/batch, 1448.747 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:31,997] [train step88460] D loss: 0.32555 G loss: 2.33932 (0.036 sec/batch, 1769.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:32,389] [train step88470] D loss: 0.32563 G loss: 2.27553 (0.037 sec/batch, 1732.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:32,779] [train step88480] D loss: 0.32542 G loss: 2.30131 (0.040 sec/batch, 1606.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:33,164] [train step88491] D loss: 0.32568 G loss: 2.26354 (0.048 sec/batch, 1338.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:33,549] [train step88500] D loss: 0.32549 G loss: 2.32690 (0.037 sec/batch, 1741.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:33,937] [train step88511] D loss: 0.32564 G loss: 2.25327 (0.039 sec/batch, 1631.033 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:34,322] [train step88520] D loss: 0.32548 G loss: 2.28758 (0.040 sec/batch, 1590.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:34,710] [train step88530] D loss: 0.32545 G loss: 2.32905 (0.051 sec/batch, 1262.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:35,095] [train step88541] D loss: 0.32541 G loss: 2.30162 (0.036 sec/batch, 1785.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:35,487] [train step88551] D loss: 0.32548 G loss: 2.31451 (0.040 sec/batch, 1597.935 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:35,879] [train step88560] D loss: 0.32562 G loss: 2.27823 (0.035 sec/batch, 1831.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:36,271] [train step88570] D loss: 0.32563 G loss: 2.35647 (0.039 sec/batch, 1645.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:36,647] [train step88580] D loss: 0.32548 G loss: 2.33440 (0.032 sec/batch, 2004.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:37,054] [train step88590] D loss: 0.32566 G loss: 2.34905 (0.033 sec/batch, 1944.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:37,437] [train step88601] D loss: 0.32558 G loss: 2.32314 (0.039 sec/batch, 1651.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:37,838] [train step88610] D loss: 0.32557 G loss: 2.32725 (0.038 sec/batch, 1690.378 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:38,221] [train step88620] D loss: 0.32542 G loss: 2.30675 (0.035 sec/batch, 1845.539 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:38,605] [train step88630] D loss: 0.32545 G loss: 2.29625 (0.041 sec/batch, 1573.340 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:39,014] [train step88641] D loss: 0.32555 G loss: 2.34580 (0.041 sec/batch, 1567.021 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:39,410] [train step88650] D loss: 0.32548 G loss: 2.32841 (0.036 sec/batch, 1771.395 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:39,794] [train step88661] D loss: 0.32547 G loss: 2.33092 (0.033 sec/batch, 1915.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:40,184] [train step88671] D loss: 0.32545 G loss: 2.29805 (0.044 sec/batch, 1463.733 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:40,588] [train step88680] D loss: 0.32554 G loss: 2.33106 (0.040 sec/batch, 1614.552 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:40,971] [train step88690] D loss: 0.32552 G loss: 2.34125 (0.030 sec/batch, 2139.064 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:41,367] [train step88700] D loss: 0.32545 G loss: 2.28380 (0.038 sec/batch, 1694.112 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:41,750] [train step88710] D loss: 0.32550 G loss: 2.31368 (0.039 sec/batch, 1638.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:42,143] [train step88721] D loss: 0.32558 G loss: 2.35808 (0.036 sec/batch, 1780.631 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:42,531] [train step88730] D loss: 0.32569 G loss: 2.23295 (0.038 sec/batch, 1683.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:42,913] [train step88740] D loss: 0.32544 G loss: 2.31809 (0.042 sec/batch, 1513.199 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:43,307] [train step88750] D loss: 0.32553 G loss: 2.27367 (0.038 sec/batch, 1679.149 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:43,705] [train step88761] D loss: 0.32544 G loss: 2.27272 (0.044 sec/batch, 1466.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:44,113] [train step88770] D loss: 0.32554 G loss: 2.27842 (0.035 sec/batch, 1827.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:44,509] [train step88780] D loss: 0.32538 G loss: 2.32709 (0.041 sec/batch, 1575.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:44,882] [train step88790] D loss: 0.32558 G loss: 2.36145 (0.037 sec/batch, 1748.036 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:45,277] [train step88800] D loss: 0.32569 G loss: 2.23591 (0.035 sec/batch, 1809.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:45,667] [train step88811] D loss: 0.32538 G loss: 2.30953 (0.037 sec/batch, 1720.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:46,060] [train step88820] D loss: 0.32545 G loss: 2.33594 (0.039 sec/batch, 1643.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:46,444] [train step88830] D loss: 0.32547 G loss: 2.32129 (0.044 sec/batch, 1445.237 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:46,828] [train step88840] D loss: 0.32575 G loss: 2.38490 (0.036 sec/batch, 1755.892 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:47,224] [train step88850] D loss: 0.32545 G loss: 2.28894 (0.034 sec/batch, 1857.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:47,616] [train step88860] D loss: 0.32548 G loss: 2.27197 (0.042 sec/batch, 1538.391 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:47,996] [train step88871] D loss: 0.32539 G loss: 2.30483 (0.039 sec/batch, 1649.738 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:48,390] [train step88880] D loss: 0.32552 G loss: 2.27405 (0.036 sec/batch, 1795.699 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:48,788] [train step88890] D loss: 0.32548 G loss: 2.32224 (0.041 sec/batch, 1564.437 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:49,179] [train step88901] D loss: 0.32548 G loss: 2.27990 (0.039 sec/batch, 1637.161 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:49,562] [train step88911] D loss: 0.32554 G loss: 2.26985 (0.037 sec/batch, 1734.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:49,942] [train step88920] D loss: 0.32564 G loss: 2.25700 (0.040 sec/batch, 1588.188 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:50,331] [train step88930] D loss: 0.32555 G loss: 2.26420 (0.036 sec/batch, 1791.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:50,714] [train step88941] D loss: 0.32552 G loss: 2.32779 (0.038 sec/batch, 1667.332 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:51,105] [train step88950] D loss: 0.32550 G loss: 2.30068 (0.028 sec/batch, 2323.091 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:51,507] [train step88961] D loss: 0.32550 G loss: 2.28939 (0.039 sec/batch, 1625.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:51,891] [train step88971] D loss: 0.32562 G loss: 2.32429 (0.037 sec/batch, 1749.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:52,293] [train step88980] D loss: 0.32553 G loss: 2.25774 (0.040 sec/batch, 1615.670 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:52,674] [train step88990] D loss: 0.32557 G loss: 2.31665 (0.036 sec/batch, 1793.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:53,055] [train step89001] D loss: 0.32547 G loss: 2.30916 (0.036 sec/batch, 1782.700 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:06:53,055] Saved checkpoint at 89000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:53,660] [train step89010] D loss: 0.32559 G loss: 2.26640 (0.027 sec/batch, 2329.684 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:54,065] [train step89020] D loss: 0.32555 G loss: 2.32544 (0.039 sec/batch, 1655.568 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:54,471] [train step89030] D loss: 0.32559 G loss: 2.31190 (0.038 sec/batch, 1699.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:54,845] [train step89040] D loss: 0.32558 G loss: 2.32839 (0.036 sec/batch, 1800.143 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:55,223] [train step89051] D loss: 0.32576 G loss: 2.37881 (0.045 sec/batch, 1422.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:55,601] [train step89061] D loss: 0.32551 G loss: 2.33359 (0.035 sec/batch, 1852.263 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:55,973] [train step89070] D loss: 0.32596 G loss: 2.40301 (0.040 sec/batch, 1612.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:56,354] [train step89080] D loss: 0.32558 G loss: 2.36592 (0.037 sec/batch, 1738.819 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:56,730] [train step89091] D loss: 0.32548 G loss: 2.28181 (0.039 sec/batch, 1661.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:57,105] [train step89100] D loss: 0.32551 G loss: 2.31418 (0.040 sec/batch, 1590.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:57,478] [train step89110] D loss: 0.32548 G loss: 2.33399 (0.030 sec/batch, 2147.965 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:57,861] [train step89120] D loss: 0.32573 G loss: 2.24694 (0.036 sec/batch, 1776.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:58,240] [train step89130] D loss: 0.32542 G loss: 2.32393 (0.037 sec/batch, 1729.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:58,631] [train step89141] D loss: 0.32547 G loss: 2.30948 (0.038 sec/batch, 1702.612 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:59,004] [train step89150] D loss: 0.32552 G loss: 2.28230 (0.032 sec/batch, 1986.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:59,388] [train step89160] D loss: 0.32544 G loss: 2.32351 (0.048 sec/batch, 1331.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:06:59,755] [train step89170] D loss: 0.32551 G loss: 2.28233 (0.032 sec/batch, 1973.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:00,136] [train step89180] D loss: 0.32570 G loss: 2.24454 (0.039 sec/batch, 1628.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:00,525] [train step89190] D loss: 0.32556 G loss: 2.35737 (0.037 sec/batch, 1752.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:00,898] [train step89201] D loss: 0.32583 G loss: 2.38491 (0.034 sec/batch, 1873.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:01,272] [train step89211] D loss: 0.32567 G loss: 2.28971 (0.031 sec/batch, 2054.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:01,671] [train step89220] D loss: 0.32558 G loss: 2.29445 (0.038 sec/batch, 1684.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:02,051] [train step89230] D loss: 0.32568 G loss: 2.24362 (0.041 sec/batch, 1565.523 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:02,433] [train step89240] D loss: 0.32557 G loss: 2.29614 (0.040 sec/batch, 1592.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:02,826] [train step89250] D loss: 0.32548 G loss: 2.27155 (0.042 sec/batch, 1517.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:03,201] [train step89261] D loss: 0.32576 G loss: 2.38284 (0.040 sec/batch, 1586.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:03,588] [train step89271] D loss: 0.32578 G loss: 2.24621 (0.036 sec/batch, 1770.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:03,964] [train step89280] D loss: 0.32680 G loss: 2.45519 (0.036 sec/batch, 1767.418 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:04,347] [train step89291] D loss: 0.32609 G loss: 2.40946 (0.038 sec/batch, 1687.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:04,751] [train step89301] D loss: 0.32618 G loss: 2.22790 (0.038 sec/batch, 1680.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:05,145] [train step89310] D loss: 0.32654 G loss: 2.44936 (0.036 sec/batch, 1770.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:05,527] [train step89320] D loss: 0.32585 G loss: 2.37026 (0.038 sec/batch, 1674.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:05,927] [train step89330] D loss: 0.32557 G loss: 2.32162 (0.037 sec/batch, 1740.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:06,309] [train step89340] D loss: 0.32579 G loss: 2.28203 (0.034 sec/batch, 1892.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:06,701] [train step89351] D loss: 0.32553 G loss: 2.28673 (0.042 sec/batch, 1513.779 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:07,084] [train step89360] D loss: 0.32552 G loss: 2.30269 (0.042 sec/batch, 1523.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:07,474] [train step89370] D loss: 0.32552 G loss: 2.28460 (0.038 sec/batch, 1664.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:07,876] [train step89380] D loss: 0.32540 G loss: 2.29508 (0.039 sec/batch, 1628.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:08,257] [train step89391] D loss: 0.32555 G loss: 2.35261 (0.035 sec/batch, 1821.878 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:08,667] [train step89400] D loss: 0.32552 G loss: 2.27876 (0.054 sec/batch, 1185.742 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:09,054] [train step89411] D loss: 0.32563 G loss: 2.35756 (0.037 sec/batch, 1719.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:09,447] [train step89420] D loss: 0.32550 G loss: 2.34473 (0.039 sec/batch, 1624.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:09,833] [train step89430] D loss: 0.32557 G loss: 2.31725 (0.040 sec/batch, 1596.813 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:10,218] [train step89441] D loss: 0.32567 G loss: 2.36167 (0.041 sec/batch, 1579.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:10,619] [train step89450] D loss: 0.32548 G loss: 2.26220 (0.052 sec/batch, 1235.680 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:11,015] [train step89460] D loss: 0.32545 G loss: 2.31749 (0.037 sec/batch, 1709.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:11,397] [train step89471] D loss: 0.32569 G loss: 2.23992 (0.035 sec/batch, 1829.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:11,782] [train step89480] D loss: 0.32536 G loss: 2.28132 (0.036 sec/batch, 1777.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:12,177] [train step89490] D loss: 0.32551 G loss: 2.25243 (0.041 sec/batch, 1543.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:12,558] [train step89501] D loss: 0.32592 G loss: 2.21624 (0.037 sec/batch, 1723.447 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:12,954] [train step89511] D loss: 0.32544 G loss: 2.32701 (0.036 sec/batch, 1779.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:13,338] [train step89520] D loss: 0.32552 G loss: 2.26669 (0.050 sec/batch, 1282.729 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:13,741] [train step89531] D loss: 0.32542 G loss: 2.28051 (0.037 sec/batch, 1737.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:14,144] [train step89540] D loss: 0.32547 G loss: 2.33646 (0.038 sec/batch, 1690.293 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:14,531] [train step89550] D loss: 0.32547 G loss: 2.31375 (0.035 sec/batch, 1838.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:14,926] [train step89561] D loss: 0.32549 G loss: 2.34835 (0.041 sec/batch, 1550.413 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:15,300] [train step89571] D loss: 0.32543 G loss: 2.30488 (0.037 sec/batch, 1729.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:15,701] [train step89580] D loss: 0.32554 G loss: 2.33815 (0.035 sec/batch, 1853.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:16,108] [train step89591] D loss: 0.32550 G loss: 2.33734 (0.040 sec/batch, 1596.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:16,497] [train step89600] D loss: 0.32570 G loss: 2.24518 (0.037 sec/batch, 1748.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:16,888] [train step89610] D loss: 0.32536 G loss: 2.32301 (0.044 sec/batch, 1456.735 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:17,271] [train step89620] D loss: 0.32549 G loss: 2.24826 (0.035 sec/batch, 1813.704 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:17,652] [train step89630] D loss: 0.32559 G loss: 2.35086 (0.036 sec/batch, 1802.572 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:18,055] [train step89640] D loss: 0.32553 G loss: 2.24739 (0.040 sec/batch, 1600.364 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:18,434] [train step89651] D loss: 0.32569 G loss: 2.37775 (0.036 sec/batch, 1760.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:18,816] [train step89660] D loss: 0.32573 G loss: 2.38152 (0.037 sec/batch, 1728.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:19,210] [train step89670] D loss: 0.32543 G loss: 2.32570 (0.038 sec/batch, 1676.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:19,587] [train step89680] D loss: 0.32548 G loss: 2.35661 (0.045 sec/batch, 1423.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:19,967] [train step89690] D loss: 0.32543 G loss: 2.27563 (0.036 sec/batch, 1801.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:20,364] [train step89700] D loss: 0.32541 G loss: 2.32747 (0.035 sec/batch, 1818.311 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:20,747] [train step89711] D loss: 0.32549 G loss: 2.25754 (0.040 sec/batch, 1617.705 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:21,139] [train step89721] D loss: 0.32537 G loss: 2.28745 (0.038 sec/batch, 1692.798 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:21,524] [train step89730] D loss: 0.32539 G loss: 2.29453 (0.040 sec/batch, 1619.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:21,903] [train step89741] D loss: 0.32569 G loss: 2.22585 (0.036 sec/batch, 1779.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:22,302] [train step89750] D loss: 0.32545 G loss: 2.31302 (0.041 sec/batch, 1571.839 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:22,691] [train step89760] D loss: 0.32565 G loss: 2.23992 (0.036 sec/batch, 1777.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:23,093] [train step89770] D loss: 0.32537 G loss: 2.27909 (0.048 sec/batch, 1343.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:23,483] [train step89780] D loss: 0.32547 G loss: 2.29121 (0.034 sec/batch, 1855.130 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:23,871] [train step89790] D loss: 0.32552 G loss: 2.35097 (0.035 sec/batch, 1807.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:24,265] [train step89801] D loss: 0.32541 G loss: 2.33200 (0.037 sec/batch, 1740.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:24,658] [train step89811] D loss: 0.32536 G loss: 2.30655 (0.038 sec/batch, 1692.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:25,037] [train step89820] D loss: 0.32550 G loss: 2.25838 (0.037 sec/batch, 1749.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:25,431] [train step89830] D loss: 0.32530 G loss: 2.29346 (0.037 sec/batch, 1733.206 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:25,800] [train step89841] D loss: 0.32540 G loss: 2.33577 (0.030 sec/batch, 2102.095 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:26,190] [train step89850] D loss: 0.32567 G loss: 2.23918 (0.046 sec/batch, 1405.083 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:26,585] [train step89861] D loss: 0.32554 G loss: 2.32905 (0.036 sec/batch, 1755.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:26,967] [train step89870] D loss: 0.32537 G loss: 2.30823 (0.040 sec/batch, 1619.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:27,359] [train step89880] D loss: 0.32537 G loss: 2.32539 (0.038 sec/batch, 1671.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:27,733] [train step89891] D loss: 0.32554 G loss: 2.35486 (0.039 sec/batch, 1630.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:28,115] [train step89900] D loss: 0.32541 G loss: 2.29129 (0.044 sec/batch, 1466.011 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:28,515] [train step89910] D loss: 0.32554 G loss: 2.35905 (0.041 sec/batch, 1547.642 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:28,895] [train step89920] D loss: 0.32537 G loss: 2.33696 (0.037 sec/batch, 1715.539 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:29,283] [train step89931] D loss: 0.32597 G loss: 2.20908 (0.043 sec/batch, 1502.880 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:29,657] [train step89940] D loss: 0.32591 G loss: 2.39990 (0.039 sec/batch, 1620.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:30,035] [train step89950] D loss: 0.32535 G loss: 2.30518 (0.034 sec/batch, 1856.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:30,424] [train step89961] D loss: 0.32541 G loss: 2.27359 (0.035 sec/batch, 1824.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:30,812] [train step89970] D loss: 0.32532 G loss: 2.29834 (0.041 sec/batch, 1573.885 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:31,201] [train step89980] D loss: 0.32565 G loss: 2.24132 (0.043 sec/batch, 1479.000 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:31,588] [train step89991] D loss: 0.32531 G loss: 2.28518 (0.034 sec/batch, 1883.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:31,974] [train step90000] D loss: 0.32538 G loss: 2.30728 (0.039 sec/batch, 1660.946 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:07:31,974] Saved checkpoint at 90000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:32,585] [train step90010] D loss: 0.32531 G loss: 2.29818 (0.040 sec/batch, 1613.669 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:32,962] [train step90021] D loss: 0.32545 G loss: 2.35111 (0.037 sec/batch, 1746.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:33,339] [train step90030] D loss: 0.32554 G loss: 2.23715 (0.033 sec/batch, 1953.764 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:33,734] [train step90040] D loss: 0.32545 G loss: 2.27029 (0.043 sec/batch, 1500.645 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:34,109] [train step90051] D loss: 0.32549 G loss: 2.36591 (0.039 sec/batch, 1626.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:34,493] [train step90060] D loss: 0.32538 G loss: 2.28128 (0.033 sec/batch, 1947.584 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:34,881] [train step90070] D loss: 0.32552 G loss: 2.34758 (0.033 sec/batch, 1943.565 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:35,264] [train step90080] D loss: 0.32535 G loss: 2.30510 (0.038 sec/batch, 1695.471 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:35,652] [train step90090] D loss: 0.32530 G loss: 2.27888 (0.040 sec/batch, 1601.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:36,038] [train step90101] D loss: 0.32540 G loss: 2.28280 (0.039 sec/batch, 1623.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:36,418] [train step90111] D loss: 0.32533 G loss: 2.31525 (0.039 sec/batch, 1650.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:36,808] [train step90120] D loss: 0.32539 G loss: 2.33677 (0.038 sec/batch, 1705.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:37,214] [train step90131] D loss: 0.32536 G loss: 2.32877 (0.039 sec/batch, 1656.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:37,611] [train step90141] D loss: 0.32541 G loss: 2.32129 (0.035 sec/batch, 1820.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:37,997] [train step90150] D loss: 0.32542 G loss: 2.35061 (0.048 sec/batch, 1343.070 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:38,385] [train step90160] D loss: 0.32603 G loss: 2.41543 (0.038 sec/batch, 1674.718 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:38,782] [train step90170] D loss: 0.32541 G loss: 2.27333 (0.037 sec/batch, 1714.388 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:39,167] [train step90180] D loss: 0.32533 G loss: 2.32101 (0.040 sec/batch, 1620.048 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:39,575] [train step90191] D loss: 0.32530 G loss: 2.31517 (0.046 sec/batch, 1394.028 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:39,959] [train step90201] D loss: 0.32537 G loss: 2.30109 (0.035 sec/batch, 1824.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:40,346] [train step90210] D loss: 0.32545 G loss: 2.32578 (0.039 sec/batch, 1650.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:40,747] [train step90220] D loss: 0.32553 G loss: 2.24253 (0.038 sec/batch, 1666.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:41,135] [train step90230] D loss: 0.32534 G loss: 2.29338 (0.039 sec/batch, 1648.664 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:41,514] [train step90240] D loss: 0.32536 G loss: 2.27666 (0.034 sec/batch, 1910.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:41,910] [train step90251] D loss: 0.32533 G loss: 2.31806 (0.033 sec/batch, 1918.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:42,305] [train step90260] D loss: 0.32534 G loss: 2.29140 (0.040 sec/batch, 1604.190 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:42,696] [train step90270] D loss: 0.32546 G loss: 2.27419 (0.042 sec/batch, 1533.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:43,082] [train step90281] D loss: 0.32545 G loss: 2.25478 (0.037 sec/batch, 1743.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:43,467] [train step90290] D loss: 0.32542 G loss: 2.26330 (0.040 sec/batch, 1589.448 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:43,863] [train step90300] D loss: 0.32537 G loss: 2.33081 (0.039 sec/batch, 1662.046 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:44,253] [train step90310] D loss: 0.32537 G loss: 2.29179 (0.040 sec/batch, 1618.553 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:44,666] [train step90321] D loss: 0.32539 G loss: 2.26768 (0.038 sec/batch, 1701.964 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:45,065] [train step90330] D loss: 0.32544 G loss: 2.33090 (0.036 sec/batch, 1762.984 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:45,455] [train step90341] D loss: 0.32541 G loss: 2.27437 (0.038 sec/batch, 1693.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:45,849] [train step90350] D loss: 0.32542 G loss: 2.26151 (0.040 sec/batch, 1609.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:46,234] [train step90360] D loss: 0.32548 G loss: 2.34397 (0.041 sec/batch, 1548.749 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:46,615] [train step90371] D loss: 0.32540 G loss: 2.26562 (0.037 sec/batch, 1718.977 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:47,011] [train step90380] D loss: 0.32544 G loss: 2.33469 (0.041 sec/batch, 1572.972 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:47,395] [train step90390] D loss: 0.32549 G loss: 2.26274 (0.037 sec/batch, 1751.447 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:47,810] [train step90401] D loss: 0.32549 G loss: 2.34959 (0.039 sec/batch, 1655.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:48,194] [train step90410] D loss: 0.32541 G loss: 2.34887 (0.036 sec/batch, 1780.513 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:48,583] [train step90420] D loss: 0.32542 G loss: 2.26688 (0.040 sec/batch, 1601.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:48,978] [train step90430] D loss: 0.32531 G loss: 2.29434 (0.035 sec/batch, 1835.030 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:49,376] [train step90440] D loss: 0.32560 G loss: 2.36801 (0.039 sec/batch, 1632.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:49,761] [train step90450] D loss: 0.32541 G loss: 2.27810 (0.035 sec/batch, 1805.057 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:50,158] [train step90460] D loss: 0.32542 G loss: 2.29902 (0.047 sec/batch, 1365.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:50,549] [train step90470] D loss: 0.32557 G loss: 2.36616 (0.039 sec/batch, 1662.139 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:50,937] [train step90480] D loss: 0.32541 G loss: 2.25990 (0.036 sec/batch, 1782.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:51,321] [train step90491] D loss: 0.32539 G loss: 2.32107 (0.034 sec/batch, 1891.589 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:51,704] [train step90501] D loss: 0.32555 G loss: 2.37069 (0.038 sec/batch, 1664.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:52,098] [train step90510] D loss: 0.32545 G loss: 2.28087 (0.036 sec/batch, 1773.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:52,494] [train step90520] D loss: 0.32535 G loss: 2.31980 (0.037 sec/batch, 1721.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:52,880] [train step90530] D loss: 0.32538 G loss: 2.28021 (0.039 sec/batch, 1628.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:53,272] [train step90540] D loss: 0.32540 G loss: 2.33211 (0.039 sec/batch, 1650.204 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:53,659] [train step90551] D loss: 0.32586 G loss: 2.39914 (0.034 sec/batch, 1861.433 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:54,059] [train step90561] D loss: 0.32532 G loss: 2.29593 (0.048 sec/batch, 1346.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:54,445] [train step90570] D loss: 0.32585 G loss: 2.40204 (0.038 sec/batch, 1687.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:54,827] [train step90581] D loss: 0.32599 G loss: 2.41037 (0.039 sec/batch, 1624.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:55,224] [train step90591] D loss: 0.32541 G loss: 2.34411 (0.036 sec/batch, 1790.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:55,598] [train step90600] D loss: 0.32549 G loss: 2.28365 (0.033 sec/batch, 1938.134 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:55,983] [train step90610] D loss: 0.32535 G loss: 2.31828 (0.038 sec/batch, 1696.714 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:56,376] [train step90621] D loss: 0.32533 G loss: 2.28030 (0.038 sec/batch, 1683.561 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:56,754] [train step90630] D loss: 0.32548 G loss: 2.33682 (0.040 sec/batch, 1602.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:57,134] [train step90641] D loss: 0.32532 G loss: 2.32963 (0.040 sec/batch, 1582.019 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:57,519] [train step90651] D loss: 0.32538 G loss: 2.30204 (0.042 sec/batch, 1508.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:57,892] [train step90660] D loss: 0.32538 G loss: 2.31987 (0.036 sec/batch, 1760.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:58,289] [train step90671] D loss: 0.32539 G loss: 2.29638 (0.035 sec/batch, 1806.722 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:58,661] [train step90680] D loss: 0.32538 G loss: 2.30016 (0.034 sec/batch, 1898.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:59,039] [train step90690] D loss: 0.32537 G loss: 2.27894 (0.036 sec/batch, 1799.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:59,425] [train step90701] D loss: 0.32561 G loss: 2.23671 (0.041 sec/batch, 1563.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:07:59,809] [train step90711] D loss: 0.32539 G loss: 2.34364 (0.035 sec/batch, 1802.948 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:00,191] [train step90720] D loss: 0.32592 G loss: 2.20352 (0.046 sec/batch, 1396.102 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:00,565] [train step90731] D loss: 0.32559 G loss: 2.23433 (0.039 sec/batch, 1631.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:00,940] [train step90741] D loss: 0.32536 G loss: 2.32823 (0.038 sec/batch, 1687.997 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:01,354] [train step90750] D loss: 0.32543 G loss: 2.27273 (0.045 sec/batch, 1407.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:01,764] [train step90761] D loss: 0.32534 G loss: 2.29353 (0.046 sec/batch, 1376.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:02,160] [train step90771] D loss: 0.32530 G loss: 2.29870 (0.041 sec/batch, 1544.890 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:02,547] [train step90780] D loss: 0.32538 G loss: 2.33760 (0.028 sec/batch, 2269.165 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:02,942] [train step90790] D loss: 0.32541 G loss: 2.35333 (0.036 sec/batch, 1769.282 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:03,332] [train step90801] D loss: 0.32533 G loss: 2.28174 (0.036 sec/batch, 1780.241 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:03,711] [train step90810] D loss: 0.32533 G loss: 2.32526 (0.034 sec/batch, 1869.302 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:04,101] [train step90820] D loss: 0.32536 G loss: 2.34016 (0.036 sec/batch, 1756.409 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:04,497] [train step90831] D loss: 0.32532 G loss: 2.30949 (0.038 sec/batch, 1706.378 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:04,886] [train step90840] D loss: 0.32537 G loss: 2.34817 (0.038 sec/batch, 1706.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:05,274] [train step90851] D loss: 0.32534 G loss: 2.28464 (0.040 sec/batch, 1602.954 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:05,670] [train step90861] D loss: 0.32537 G loss: 2.29853 (0.039 sec/batch, 1650.194 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:06,056] [train step90870] D loss: 0.32539 G loss: 2.29364 (0.038 sec/batch, 1666.007 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:06,456] [train step90880] D loss: 0.32540 G loss: 2.34596 (0.040 sec/batch, 1588.695 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:06,856] [train step90890] D loss: 0.32529 G loss: 2.28329 (0.043 sec/batch, 1476.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:07,245] [train step90900] D loss: 0.32536 G loss: 2.28183 (0.041 sec/batch, 1547.482 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:07,643] [train step90910] D loss: 0.32579 G loss: 2.21410 (0.040 sec/batch, 1600.603 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:08,040] [train step90921] D loss: 0.32579 G loss: 2.21427 (0.036 sec/batch, 1798.322 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:08,440] [train step90930] D loss: 0.32573 G loss: 2.39363 (0.038 sec/batch, 1669.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:08,841] [train step90941] D loss: 0.32535 G loss: 2.30307 (0.040 sec/batch, 1585.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:09,229] [train step90950] D loss: 0.32539 G loss: 2.25444 (0.039 sec/batch, 1622.692 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:09,637] [train step90960] D loss: 0.32542 G loss: 2.34449 (0.046 sec/batch, 1388.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:10,031] [train step90971] D loss: 0.32537 G loss: 2.28459 (0.042 sec/batch, 1525.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:10,434] [train step90980] D loss: 0.32530 G loss: 2.28167 (0.043 sec/batch, 1485.621 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:10,822] [train step90990] D loss: 0.32530 G loss: 2.31629 (0.036 sec/batch, 1779.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:11,221] [train step91001] D loss: 0.32530 G loss: 2.30202 (0.039 sec/batch, 1644.321 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:08:11,221] Saved checkpoint at 91000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:11,859] [train step91011] D loss: 0.32540 G loss: 2.34106 (0.040 sec/batch, 1588.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:12,245] [train step91020] D loss: 0.32538 G loss: 2.32127 (0.040 sec/batch, 1612.302 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:12,640] [train step91030] D loss: 0.32557 G loss: 2.36972 (0.039 sec/batch, 1662.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:13,025] [train step91040] D loss: 0.32545 G loss: 2.35598 (0.041 sec/batch, 1555.255 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:13,408] [train step91050] D loss: 0.32532 G loss: 2.30499 (0.035 sec/batch, 1810.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:13,802] [train step91060] D loss: 0.32540 G loss: 2.34110 (0.039 sec/batch, 1643.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:14,185] [train step91070] D loss: 0.32536 G loss: 2.29466 (0.042 sec/batch, 1530.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:14,574] [train step91080] D loss: 0.32539 G loss: 2.33216 (0.037 sec/batch, 1715.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:14,964] [train step91090] D loss: 0.32534 G loss: 2.27921 (0.039 sec/batch, 1649.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:15,360] [train step91100] D loss: 0.32533 G loss: 2.32528 (0.038 sec/batch, 1686.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:15,755] [train step91110] D loss: 0.32532 G loss: 2.31858 (0.037 sec/batch, 1710.074 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:16,133] [train step91120] D loss: 0.32542 G loss: 2.35109 (0.040 sec/batch, 1619.784 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:16,534] [train step91131] D loss: 0.32528 G loss: 2.29317 (0.042 sec/batch, 1506.372 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:16,937] [train step91140] D loss: 0.32529 G loss: 2.31254 (0.036 sec/batch, 1782.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:17,319] [train step91151] D loss: 0.32532 G loss: 2.27853 (0.039 sec/batch, 1660.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:17,717] [train step91161] D loss: 0.32576 G loss: 2.21254 (0.038 sec/batch, 1693.813 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:18,105] [train step91170] D loss: 0.32554 G loss: 2.36561 (0.038 sec/batch, 1706.519 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:18,492] [train step91180] D loss: 0.32526 G loss: 2.29837 (0.034 sec/batch, 1894.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:18,899] [train step91191] D loss: 0.32547 G loss: 2.35637 (0.039 sec/batch, 1645.985 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:19,285] [train step91200] D loss: 0.32563 G loss: 2.22712 (0.033 sec/batch, 1913.460 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:19,877] [train step91211] D loss: 0.32548 G loss: 2.26225 (0.131 sec/batch, 488.946 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:20,559] [train step91221] D loss: 0.32539 G loss: 2.26540 (0.054 sec/batch, 1179.034 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:21,377] [train step91230] D loss: 0.32531 G loss: 2.31290 (0.064 sec/batch, 999.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:21,858] [train step91241] D loss: 0.32532 G loss: 2.29662 (0.043 sec/batch, 1495.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:22,245] [train step91250] D loss: 0.32524 G loss: 2.32060 (0.040 sec/batch, 1609.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:22,636] [train step91260] D loss: 0.32536 G loss: 2.32531 (0.043 sec/batch, 1502.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:23,031] [train step91271] D loss: 0.32541 G loss: 2.34977 (0.043 sec/batch, 1504.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:23,434] [train step91281] D loss: 0.32527 G loss: 2.28827 (0.048 sec/batch, 1331.618 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:23,848] [train step91290] D loss: 0.32528 G loss: 2.31527 (0.042 sec/batch, 1511.699 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:24,237] [train step91301] D loss: 0.32530 G loss: 2.27833 (0.034 sec/batch, 1866.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:24,624] [train step91311] D loss: 0.32527 G loss: 2.29999 (0.036 sec/batch, 1783.565 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:25,029] [train step91320] D loss: 0.32545 G loss: 2.25321 (0.047 sec/batch, 1359.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:25,407] [train step91330] D loss: 0.32531 G loss: 2.28821 (0.038 sec/batch, 1668.534 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:25,784] [train step91341] D loss: 0.32533 G loss: 2.30647 (0.044 sec/batch, 1469.390 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:26,183] [train step91350] D loss: 0.32543 G loss: 2.30665 (0.040 sec/batch, 1595.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:26,567] [train step91361] D loss: 0.32531 G loss: 2.32169 (0.036 sec/batch, 1756.570 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:26,967] [train step91371] D loss: 0.32526 G loss: 2.29627 (0.037 sec/batch, 1724.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:27,336] [train step91380] D loss: 0.32533 G loss: 2.30617 (0.026 sec/batch, 2500.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:27,719] [train step91391] D loss: 0.32533 G loss: 2.32676 (0.034 sec/batch, 1870.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:28,108] [train step91400] D loss: 0.32534 G loss: 2.29879 (0.038 sec/batch, 1686.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:28,481] [train step91410] D loss: 0.32528 G loss: 2.28937 (0.036 sec/batch, 1773.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:28,857] [train step91421] D loss: 0.32531 G loss: 2.30461 (0.038 sec/batch, 1680.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:29,242] [train step91430] D loss: 0.32531 G loss: 2.30700 (0.031 sec/batch, 2049.298 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:29,614] [train step91440] D loss: 0.32534 G loss: 2.28311 (0.036 sec/batch, 1779.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:30,000] [train step91451] D loss: 0.32535 G loss: 2.29559 (0.047 sec/batch, 1352.061 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:30,378] [train step91461] D loss: 0.32541 G loss: 2.29611 (0.037 sec/batch, 1744.651 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:30,745] [train step91470] D loss: 0.32537 G loss: 2.28074 (0.032 sec/batch, 1991.597 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:31,132] [train step91480] D loss: 0.32541 G loss: 2.25594 (0.039 sec/batch, 1625.955 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:31,502] [train step91490] D loss: 0.32539 G loss: 2.28038 (0.036 sec/batch, 1775.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:31,876] [train step91500] D loss: 0.32533 G loss: 2.28806 (0.036 sec/batch, 1755.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:32,254] [train step91511] D loss: 0.32551 G loss: 2.24936 (0.035 sec/batch, 1808.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:32,623] [train step91520] D loss: 0.32564 G loss: 2.23401 (0.032 sec/batch, 2028.040 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:33,001] [train step91530] D loss: 0.32593 G loss: 2.40619 (0.038 sec/batch, 1688.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:33,387] [train step91540] D loss: 0.32581 G loss: 2.38875 (0.039 sec/batch, 1661.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:33,769] [train step91551] D loss: 0.32605 G loss: 2.41434 (0.037 sec/batch, 1722.827 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:34,153] [train step91560] D loss: 0.32543 G loss: 2.25735 (0.039 sec/batch, 1641.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:34,533] [train step91570] D loss: 0.32562 G loss: 2.36711 (0.039 sec/batch, 1648.148 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:34,911] [train step91581] D loss: 0.32559 G loss: 2.36637 (0.037 sec/batch, 1715.648 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:35,304] [train step91590] D loss: 0.32528 G loss: 2.29149 (0.042 sec/batch, 1533.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:35,693] [train step91601] D loss: 0.32537 G loss: 2.31150 (0.045 sec/batch, 1432.404 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:36,083] [train step91610] D loss: 0.32530 G loss: 2.29890 (0.040 sec/batch, 1600.813 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:36,480] [train step91620] D loss: 0.32532 G loss: 2.31487 (0.041 sec/batch, 1575.954 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:36,867] [train step91630] D loss: 0.32530 G loss: 2.29992 (0.037 sec/batch, 1736.379 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:37,260] [train step91641] D loss: 0.32533 G loss: 2.28140 (0.038 sec/batch, 1666.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:37,637] [train step91650] D loss: 0.32536 G loss: 2.32769 (0.034 sec/batch, 1862.854 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:38,016] [train step91661] D loss: 0.32524 G loss: 2.30529 (0.036 sec/batch, 1764.804 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:38,413] [train step91671] D loss: 0.32538 G loss: 2.33202 (0.033 sec/batch, 1942.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:38,806] [train step91680] D loss: 0.32557 G loss: 2.23447 (0.038 sec/batch, 1668.306 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:39,191] [train step91691] D loss: 0.32547 G loss: 2.25456 (0.037 sec/batch, 1713.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:39,581] [train step91701] D loss: 0.32531 G loss: 2.30869 (0.039 sec/batch, 1659.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:39,966] [train step91710] D loss: 0.32533 G loss: 2.34017 (0.037 sec/batch, 1738.988 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:40,366] [train step91721] D loss: 0.32542 G loss: 2.34048 (0.037 sec/batch, 1724.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:40,756] [train step91730] D loss: 0.32562 G loss: 2.36888 (0.042 sec/batch, 1510.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:41,148] [train step91740] D loss: 0.32549 G loss: 2.24644 (0.039 sec/batch, 1637.121 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:41,538] [train step91750] D loss: 0.32575 G loss: 2.21879 (0.036 sec/batch, 1793.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:41,923] [train step91761] D loss: 0.32532 G loss: 2.28962 (0.037 sec/batch, 1746.910 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:42,321] [train step91770] D loss: 0.32536 G loss: 2.27075 (0.043 sec/batch, 1487.902 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:42,711] [train step91780] D loss: 0.32549 G loss: 2.25539 (0.027 sec/batch, 2354.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:43,108] [train step91791] D loss: 0.32536 G loss: 2.28580 (0.038 sec/batch, 1665.449 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:43,512] [train step91800] D loss: 0.32542 G loss: 2.35614 (0.038 sec/batch, 1674.613 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:43,902] [train step91811] D loss: 0.32527 G loss: 2.29624 (0.040 sec/batch, 1585.308 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:44,288] [train step91821] D loss: 0.32533 G loss: 2.28556 (0.040 sec/batch, 1601.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:44,687] [train step91830] D loss: 0.32558 G loss: 2.35696 (0.040 sec/batch, 1597.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:45,076] [train step91841] D loss: 0.32530 G loss: 2.29514 (0.037 sec/batch, 1716.208 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:45,495] [train step91850] D loss: 0.32528 G loss: 2.29068 (0.040 sec/batch, 1602.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:45,895] [train step91860] D loss: 0.32532 G loss: 2.29736 (0.041 sec/batch, 1578.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:46,284] [train step91871] D loss: 0.32546 G loss: 2.35562 (0.041 sec/batch, 1561.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:46,675] [train step91881] D loss: 0.32531 G loss: 2.28232 (0.039 sec/batch, 1649.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:47,067] [train step91890] D loss: 0.32529 G loss: 2.28525 (0.043 sec/batch, 1483.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:47,472] [train step91901] D loss: 0.32537 G loss: 2.26801 (0.048 sec/batch, 1329.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:47,866] [train step91911] D loss: 0.32534 G loss: 2.32497 (0.044 sec/batch, 1446.803 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:48,261] [train step91920] D loss: 0.32546 G loss: 2.25234 (0.038 sec/batch, 1675.847 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:48,667] [train step91930] D loss: 0.32536 G loss: 2.25635 (0.040 sec/batch, 1600.106 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:49,056] [train step91941] D loss: 0.32532 G loss: 2.32510 (0.046 sec/batch, 1388.993 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:49,439] [train step91950] D loss: 0.32531 G loss: 2.32605 (0.032 sec/batch, 2016.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:49,843] [train step91960] D loss: 0.32537 G loss: 2.34787 (0.038 sec/batch, 1677.271 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:50,228] [train step91970] D loss: 0.32583 G loss: 2.40085 (0.032 sec/batch, 2029.819 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:50,637] [train step91980] D loss: 0.32547 G loss: 2.26785 (0.036 sec/batch, 1754.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:51,031] [train step91990] D loss: 0.32531 G loss: 2.28206 (0.034 sec/batch, 1907.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:51,426] [train step92000] D loss: 0.32529 G loss: 2.30426 (0.048 sec/batch, 1342.667 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:08:51,426] Saved checkpoint at 92000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:52,047] [train step92010] D loss: 0.32535 G loss: 2.31672 (0.034 sec/batch, 1861.511 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:52,434] [train step92020] D loss: 0.32548 G loss: 2.34391 (0.030 sec/batch, 2140.873 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:52,850] [train step92031] D loss: 0.32537 G loss: 2.34800 (0.040 sec/batch, 1588.300 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:53,235] [train step92040] D loss: 0.32528 G loss: 2.30438 (0.041 sec/batch, 1563.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:53,623] [train step92051] D loss: 0.32552 G loss: 2.36014 (0.042 sec/batch, 1514.608 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:54,029] [train step92061] D loss: 0.32532 G loss: 2.29795 (0.038 sec/batch, 1673.548 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:54,417] [train step92070] D loss: 0.32559 G loss: 2.37304 (0.031 sec/batch, 2081.766 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:54,828] [train step92080] D loss: 0.32532 G loss: 2.31303 (0.037 sec/batch, 1736.345 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:55,222] [train step92090] D loss: 0.32567 G loss: 2.23136 (0.040 sec/batch, 1583.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:55,614] [train step92100] D loss: 0.32566 G loss: 2.37747 (0.036 sec/batch, 1789.403 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:56,026] [train step92111] D loss: 0.32541 G loss: 2.34770 (0.039 sec/batch, 1642.933 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:56,408] [train step92121] D loss: 0.32530 G loss: 2.32288 (0.037 sec/batch, 1720.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:56,800] [train step92130] D loss: 0.32536 G loss: 2.26449 (0.037 sec/batch, 1722.319 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:57,191] [train step92140] D loss: 0.32538 G loss: 2.28881 (0.038 sec/batch, 1695.193 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:57,578] [train step92150] D loss: 0.32539 G loss: 2.29835 (0.038 sec/batch, 1667.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:57,964] [train step92160] D loss: 0.32541 G loss: 2.33085 (0.039 sec/batch, 1656.058 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:58,337] [train step92171] D loss: 0.32531 G loss: 2.29061 (0.035 sec/batch, 1805.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:58,711] [train step92181] D loss: 0.32537 G loss: 2.31680 (0.042 sec/batch, 1518.996 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:59,103] [train step92190] D loss: 0.32548 G loss: 2.25339 (0.038 sec/batch, 1682.591 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:59,477] [train step92201] D loss: 0.32547 G loss: 2.25508 (0.040 sec/batch, 1581.796 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:08:59,866] [train step92211] D loss: 0.32536 G loss: 2.26410 (0.036 sec/batch, 1780.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:00,252] [train step92220] D loss: 0.32547 G loss: 2.35182 (0.038 sec/batch, 1704.720 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:00,633] [train step92230] D loss: 0.32567 G loss: 2.37425 (0.035 sec/batch, 1818.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:01,020] [train step92240] D loss: 0.32543 G loss: 2.26224 (0.039 sec/batch, 1662.314 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:01,396] [train step92250] D loss: 0.32562 G loss: 2.37787 (0.036 sec/batch, 1779.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:01,762] [train step92260] D loss: 0.32534 G loss: 2.27299 (0.028 sec/batch, 2278.894 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:02,157] [train step92271] D loss: 0.32540 G loss: 2.26058 (0.037 sec/batch, 1751.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:02,536] [train step92280] D loss: 0.32551 G loss: 2.36383 (0.033 sec/batch, 1913.337 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:02,928] [train step92291] D loss: 0.32629 G loss: 2.44111 (0.037 sec/batch, 1729.877 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:03,301] [train step92301] D loss: 0.32562 G loss: 2.38287 (0.039 sec/batch, 1635.425 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:03,677] [train step92310] D loss: 0.32528 G loss: 2.29918 (0.036 sec/batch, 1772.062 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:04,063] [train step92321] D loss: 0.32619 G loss: 2.42864 (0.034 sec/batch, 1862.066 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:04,442] [train step92331] D loss: 0.32539 G loss: 2.27147 (0.036 sec/batch, 1769.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:04,826] [train step92340] D loss: 0.32539 G loss: 2.34918 (0.038 sec/batch, 1679.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:05,210] [train step92351] D loss: 0.32545 G loss: 2.36277 (0.043 sec/batch, 1492.743 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:05,590] [train step92360] D loss: 0.32528 G loss: 2.30137 (0.036 sec/batch, 1789.331 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:05,982] [train step92370] D loss: 0.32529 G loss: 2.29590 (0.048 sec/batch, 1326.387 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:06,369] [train step92380] D loss: 0.32534 G loss: 2.26887 (0.047 sec/batch, 1365.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:06,744] [train step92390] D loss: 0.32535 G loss: 2.27448 (0.039 sec/batch, 1634.868 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:07,139] [train step92400] D loss: 0.32539 G loss: 2.33719 (0.043 sec/batch, 1486.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:07,529] [train step92411] D loss: 0.32540 G loss: 2.35103 (0.037 sec/batch, 1728.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:07,918] [train step92421] D loss: 0.32526 G loss: 2.30152 (0.036 sec/batch, 1793.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:08,310] [train step92430] D loss: 0.32529 G loss: 2.31456 (0.047 sec/batch, 1364.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:08,694] [train step92440] D loss: 0.32530 G loss: 2.31325 (0.037 sec/batch, 1721.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:09,106] [train step92451] D loss: 0.32550 G loss: 2.36458 (0.046 sec/batch, 1385.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:09,505] [train step92460] D loss: 0.32571 G loss: 2.22435 (0.035 sec/batch, 1821.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:09,904] [train step92471] D loss: 0.32531 G loss: 2.26419 (0.045 sec/batch, 1430.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:10,312] [train step92481] D loss: 0.32534 G loss: 2.33245 (0.038 sec/batch, 1680.252 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:10,713] [train step92490] D loss: 0.32568 G loss: 2.22400 (0.041 sec/batch, 1565.176 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:11,113] [train step92500] D loss: 0.32586 G loss: 2.20520 (0.039 sec/batch, 1635.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:11,519] [train step92510] D loss: 0.32560 G loss: 2.22793 (0.043 sec/batch, 1505.265 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:11,915] [train step92520] D loss: 0.32546 G loss: 2.36670 (0.036 sec/batch, 1776.119 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:12,306] [train step92530] D loss: 0.32550 G loss: 2.35088 (0.032 sec/batch, 1984.369 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:12,706] [train step92541] D loss: 0.32528 G loss: 2.28003 (0.037 sec/batch, 1737.986 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:13,103] [train step92550] D loss: 0.32540 G loss: 2.34904 (0.035 sec/batch, 1836.335 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:13,514] [train step92560] D loss: 0.32531 G loss: 2.32498 (0.039 sec/batch, 1642.370 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:13,916] [train step92571] D loss: 0.32532 G loss: 2.33705 (0.039 sec/batch, 1628.411 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:14,329] [train step92580] D loss: 0.32532 G loss: 2.28128 (0.038 sec/batch, 1699.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:14,722] [train step92590] D loss: 0.32536 G loss: 2.33385 (0.039 sec/batch, 1658.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:15,122] [train step92600] D loss: 0.32542 G loss: 2.34525 (0.039 sec/batch, 1623.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:15,513] [train step92610] D loss: 0.32548 G loss: 2.25866 (0.038 sec/batch, 1703.303 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:15,901] [train step92621] D loss: 0.32527 G loss: 2.28552 (0.039 sec/batch, 1638.710 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:16,291] [train step92631] D loss: 0.32543 G loss: 2.25753 (0.045 sec/batch, 1435.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:16,685] [train step92640] D loss: 0.32533 G loss: 2.33767 (0.034 sec/batch, 1880.235 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:17,086] [train step92650] D loss: 0.32528 G loss: 2.28325 (0.037 sec/batch, 1721.347 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:17,487] [train step92661] D loss: 0.32543 G loss: 2.26786 (0.035 sec/batch, 1840.692 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:17,867] [train step92670] D loss: 0.32529 G loss: 2.29260 (0.036 sec/batch, 1776.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:18,253] [train step92681] D loss: 0.32556 G loss: 2.23440 (0.036 sec/batch, 1795.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:18,655] [train step92690] D loss: 0.32544 G loss: 2.24847 (0.037 sec/batch, 1725.441 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:19,042] [train step92700] D loss: 0.32544 G loss: 2.35659 (0.041 sec/batch, 1561.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:19,436] [train step92711] D loss: 0.32534 G loss: 2.30877 (0.037 sec/batch, 1712.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:19,827] [train step92720] D loss: 0.32532 G loss: 2.28023 (0.047 sec/batch, 1372.538 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:20,216] [train step92730] D loss: 0.32534 G loss: 2.33114 (0.039 sec/batch, 1636.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:20,609] [train step92741] D loss: 0.32534 G loss: 2.29813 (0.038 sec/batch, 1698.647 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:21,008] [train step92751] D loss: 0.32537 G loss: 2.27573 (0.039 sec/batch, 1633.902 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:21,400] [train step92760] D loss: 0.32568 G loss: 2.22408 (0.038 sec/batch, 1684.290 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:21,796] [train step92771] D loss: 0.32582 G loss: 2.21001 (0.042 sec/batch, 1510.491 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:22,181] [train step92781] D loss: 0.32553 G loss: 2.36967 (0.038 sec/batch, 1686.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:22,574] [train step92790] D loss: 0.32570 G loss: 2.21697 (0.037 sec/batch, 1725.896 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:22,955] [train step92800] D loss: 0.32533 G loss: 2.29748 (0.041 sec/batch, 1566.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:23,340] [train step92810] D loss: 0.32523 G loss: 2.30426 (0.040 sec/batch, 1596.243 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:23,743] [train step92820] D loss: 0.32531 G loss: 2.32961 (0.043 sec/batch, 1489.992 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:24,135] [train step92830] D loss: 0.32532 G loss: 2.31246 (0.039 sec/batch, 1626.182 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:24,537] [train step92840] D loss: 0.32529 G loss: 2.30793 (0.040 sec/batch, 1603.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:24,931] [train step92850] D loss: 0.32534 G loss: 2.26980 (0.041 sec/batch, 1546.546 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:25,314] [train step92860] D loss: 0.32532 G loss: 2.28302 (0.037 sec/batch, 1718.383 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:25,712] [train step92870] D loss: 0.32533 G loss: 2.32444 (0.036 sec/batch, 1756.455 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:26,098] [train step92880] D loss: 0.32528 G loss: 2.30057 (0.040 sec/batch, 1597.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:26,484] [train step92891] D loss: 0.32525 G loss: 2.30031 (0.040 sec/batch, 1606.205 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:26,876] [train step92901] D loss: 0.32531 G loss: 2.30827 (0.042 sec/batch, 1521.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:27,270] [train step92910] D loss: 0.32529 G loss: 2.27352 (0.036 sec/batch, 1790.477 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:27,673] [train step92921] D loss: 0.32522 G loss: 2.30551 (0.048 sec/batch, 1326.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:28,057] [train step92931] D loss: 0.32532 G loss: 2.34037 (0.035 sec/batch, 1850.845 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:28,440] [train step92940] D loss: 0.32530 G loss: 2.26391 (0.039 sec/batch, 1638.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:28,820] [train step92951] D loss: 0.32523 G loss: 2.28533 (0.037 sec/batch, 1727.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:29,207] [train step92961] D loss: 0.32540 G loss: 2.26125 (0.036 sec/batch, 1778.189 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:29,589] [train step92970] D loss: 0.32525 G loss: 2.30895 (0.037 sec/batch, 1727.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:29,978] [train step92980] D loss: 0.32530 G loss: 2.29138 (0.031 sec/batch, 2070.462 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:30,367] [train step92991] D loss: 0.32534 G loss: 2.27165 (0.042 sec/batch, 1528.658 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:30,742] [train step93000] D loss: 0.32537 G loss: 2.34336 (0.037 sec/batch, 1744.776 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:09:30,742] Saved checkpoint at 93000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:31,340] [train step93011] D loss: 0.32533 G loss: 2.33021 (0.034 sec/batch, 1864.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:31,715] [train step93021] D loss: 0.32535 G loss: 2.26605 (0.042 sec/batch, 1539.327 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:32,092] [train step93030] D loss: 0.32531 G loss: 2.32989 (0.037 sec/batch, 1728.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:32,467] [train step93041] D loss: 0.32555 G loss: 2.37682 (0.040 sec/batch, 1612.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:32,844] [train step93051] D loss: 0.32535 G loss: 2.33149 (0.037 sec/batch, 1748.435 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:33,215] [train step93060] D loss: 0.32525 G loss: 2.30346 (0.036 sec/batch, 1759.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:33,598] [train step93070] D loss: 0.32529 G loss: 2.30365 (0.038 sec/batch, 1670.923 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:33,987] [train step93081] D loss: 0.32531 G loss: 2.27712 (0.034 sec/batch, 1855.823 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:34,366] [train step93090] D loss: 0.32542 G loss: 2.35115 (0.036 sec/batch, 1801.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:34,748] [train step93101] D loss: 0.32532 G loss: 2.31457 (0.033 sec/batch, 1916.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:35,149] [train step93110] D loss: 0.32537 G loss: 2.33871 (0.039 sec/batch, 1624.155 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:35,535] [train step93120] D loss: 0.32548 G loss: 2.25018 (0.042 sec/batch, 1521.812 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:35,917] [train step93131] D loss: 0.32536 G loss: 2.26287 (0.034 sec/batch, 1864.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:36,292] [train step93140] D loss: 0.32539 G loss: 2.34540 (0.029 sec/batch, 2203.144 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:36,678] [train step93150] D loss: 0.32533 G loss: 2.29459 (0.037 sec/batch, 1743.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:37,066] [train step93160] D loss: 0.32538 G loss: 2.35371 (0.037 sec/batch, 1712.343 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:37,446] [train step93170] D loss: 0.32544 G loss: 2.35568 (0.033 sec/batch, 1957.425 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:37,832] [train step93180] D loss: 0.32538 G loss: 2.26031 (0.038 sec/batch, 1696.478 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:38,222] [train step93191] D loss: 0.32533 G loss: 2.32704 (0.035 sec/batch, 1818.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:38,605] [train step93201] D loss: 0.32528 G loss: 2.29890 (0.038 sec/batch, 1664.365 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:38,995] [train step93210] D loss: 0.32531 G loss: 2.28048 (0.036 sec/batch, 1765.384 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:39,381] [train step93220] D loss: 0.32546 G loss: 2.25860 (0.040 sec/batch, 1583.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:39,762] [train step93230] D loss: 0.32535 G loss: 2.27860 (0.038 sec/batch, 1668.171 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:40,169] [train step93240] D loss: 0.32529 G loss: 2.29179 (0.037 sec/batch, 1736.738 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:40,566] [train step93250] D loss: 0.32534 G loss: 2.28724 (0.037 sec/batch, 1710.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:40,949] [train step93260] D loss: 0.32567 G loss: 2.39168 (0.042 sec/batch, 1529.730 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:41,345] [train step93270] D loss: 0.32532 G loss: 2.27704 (0.042 sec/batch, 1531.344 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:41,729] [train step93281] D loss: 0.32535 G loss: 2.34248 (0.035 sec/batch, 1828.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:42,128] [train step93290] D loss: 0.32524 G loss: 2.31791 (0.038 sec/batch, 1691.689 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:42,511] [train step93300] D loss: 0.32533 G loss: 2.30928 (0.039 sec/batch, 1639.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:42,892] [train step93311] D loss: 0.32586 G loss: 2.41156 (0.038 sec/batch, 1702.622 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:43,293] [train step93321] D loss: 0.32547 G loss: 2.36252 (0.038 sec/batch, 1668.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:43,682] [train step93330] D loss: 0.32576 G loss: 2.21469 (0.039 sec/batch, 1622.623 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:44,072] [train step93340] D loss: 0.32550 G loss: 2.24081 (0.036 sec/batch, 1801.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:44,465] [train step93350] D loss: 0.32530 G loss: 2.28258 (0.039 sec/batch, 1637.001 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:44,845] [train step93360] D loss: 0.32526 G loss: 2.32715 (0.042 sec/batch, 1525.063 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:45,241] [train step93370] D loss: 0.32531 G loss: 2.33925 (0.037 sec/batch, 1731.172 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:45,631] [train step93381] D loss: 0.32533 G loss: 2.33664 (0.039 sec/batch, 1630.469 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:46,026] [train step93390] D loss: 0.32532 G loss: 2.31727 (0.039 sec/batch, 1657.060 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:46,425] [train step93401] D loss: 0.32524 G loss: 2.29557 (0.039 sec/batch, 1654.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:46,816] [train step93411] D loss: 0.32536 G loss: 2.27178 (0.036 sec/batch, 1787.353 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:47,231] [train step93420] D loss: 0.32525 G loss: 2.30329 (0.035 sec/batch, 1811.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:47,625] [train step93430] D loss: 0.32529 G loss: 2.29912 (0.038 sec/batch, 1682.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:48,020] [train step93440] D loss: 0.32532 G loss: 2.27078 (0.039 sec/batch, 1656.467 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:48,426] [train step93450] D loss: 0.32530 G loss: 2.33893 (0.043 sec/batch, 1484.176 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:48,814] [train step93460] D loss: 0.32522 G loss: 2.31703 (0.041 sec/batch, 1555.778 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:49,201] [train step93471] D loss: 0.32532 G loss: 2.33274 (0.049 sec/batch, 1307.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:49,586] [train step93480] D loss: 0.32530 G loss: 2.28322 (0.041 sec/batch, 1562.615 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:49,969] [train step93491] D loss: 0.32534 G loss: 2.34095 (0.038 sec/batch, 1673.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:50,363] [train step93501] D loss: 0.32580 G loss: 2.40056 (0.041 sec/batch, 1565.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:50,766] [train step93510] D loss: 0.32541 G loss: 2.25832 (0.042 sec/batch, 1510.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:51,160] [train step93521] D loss: 0.32527 G loss: 2.29840 (0.036 sec/batch, 1755.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:51,559] [train step93530] D loss: 0.32528 G loss: 2.30538 (0.042 sec/batch, 1524.283 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:51,948] [train step93540] D loss: 0.32523 G loss: 2.30657 (0.036 sec/batch, 1793.072 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:52,346] [train step93551] D loss: 0.32524 G loss: 2.31043 (0.041 sec/batch, 1548.508 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:52,736] [train step93561] D loss: 0.32531 G loss: 2.29120 (0.037 sec/batch, 1711.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:53,121] [train step93570] D loss: 0.32522 G loss: 2.29642 (0.037 sec/batch, 1724.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:53,518] [train step93581] D loss: 0.32526 G loss: 2.30558 (0.043 sec/batch, 1504.261 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:53,906] [train step93590] D loss: 0.32536 G loss: 2.35108 (0.035 sec/batch, 1818.213 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:54,293] [train step93600] D loss: 0.32522 G loss: 2.30980 (0.028 sec/batch, 2312.086 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:54,697] [train step93610] D loss: 0.32547 G loss: 2.35829 (0.040 sec/batch, 1611.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:55,084] [train step93621] D loss: 0.32539 G loss: 2.25380 (0.041 sec/batch, 1562.452 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:55,478] [train step93630] D loss: 0.32590 G loss: 2.41526 (0.039 sec/batch, 1643.667 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:55,857] [train step93641] D loss: 0.32542 G loss: 2.35088 (0.037 sec/batch, 1735.638 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:56,244] [train step93651] D loss: 0.32528 G loss: 2.34322 (0.037 sec/batch, 1739.665 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:56,636] [train step93660] D loss: 0.32536 G loss: 2.27590 (0.039 sec/batch, 1627.967 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:57,026] [train step93670] D loss: 0.32524 G loss: 2.29776 (0.042 sec/batch, 1513.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:57,410] [train step93681] D loss: 0.32525 G loss: 2.31229 (0.036 sec/batch, 1768.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:57,796] [train step93690] D loss: 0.32556 G loss: 2.23329 (0.036 sec/batch, 1774.451 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:58,183] [train step93700] D loss: 0.32538 G loss: 2.25015 (0.038 sec/batch, 1677.292 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:58,587] [train step93710] D loss: 0.32521 G loss: 2.29666 (0.038 sec/batch, 1691.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:58,975] [train step93720] D loss: 0.32527 G loss: 2.32031 (0.037 sec/batch, 1723.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:59,354] [train step93730] D loss: 0.32530 G loss: 2.27781 (0.037 sec/batch, 1740.307 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:09:59,741] [train step93740] D loss: 0.32528 G loss: 2.27387 (0.037 sec/batch, 1711.251 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:00,111] [train step93750] D loss: 0.32541 G loss: 2.34452 (0.027 sec/batch, 2345.275 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:00,516] [train step93761] D loss: 0.32523 G loss: 2.30730 (0.045 sec/batch, 1414.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:00,892] [train step93770] D loss: 0.32524 G loss: 2.31218 (0.038 sec/batch, 1692.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:01,272] [train step93780] D loss: 0.32528 G loss: 2.28776 (0.042 sec/batch, 1513.856 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:01,660] [train step93791] D loss: 0.32528 G loss: 2.30272 (0.043 sec/batch, 1490.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:02,039] [train step93801] D loss: 0.32523 G loss: 2.30919 (0.035 sec/batch, 1807.488 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:02,411] [train step93810] D loss: 0.32528 G loss: 2.29072 (0.034 sec/batch, 1896.266 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:02,804] [train step93821] D loss: 0.32524 G loss: 2.28634 (0.034 sec/batch, 1895.905 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:03,195] [train step93830] D loss: 0.32531 G loss: 2.33882 (0.040 sec/batch, 1598.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:03,593] [train step93840] D loss: 0.32534 G loss: 2.27194 (0.051 sec/batch, 1262.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:03,976] [train step93850] D loss: 0.32526 G loss: 2.32704 (0.038 sec/batch, 1702.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:04,358] [train step93860] D loss: 0.32530 G loss: 2.33958 (0.037 sec/batch, 1721.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:04,766] [train step93870] D loss: 0.32529 G loss: 2.28967 (0.039 sec/batch, 1649.221 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:05,148] [train step93881] D loss: 0.32534 G loss: 2.27404 (0.037 sec/batch, 1716.636 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:05,542] [train step93891] D loss: 0.32531 G loss: 2.27638 (0.037 sec/batch, 1708.115 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:05,940] [train step93900] D loss: 0.32542 G loss: 2.35445 (0.038 sec/batch, 1683.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:06,321] [train step93911] D loss: 0.32567 G loss: 2.38884 (0.036 sec/batch, 1776.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:06,720] [train step93920] D loss: 0.32584 G loss: 2.40521 (0.037 sec/batch, 1738.650 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:07,115] [train step93930] D loss: 0.32527 G loss: 2.28355 (0.040 sec/batch, 1596.842 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:07,500] [train step93941] D loss: 0.32551 G loss: 2.37014 (0.038 sec/batch, 1662.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:07,905] [train step93950] D loss: 0.32556 G loss: 2.37503 (0.040 sec/batch, 1583.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:08,286] [train step93960] D loss: 0.32586 G loss: 2.20052 (0.032 sec/batch, 2003.833 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:08,683] [train step93970] D loss: 0.32587 G loss: 2.19822 (0.034 sec/batch, 1908.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:09,094] [train step93980] D loss: 0.32542 G loss: 2.25139 (0.047 sec/batch, 1347.859 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:09,475] [train step93990] D loss: 0.32540 G loss: 2.34618 (0.035 sec/batch, 1811.953 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:09,868] [train step94001] D loss: 0.32534 G loss: 2.33132 (0.032 sec/batch, 2018.236 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:10:09,869] Saved checkpoint at 94000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:10,495] [train step94010] D loss: 0.32536 G loss: 2.26435 (0.039 sec/batch, 1640.252 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:10,897] [train step94020] D loss: 0.32550 G loss: 2.35535 (0.039 sec/batch, 1658.729 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:11,285] [train step94030] D loss: 0.32556 G loss: 2.37625 (0.042 sec/batch, 1522.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:11,688] [train step94041] D loss: 0.32526 G loss: 2.32661 (0.046 sec/batch, 1383.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:12,089] [train step94050] D loss: 0.32520 G loss: 2.29740 (0.036 sec/batch, 1800.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:12,478] [train step94061] D loss: 0.32532 G loss: 2.33258 (0.040 sec/batch, 1613.436 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:12,870] [train step94071] D loss: 0.32534 G loss: 2.34453 (0.042 sec/batch, 1515.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:13,260] [train step94080] D loss: 0.32538 G loss: 2.25807 (0.035 sec/batch, 1850.807 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:13,655] [train step94091] D loss: 0.32533 G loss: 2.26395 (0.039 sec/batch, 1634.350 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:14,046] [train step94101] D loss: 0.32528 G loss: 2.31558 (0.039 sec/batch, 1639.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:14,428] [train step94110] D loss: 0.32525 G loss: 2.29522 (0.038 sec/batch, 1678.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:14,816] [train step94120] D loss: 0.32529 G loss: 2.32328 (0.036 sec/batch, 1789.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:15,221] [train step94130] D loss: 0.32542 G loss: 2.35919 (0.047 sec/batch, 1348.739 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:15,606] [train step94140] D loss: 0.32553 G loss: 2.23185 (0.034 sec/batch, 1897.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:16,006] [train step94150] D loss: 0.32533 G loss: 2.25943 (0.038 sec/batch, 1683.065 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:16,390] [train step94161] D loss: 0.32526 G loss: 2.32115 (0.035 sec/batch, 1836.197 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:16,778] [train step94170] D loss: 0.32531 G loss: 2.27746 (0.039 sec/batch, 1637.151 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:17,168] [train step94181] D loss: 0.32527 G loss: 2.28228 (0.037 sec/batch, 1719.506 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:17,562] [train step94190] D loss: 0.32528 G loss: 2.33001 (0.037 sec/batch, 1732.781 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:17,974] [train step94200] D loss: 0.32537 G loss: 2.25436 (0.042 sec/batch, 1540.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:18,369] [train step94210] D loss: 0.32532 G loss: 2.32517 (0.036 sec/batch, 1758.169 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:18,762] [train step94221] D loss: 0.32607 G loss: 2.42202 (0.040 sec/batch, 1584.906 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:19,162] [train step94230] D loss: 0.32529 G loss: 2.26608 (0.041 sec/batch, 1563.535 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:19,543] [train step94241] D loss: 0.32544 G loss: 2.35294 (0.038 sec/batch, 1690.815 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:19,930] [train step94250] D loss: 0.32527 G loss: 2.31544 (0.040 sec/batch, 1613.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:20,331] [train step94260] D loss: 0.32531 G loss: 2.28263 (0.038 sec/batch, 1678.634 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:20,717] [train step94271] D loss: 0.32531 G loss: 2.27726 (0.035 sec/batch, 1821.124 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:21,128] [train step94281] D loss: 0.32534 G loss: 2.26633 (0.039 sec/batch, 1661.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:21,520] [train step94290] D loss: 0.32554 G loss: 2.37349 (0.036 sec/batch, 1754.641 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:21,899] [train step94300] D loss: 0.32550 G loss: 2.36484 (0.038 sec/batch, 1675.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:22,301] [train step94311] D loss: 0.32531 G loss: 2.31948 (0.039 sec/batch, 1629.627 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:22,685] [train step94320] D loss: 0.32538 G loss: 2.25720 (0.034 sec/batch, 1903.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:23,082] [train step94330] D loss: 0.32528 G loss: 2.29384 (0.038 sec/batch, 1698.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:23,482] [train step94340] D loss: 0.32532 G loss: 2.30649 (0.046 sec/batch, 1404.583 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:23,874] [train step94350] D loss: 0.32531 G loss: 2.28206 (0.043 sec/batch, 1485.128 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:24,262] [train step94361] D loss: 0.32529 G loss: 2.30841 (0.036 sec/batch, 1759.932 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:24,657] [train step94370] D loss: 0.32520 G loss: 2.28979 (0.039 sec/batch, 1632.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:25,041] [train step94380] D loss: 0.32562 G loss: 2.38298 (0.032 sec/batch, 1998.239 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:25,442] [train step94390] D loss: 0.32624 G loss: 2.43668 (0.037 sec/batch, 1718.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:25,824] [train step94401] D loss: 0.32581 G loss: 2.40184 (0.034 sec/batch, 1875.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:26,225] [train step94410] D loss: 0.32545 G loss: 2.24248 (0.046 sec/batch, 1386.067 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:26,611] [train step94421] D loss: 0.32530 G loss: 2.32711 (0.039 sec/batch, 1652.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:26,992] [train step94431] D loss: 0.32559 G loss: 2.37984 (0.035 sec/batch, 1834.540 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:27,379] [train step94440] D loss: 0.32592 G loss: 2.20094 (0.031 sec/batch, 2046.782 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:27,769] [train step94451] D loss: 0.32596 G loss: 2.19705 (0.036 sec/batch, 1768.233 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:28,155] [train step94461] D loss: 0.32543 G loss: 2.24737 (0.040 sec/batch, 1616.886 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:28,556] [train step94470] D loss: 0.32533 G loss: 2.26576 (0.044 sec/batch, 1469.526 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:28,939] [train step94481] D loss: 0.32569 G loss: 2.21409 (0.037 sec/batch, 1736.862 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:29,333] [train step94490] D loss: 0.32523 G loss: 2.30772 (0.040 sec/batch, 1582.532 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:29,721] [train step94500] D loss: 0.32529 G loss: 2.30202 (0.038 sec/batch, 1669.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:30,100] [train step94510] D loss: 0.32527 G loss: 2.32136 (0.035 sec/batch, 1821.630 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:30,487] [train step94521] D loss: 0.32528 G loss: 2.31209 (0.035 sec/batch, 1811.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:30,856] [train step94530] D loss: 0.32527 G loss: 2.31313 (0.038 sec/batch, 1697.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:31,235] [train step94541] D loss: 0.32525 G loss: 2.30695 (0.034 sec/batch, 1887.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:31,624] [train step94551] D loss: 0.32535 G loss: 2.25546 (0.037 sec/batch, 1715.418 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:32,004] [train step94560] D loss: 0.32535 G loss: 2.33596 (0.038 sec/batch, 1678.299 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:32,384] [train step94571] D loss: 0.32553 G loss: 2.36862 (0.047 sec/batch, 1362.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:32,763] [train step94581] D loss: 0.32575 G loss: 2.39533 (0.038 sec/batch, 1680.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:33,145] [train step94590] D loss: 0.32543 G loss: 2.24840 (0.034 sec/batch, 1885.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:33,539] [train step94601] D loss: 0.32525 G loss: 2.27463 (0.039 sec/batch, 1655.537 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:33,920] [train step94610] D loss: 0.32530 G loss: 2.27657 (0.039 sec/batch, 1633.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:34,305] [train step94620] D loss: 0.32547 G loss: 2.36818 (0.036 sec/batch, 1770.788 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:34,694] [train step94631] D loss: 0.32566 G loss: 2.38676 (0.040 sec/batch, 1588.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:35,082] [train step94641] D loss: 0.32566 G loss: 2.38384 (0.035 sec/batch, 1814.415 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:35,463] [train step94650] D loss: 0.32566 G loss: 2.22106 (0.045 sec/batch, 1424.711 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:35,845] [train step94661] D loss: 0.32586 G loss: 2.19988 (0.038 sec/batch, 1692.659 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:36,236] [train step94670] D loss: 0.32639 G loss: 2.16641 (0.036 sec/batch, 1755.203 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:36,624] [train step94680] D loss: 0.32671 G loss: 2.46443 (0.041 sec/batch, 1566.034 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:37,007] [train step94691] D loss: 0.32555 G loss: 2.36412 (0.034 sec/batch, 1892.456 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:37,395] [train step94701] D loss: 0.32548 G loss: 2.23567 (0.036 sec/batch, 1759.529 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:37,788] [train step94710] D loss: 0.32573 G loss: 2.39845 (0.036 sec/batch, 1790.405 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:38,169] [train step94720] D loss: 0.32543 G loss: 2.36258 (0.041 sec/batch, 1570.542 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:38,553] [train step94731] D loss: 0.32528 G loss: 2.31536 (0.049 sec/batch, 1311.841 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:38,928] [train step94740] D loss: 0.32524 G loss: 2.30933 (0.036 sec/batch, 1763.934 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:39,305] [train step94750] D loss: 0.32538 G loss: 2.35364 (0.034 sec/batch, 1855.566 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:39,700] [train step94761] D loss: 0.32530 G loss: 2.32692 (0.035 sec/batch, 1839.393 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:40,094] [train step94770] D loss: 0.32524 G loss: 2.31498 (0.035 sec/batch, 1803.723 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:40,492] [train step94780] D loss: 0.32526 G loss: 2.31489 (0.037 sec/batch, 1706.801 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:40,885] [train step94790] D loss: 0.32533 G loss: 2.27646 (0.038 sec/batch, 1698.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:41,278] [train step94800] D loss: 0.32528 G loss: 2.33376 (0.036 sec/batch, 1755.834 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:41,674] [train step94811] D loss: 0.32526 G loss: 2.29057 (0.040 sec/batch, 1609.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:42,055] [train step94820] D loss: 0.32531 G loss: 2.29020 (0.037 sec/batch, 1750.111 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:42,439] [train step94830] D loss: 0.32528 G loss: 2.28347 (0.033 sec/batch, 1916.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:42,841] [train step94841] D loss: 0.32531 G loss: 2.27664 (0.036 sec/batch, 1782.475 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:43,231] [train step94850] D loss: 0.32532 G loss: 2.26763 (0.039 sec/batch, 1647.966 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:43,616] [train step94860] D loss: 0.32535 G loss: 2.33863 (0.030 sec/batch, 2111.504 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:44,019] [train step94870] D loss: 0.32524 G loss: 2.29891 (0.040 sec/batch, 1597.564 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:44,408] [train step94880] D loss: 0.32595 G loss: 2.19098 (0.037 sec/batch, 1709.900 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:44,799] [train step94890] D loss: 0.32625 G loss: 2.43978 (0.033 sec/batch, 1917.040 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:45,188] [train step94901] D loss: 0.32572 G loss: 2.39370 (0.036 sec/batch, 1760.175 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:45,577] [train step94910] D loss: 0.32523 G loss: 2.31314 (0.041 sec/batch, 1547.830 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:45,977] [train step94920] D loss: 0.32529 G loss: 2.33311 (0.037 sec/batch, 1739.011 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:46,377] [train step94930] D loss: 0.32557 G loss: 2.37602 (0.040 sec/batch, 1591.200 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:46,762] [train step94941] D loss: 0.32523 G loss: 2.29373 (0.040 sec/batch, 1593.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:47,154] [train step94950] D loss: 0.32537 G loss: 2.34488 (0.035 sec/batch, 1824.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:47,545] [train step94960] D loss: 0.32566 G loss: 2.39096 (0.041 sec/batch, 1546.269 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:47,943] [train step94971] D loss: 0.32583 G loss: 2.40536 (0.036 sec/batch, 1753.838 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:48,332] [train step94980] D loss: 0.32541 G loss: 2.25658 (0.039 sec/batch, 1639.661 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:48,722] [train step94990] D loss: 0.32532 G loss: 2.34653 (0.037 sec/batch, 1726.284 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:49,125] [train step95001] D loss: 0.32577 G loss: 2.39860 (0.042 sec/batch, 1514.164 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:10:49,125] Saved checkpoint at 95000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:49,725] [train step95010] D loss: 0.32572 G loss: 2.21626 (0.037 sec/batch, 1725.031 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:50,122] [train step95021] D loss: 0.32532 G loss: 2.25799 (0.037 sec/batch, 1722.872 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:50,506] [train step95031] D loss: 0.32534 G loss: 2.28743 (0.037 sec/batch, 1734.214 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:50,889] [train step95040] D loss: 0.32528 G loss: 2.27038 (0.038 sec/batch, 1685.443 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:51,272] [train step95051] D loss: 0.32544 G loss: 2.23972 (0.037 sec/batch, 1729.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:51,665] [train step95060] D loss: 0.32533 G loss: 2.27334 (0.041 sec/batch, 1558.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:52,061] [train step95070] D loss: 0.32529 G loss: 2.28009 (0.039 sec/batch, 1620.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:52,443] [train step95080] D loss: 0.32562 G loss: 2.22506 (0.038 sec/batch, 1681.516 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:52,834] [train step95091] D loss: 0.32667 G loss: 2.14876 (0.037 sec/batch, 1735.043 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:53,234] [train step95100] D loss: 0.32667 G loss: 2.47005 (0.038 sec/batch, 1675.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:53,624] [train step95111] D loss: 0.32579 G loss: 2.40355 (0.036 sec/batch, 1777.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:54,015] [train step95121] D loss: 0.32541 G loss: 2.35202 (0.036 sec/batch, 1786.699 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:54,408] [train step95130] D loss: 0.32562 G loss: 2.23175 (0.038 sec/batch, 1676.286 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:54,801] [train step95140] D loss: 0.32537 G loss: 2.26173 (0.037 sec/batch, 1722.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:55,200] [train step95151] D loss: 0.32539 G loss: 2.25715 (0.041 sec/batch, 1549.375 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:55,586] [train step95160] D loss: 0.32532 G loss: 2.34014 (0.036 sec/batch, 1755.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:55,980] [train step95171] D loss: 0.32521 G loss: 2.29497 (0.041 sec/batch, 1560.581 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:56,383] [train step95181] D loss: 0.32531 G loss: 2.30877 (0.038 sec/batch, 1702.730 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:56,765] [train step95190] D loss: 0.32531 G loss: 2.27759 (0.037 sec/batch, 1739.022 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:57,168] [train step95201] D loss: 0.32542 G loss: 2.25322 (0.038 sec/batch, 1695.771 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:57,554] [train step95211] D loss: 0.32588 G loss: 2.19940 (0.037 sec/batch, 1750.430 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:57,941] [train step95220] D loss: 0.32575 G loss: 2.38932 (0.035 sec/batch, 1838.838 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:58,354] [train step95230] D loss: 0.32521 G loss: 2.29103 (0.038 sec/batch, 1681.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:58,732] [train step95241] D loss: 0.32550 G loss: 2.23662 (0.036 sec/batch, 1790.596 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:59,128] [train step95250] D loss: 0.32553 G loss: 2.36518 (0.045 sec/batch, 1435.737 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:59,520] [train step95261] D loss: 0.32522 G loss: 2.29858 (0.038 sec/batch, 1663.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:10:59,903] [train step95271] D loss: 0.32547 G loss: 2.23679 (0.036 sec/batch, 1758.434 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:00,296] [train step95280] D loss: 0.32545 G loss: 2.35932 (0.040 sec/batch, 1582.103 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:00,677] [train step95291] D loss: 0.32523 G loss: 2.30495 (0.042 sec/batch, 1506.414 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:01,051] [train step95300] D loss: 0.32527 G loss: 2.26228 (0.035 sec/batch, 1825.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:01,436] [train step95310] D loss: 0.32535 G loss: 2.34141 (0.045 sec/batch, 1436.413 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:01,812] [train step95321] D loss: 0.32538 G loss: 2.26931 (0.037 sec/batch, 1737.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:02,186] [train step95330] D loss: 0.32543 G loss: 2.25088 (0.038 sec/batch, 1702.353 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:02,568] [train step95340] D loss: 0.32542 G loss: 2.35270 (0.036 sec/batch, 1762.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:02,937] [train step95351] D loss: 0.32524 G loss: 2.31662 (0.035 sec/batch, 1827.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:03,325] [train step95361] D loss: 0.32537 G loss: 2.25317 (0.038 sec/batch, 1671.579 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:03,699] [train step95370] D loss: 0.32553 G loss: 2.37281 (0.036 sec/batch, 1770.753 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:04,079] [train step95380] D loss: 0.32524 G loss: 2.28650 (0.038 sec/batch, 1691.945 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:04,478] [train step95390] D loss: 0.32538 G loss: 2.25513 (0.043 sec/batch, 1491.963 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:04,853] [train step95400] D loss: 0.32533 G loss: 2.33819 (0.034 sec/batch, 1892.229 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:05,236] [train step95411] D loss: 0.32525 G loss: 2.30500 (0.035 sec/batch, 1848.755 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:05,637] [train step95421] D loss: 0.32540 G loss: 2.35344 (0.041 sec/batch, 1577.464 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:06,015] [train step95430] D loss: 0.32528 G loss: 2.27185 (0.036 sec/batch, 1780.218 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:06,411] [train step95441] D loss: 0.32532 G loss: 2.26502 (0.038 sec/batch, 1666.494 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:06,792] [train step95450] D loss: 0.32535 G loss: 2.26879 (0.041 sec/batch, 1578.011 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:07,176] [train step95460] D loss: 0.32540 G loss: 2.33957 (0.039 sec/batch, 1646.773 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:07,566] [train step95471] D loss: 0.32527 G loss: 2.31732 (0.034 sec/batch, 1890.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:07,958] [train step95481] D loss: 0.32528 G loss: 2.30620 (0.044 sec/batch, 1441.628 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:08,351] [train step95490] D loss: 0.32530 G loss: 2.28910 (0.040 sec/batch, 1590.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:08,745] [train step95500] D loss: 0.32537 G loss: 2.26992 (0.040 sec/batch, 1584.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:09,134] [train step95511] D loss: 0.32535 G loss: 2.27202 (0.036 sec/batch, 1767.709 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:09,520] [train step95520] D loss: 0.32525 G loss: 2.31730 (0.034 sec/batch, 1874.575 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:09,907] [train step95531] D loss: 0.32524 G loss: 2.30263 (0.039 sec/batch, 1640.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:10,286] [train step95540] D loss: 0.32541 G loss: 2.25734 (0.037 sec/batch, 1743.518 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:10,681] [train step95550] D loss: 0.32524 G loss: 2.31826 (0.036 sec/batch, 1775.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:11,067] [train step95560] D loss: 0.32533 G loss: 2.27168 (0.038 sec/batch, 1702.849 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:11,458] [train step95571] D loss: 0.32579 G loss: 2.20979 (0.043 sec/batch, 1496.571 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:11,849] [train step95580] D loss: 0.32610 G loss: 2.42595 (0.037 sec/batch, 1752.476 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:12,228] [train step95591] D loss: 0.32687 G loss: 2.47687 (0.036 sec/batch, 1786.105 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:12,625] [train step95600] D loss: 0.32538 G loss: 2.34624 (0.042 sec/batch, 1536.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:13,006] [train step95610] D loss: 0.32533 G loss: 2.33649 (0.038 sec/batch, 1679.422 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:13,392] [train step95621] D loss: 0.32596 G loss: 2.41007 (0.039 sec/batch, 1647.551 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:13,786] [train step95631] D loss: 0.32528 G loss: 2.31452 (0.038 sec/batch, 1678.928 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:14,187] [train step95640] D loss: 0.32523 G loss: 2.29521 (0.036 sec/batch, 1777.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:14,589] [train step95651] D loss: 0.32547 G loss: 2.24368 (0.047 sec/batch, 1349.125 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:14,980] [train step95660] D loss: 0.32572 G loss: 2.21265 (0.037 sec/batch, 1743.450 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:15,366] [train step95670] D loss: 0.32614 G loss: 2.43130 (0.036 sec/batch, 1798.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:15,761] [train step95681] D loss: 0.32605 G loss: 2.42328 (0.036 sec/batch, 1788.068 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:16,144] [train step95691] D loss: 0.32582 G loss: 2.40215 (0.038 sec/batch, 1688.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:16,541] [train step95700] D loss: 0.32634 G loss: 2.16873 (0.042 sec/batch, 1529.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:16,934] [train step95711] D loss: 0.32605 G loss: 2.19233 (0.037 sec/batch, 1743.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:17,322] [train step95720] D loss: 0.32546 G loss: 2.24834 (0.040 sec/batch, 1587.446 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:17,709] [train step95730] D loss: 0.32551 G loss: 2.36295 (0.037 sec/batch, 1736.615 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:18,094] [train step95741] D loss: 0.32522 G loss: 2.29839 (0.039 sec/batch, 1659.426 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:18,481] [train step95751] D loss: 0.32574 G loss: 2.21853 (0.041 sec/batch, 1546.198 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:18,877] [train step95760] D loss: 0.32706 G loss: 2.48480 (0.041 sec/batch, 1573.746 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:19,269] [train step95771] D loss: 0.32616 G loss: 2.42762 (0.039 sec/batch, 1650.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:19,664] [train step95781] D loss: 0.32538 G loss: 2.35028 (0.039 sec/batch, 1646.853 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:20,057] [train step95790] D loss: 0.32539 G loss: 2.28872 (0.037 sec/batch, 1730.971 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:20,440] [train step95800] D loss: 0.32534 G loss: 2.33298 (0.045 sec/batch, 1423.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:20,835] [train step95811] D loss: 0.32557 G loss: 2.36650 (0.042 sec/batch, 1509.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:21,224] [train step95820] D loss: 0.32537 G loss: 2.26232 (0.039 sec/batch, 1658.381 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:21,607] [train step95830] D loss: 0.32536 G loss: 2.27201 (0.032 sec/batch, 1983.005 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:22,007] [train step95840] D loss: 0.32544 G loss: 2.24515 (0.038 sec/batch, 1691.209 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:22,398] [train step95850] D loss: 0.32533 G loss: 2.33774 (0.038 sec/batch, 1700.196 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:22,784] [train step95860] D loss: 0.32529 G loss: 2.30259 (0.043 sec/batch, 1485.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:23,185] [train step95871] D loss: 0.32531 G loss: 2.31963 (0.038 sec/batch, 1684.269 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:23,579] [train step95880] D loss: 0.32526 G loss: 2.30011 (0.039 sec/batch, 1624.902 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:23,979] [train step95891] D loss: 0.32541 G loss: 2.35475 (0.038 sec/batch, 1673.058 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:24,370] [train step95901] D loss: 0.32544 G loss: 2.35903 (0.037 sec/batch, 1729.164 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:24,764] [train step95910] D loss: 0.32585 G loss: 2.20562 (0.036 sec/batch, 1768.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:25,170] [train step95920] D loss: 0.32613 G loss: 2.18098 (0.039 sec/batch, 1637.810 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:25,564] [train step95931] D loss: 0.32581 G loss: 2.21218 (0.036 sec/batch, 1766.232 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:25,969] [train step95940] D loss: 0.32532 G loss: 2.33903 (0.038 sec/batch, 1682.274 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:26,366] [train step95950] D loss: 0.32546 G loss: 2.24318 (0.036 sec/batch, 1764.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:26,757] [train step95961] D loss: 0.32530 G loss: 2.30100 (0.038 sec/batch, 1686.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:27,149] [train step95970] D loss: 0.32529 G loss: 2.32394 (0.037 sec/batch, 1720.663 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:27,544] [train step95980] D loss: 0.32530 G loss: 2.29069 (0.037 sec/batch, 1707.844 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:27,946] [train step95991] D loss: 0.32533 G loss: 2.29043 (0.044 sec/batch, 1460.341 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:28,346] [train step96000] D loss: 0.32527 G loss: 2.30532 (0.039 sec/batch, 1643.395 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:11:28,347] Saved checkpoint at 96000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:28,973] [train step96010] D loss: 0.32525 G loss: 2.29451 (0.053 sec/batch, 1204.432 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:29,362] [train step96020] D loss: 0.32530 G loss: 2.27656 (0.037 sec/batch, 1735.716 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:29,755] [train step96030] D loss: 0.32524 G loss: 2.29808 (0.044 sec/batch, 1455.976 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:30,151] [train step96040] D loss: 0.32534 G loss: 2.26746 (0.047 sec/batch, 1363.267 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:30,543] [train step96051] D loss: 0.32548 G loss: 2.24665 (0.037 sec/batch, 1716.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:30,934] [train step96060] D loss: 0.32529 G loss: 2.33140 (0.045 sec/batch, 1425.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:31,321] [train step96070] D loss: 0.32535 G loss: 2.34559 (0.035 sec/batch, 1814.342 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:31,704] [train step96081] D loss: 0.32529 G loss: 2.29925 (0.039 sec/batch, 1642.169 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:32,094] [train step96090] D loss: 0.32537 G loss: 2.34342 (0.040 sec/batch, 1617.471 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:32,464] [train step96100] D loss: 0.32567 G loss: 2.38898 (0.033 sec/batch, 1957.240 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:32,855] [train step96111] D loss: 0.32587 G loss: 2.40475 (0.037 sec/batch, 1725.319 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:33,245] [train step96120] D loss: 0.32576 G loss: 2.20586 (0.039 sec/batch, 1622.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:33,624] [train step96131] D loss: 0.32534 G loss: 2.27384 (0.035 sec/batch, 1813.557 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:34,002] [train step96141] D loss: 0.32527 G loss: 2.29187 (0.040 sec/batch, 1615.592 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:34,388] [train step96150] D loss: 0.32525 G loss: 2.29152 (0.038 sec/batch, 1697.594 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:34,774] [train step96160] D loss: 0.32522 G loss: 2.30551 (0.044 sec/batch, 1468.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:35,163] [train step96171] D loss: 0.32525 G loss: 2.31714 (0.048 sec/batch, 1321.470 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:35,539] [train step96180] D loss: 0.32536 G loss: 2.25932 (0.040 sec/batch, 1588.404 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:35,915] [train step96191] D loss: 0.32557 G loss: 2.22601 (0.037 sec/batch, 1746.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:36,314] [train step96201] D loss: 0.32538 G loss: 2.35530 (0.037 sec/batch, 1717.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:36,690] [train step96210] D loss: 0.32535 G loss: 2.25947 (0.040 sec/batch, 1612.961 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:37,071] [train step96221] D loss: 0.32526 G loss: 2.33373 (0.036 sec/batch, 1794.247 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:37,456] [train step96231] D loss: 0.32523 G loss: 2.30856 (0.039 sec/batch, 1650.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:37,827] [train step96240] D loss: 0.32546 G loss: 2.35925 (0.036 sec/batch, 1755.157 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:38,207] [train step96251] D loss: 0.32565 G loss: 2.38834 (0.037 sec/batch, 1710.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:38,604] [train step96260] D loss: 0.32530 G loss: 2.28756 (0.036 sec/batch, 1778.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:38,988] [train step96270] D loss: 0.32523 G loss: 2.31885 (0.039 sec/batch, 1640.172 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:39,384] [train step96280] D loss: 0.32534 G loss: 2.27385 (0.040 sec/batch, 1607.947 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:39,771] [train step96291] D loss: 0.32527 G loss: 2.30274 (0.043 sec/batch, 1504.498 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:40,150] [train step96300] D loss: 0.32543 G loss: 2.25471 (0.040 sec/batch, 1618.417 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:40,544] [train step96311] D loss: 0.32569 G loss: 2.21745 (0.037 sec/batch, 1710.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:40,938] [train step96321] D loss: 0.32530 G loss: 2.27530 (0.037 sec/batch, 1711.917 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:41,329] [train step96330] D loss: 0.32522 G loss: 2.28807 (0.046 sec/batch, 1399.924 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:41,708] [train step96341] D loss: 0.32526 G loss: 2.27270 (0.035 sec/batch, 1841.563 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:42,092] [train step96350] D loss: 0.32535 G loss: 2.26336 (0.043 sec/batch, 1495.912 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:42,484] [train step96360] D loss: 0.32553 G loss: 2.37366 (0.035 sec/batch, 1836.599 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:42,866] [train step96371] D loss: 0.32539 G loss: 2.35503 (0.043 sec/batch, 1479.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:43,256] [train step96380] D loss: 0.32529 G loss: 2.32869 (0.039 sec/batch, 1652.683 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:43,641] [train step96390] D loss: 0.32522 G loss: 2.30693 (0.034 sec/batch, 1888.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:44,037] [train step96401] D loss: 0.32524 G loss: 2.27342 (0.036 sec/batch, 1767.313 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:44,434] [train step96411] D loss: 0.32528 G loss: 2.33876 (0.038 sec/batch, 1701.975 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:44,818] [train step96420] D loss: 0.32526 G loss: 2.29074 (0.036 sec/batch, 1766.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:45,211] [train step96431] D loss: 0.32525 G loss: 2.28258 (0.037 sec/batch, 1740.657 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:45,604] [train step96440] D loss: 0.32529 G loss: 2.31759 (0.039 sec/batch, 1625.453 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:45,986] [train step96450] D loss: 0.32522 G loss: 2.29925 (0.033 sec/batch, 1964.459 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:46,380] [train step96460] D loss: 0.32532 G loss: 2.27010 (0.039 sec/batch, 1652.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:46,782] [train step96470] D loss: 0.32534 G loss: 2.25367 (0.046 sec/batch, 1392.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:47,157] [train step96480] D loss: 0.32528 G loss: 2.32882 (0.036 sec/batch, 1763.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:47,558] [train step96491] D loss: 0.32528 G loss: 2.32275 (0.036 sec/batch, 1768.781 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:47,944] [train step96501] D loss: 0.32533 G loss: 2.34343 (0.035 sec/batch, 1840.970 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:48,333] [train step96510] D loss: 0.32526 G loss: 2.30655 (0.036 sec/batch, 1799.817 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:48,737] [train step96520] D loss: 0.32547 G loss: 2.36768 (0.042 sec/batch, 1527.449 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:49,132] [train step96530] D loss: 0.32530 G loss: 2.32065 (0.038 sec/batch, 1693.995 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:49,526] [train step96540] D loss: 0.32533 G loss: 2.33930 (0.044 sec/batch, 1438.245 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:49,914] [train step96550] D loss: 0.32531 G loss: 2.33160 (0.036 sec/batch, 1783.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:50,310] [train step96560] D loss: 0.32525 G loss: 2.28776 (0.038 sec/batch, 1702.698 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:50,711] [train step96570] D loss: 0.32562 G loss: 2.38802 (0.037 sec/batch, 1708.931 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:51,102] [train step96581] D loss: 0.32554 G loss: 2.37674 (0.037 sec/batch, 1752.727 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:51,488] [train step96590] D loss: 0.32524 G loss: 2.28970 (0.037 sec/batch, 1731.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:51,887] [train step96600] D loss: 0.32523 G loss: 2.28498 (0.039 sec/batch, 1654.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:52,264] [train step96610] D loss: 0.32525 G loss: 2.32788 (0.029 sec/batch, 2175.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:52,665] [train step96620] D loss: 0.32559 G loss: 2.38251 (0.044 sec/batch, 1451.026 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:53,053] [train step96630] D loss: 0.32542 G loss: 2.24637 (0.034 sec/batch, 1861.020 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:53,443] [train step96641] D loss: 0.32536 G loss: 2.34870 (0.040 sec/batch, 1605.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:53,842] [train step96651] D loss: 0.32524 G loss: 2.28911 (0.037 sec/batch, 1747.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:54,232] [train step96660] D loss: 0.32532 G loss: 2.33803 (0.038 sec/batch, 1676.276 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:54,617] [train step96671] D loss: 0.32523 G loss: 2.30718 (0.039 sec/batch, 1644.533 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:55,017] [train step96681] D loss: 0.32527 G loss: 2.27733 (0.038 sec/batch, 1684.851 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:55,415] [train step96690] D loss: 0.32527 G loss: 2.28608 (0.029 sec/batch, 2194.427 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:55,820] [train step96700] D loss: 0.32547 G loss: 2.23680 (0.034 sec/batch, 1876.882 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:56,205] [train step96710] D loss: 0.32527 G loss: 2.33212 (0.038 sec/batch, 1682.517 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:56,591] [train step96720] D loss: 0.32545 G loss: 2.24565 (0.037 sec/batch, 1746.831 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:57,008] [train step96730] D loss: 0.32527 G loss: 2.32236 (0.045 sec/batch, 1410.272 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:57,392] [train step96741] D loss: 0.32540 G loss: 2.35846 (0.032 sec/batch, 1970.588 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:57,791] [train step96750] D loss: 0.32528 G loss: 2.27056 (0.043 sec/batch, 1482.160 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:58,183] [train step96760] D loss: 0.32526 G loss: 2.28293 (0.040 sec/batch, 1604.017 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:58,564] [train step96770] D loss: 0.32561 G loss: 2.22224 (0.036 sec/batch, 1777.471 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:58,949] [train step96780] D loss: 0.32587 G loss: 2.40737 (0.033 sec/batch, 1928.152 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:59,347] [train step96790] D loss: 0.32525 G loss: 2.28918 (0.040 sec/batch, 1615.398 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:11:59,734] [train step96801] D loss: 0.32522 G loss: 2.30384 (0.035 sec/batch, 1818.262 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:00,133] [train step96810] D loss: 0.32525 G loss: 2.28888 (0.042 sec/batch, 1528.406 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:00,519] [train step96821] D loss: 0.32521 G loss: 2.31628 (0.036 sec/batch, 1753.632 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:00,929] [train step96831] D loss: 0.32541 G loss: 2.36395 (0.050 sec/batch, 1279.544 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:01,318] [train step96840] D loss: 0.32524 G loss: 2.28696 (0.038 sec/batch, 1667.011 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:01,697] [train step96851] D loss: 0.32545 G loss: 2.36202 (0.036 sec/batch, 1761.792 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:02,087] [train step96860] D loss: 0.32523 G loss: 2.28410 (0.037 sec/batch, 1730.547 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:02,463] [train step96870] D loss: 0.32529 G loss: 2.33814 (0.040 sec/batch, 1582.047 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:02,836] [train step96881] D loss: 0.32533 G loss: 2.33139 (0.036 sec/batch, 1785.891 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:03,218] [train step96891] D loss: 0.32524 G loss: 2.32301 (0.035 sec/batch, 1829.315 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:03,592] [train step96900] D loss: 0.32542 G loss: 2.24925 (0.036 sec/batch, 1786.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:03,981] [train step96910] D loss: 0.32541 G loss: 2.24850 (0.040 sec/batch, 1604.899 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:04,366] [train step96921] D loss: 0.32528 G loss: 2.31290 (0.036 sec/batch, 1768.023 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:04,737] [train step96930] D loss: 0.32529 G loss: 2.27525 (0.035 sec/batch, 1850.309 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:05,119] [train step96941] D loss: 0.32565 G loss: 2.21832 (0.037 sec/batch, 1749.586 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:05,498] [train step96951] D loss: 0.32539 G loss: 2.24530 (0.036 sec/batch, 1799.310 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:05,878] [train step96960] D loss: 0.32546 G loss: 2.36686 (0.037 sec/batch, 1748.173 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:06,272] [train step96970] D loss: 0.32544 G loss: 2.36445 (0.038 sec/batch, 1681.073 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:06,650] [train step96981] D loss: 0.32529 G loss: 2.33731 (0.036 sec/batch, 1770.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:07,031] [train step96990] D loss: 0.32521 G loss: 2.31057 (0.044 sec/batch, 1451.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:07,424] [train step97001] D loss: 0.32532 G loss: 2.33887 (0.035 sec/batch, 1849.863 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:12:07,425] Saved checkpoint at 97000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:08,025] [train step97010] D loss: 0.32533 G loss: 2.33051 (0.040 sec/batch, 1594.556 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:08,414] [train step97020] D loss: 0.32528 G loss: 2.26501 (0.037 sec/batch, 1750.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:08,790] [train step97031] D loss: 0.32555 G loss: 2.22708 (0.037 sec/batch, 1718.768 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:09,192] [train step97041] D loss: 0.32539 G loss: 2.24990 (0.037 sec/batch, 1711.426 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:09,572] [train step97050] D loss: 0.32524 G loss: 2.29597 (0.034 sec/batch, 1879.655 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:09,968] [train step97061] D loss: 0.32542 G loss: 2.24828 (0.037 sec/batch, 1719.131 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:10,362] [train step97070] D loss: 0.32520 G loss: 2.29190 (0.039 sec/batch, 1625.424 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:10,746] [train step97080] D loss: 0.32526 G loss: 2.29451 (0.037 sec/batch, 1734.202 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:11,142] [train step97091] D loss: 0.32526 G loss: 2.31501 (0.045 sec/batch, 1431.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:11,525] [train step97101] D loss: 0.32522 G loss: 2.32152 (0.037 sec/batch, 1742.850 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:11,914] [train step97110] D loss: 0.32523 G loss: 2.28782 (0.037 sec/batch, 1722.639 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:12,304] [train step97120] D loss: 0.32533 G loss: 2.34682 (0.038 sec/batch, 1701.187 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:12,698] [train step97131] D loss: 0.32521 G loss: 2.30387 (0.039 sec/batch, 1646.874 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:13,083] [train step97140] D loss: 0.32520 G loss: 2.30195 (0.035 sec/batch, 1848.500 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:13,486] [train step97150] D loss: 0.32521 G loss: 2.29742 (0.041 sec/batch, 1552.126 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:13,875] [train step97160] D loss: 0.32536 G loss: 2.34561 (0.039 sec/batch, 1654.925 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:14,260] [train step97170] D loss: 0.32522 G loss: 2.29130 (0.036 sec/batch, 1764.421 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:14,654] [train step97180] D loss: 0.32531 G loss: 2.33760 (0.038 sec/batch, 1705.413 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:15,048] [train step97190] D loss: 0.32530 G loss: 2.33850 (0.039 sec/batch, 1660.227 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:15,443] [train step97200] D loss: 0.32526 G loss: 2.28082 (0.040 sec/batch, 1583.316 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:15,822] [train step97211] D loss: 0.32527 G loss: 2.34274 (0.037 sec/batch, 1721.159 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:16,215] [train step97220] D loss: 0.32526 G loss: 2.32448 (0.044 sec/batch, 1461.629 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:16,609] [train step97230] D loss: 0.32523 G loss: 2.28999 (0.033 sec/batch, 1951.321 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:17,000] [train step97240] D loss: 0.32541 G loss: 2.36480 (0.035 sec/batch, 1845.691 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:17,404] [train step97251] D loss: 0.32543 G loss: 2.36154 (0.037 sec/batch, 1714.366 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:17,792] [train step97260] D loss: 0.32524 G loss: 2.28755 (0.041 sec/batch, 1573.525 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:18,182] [train step97271] D loss: 0.32522 G loss: 2.31848 (0.040 sec/batch, 1588.836 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:18,579] [train step97281] D loss: 0.32546 G loss: 2.36599 (0.035 sec/batch, 1835.180 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:18,964] [train step97290] D loss: 0.32545 G loss: 2.24045 (0.038 sec/batch, 1667.757 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:19,361] [train step97300] D loss: 0.32539 G loss: 2.24475 (0.047 sec/batch, 1372.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:19,762] [train step97310] D loss: 0.32536 G loss: 2.25826 (0.042 sec/batch, 1533.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:20,144] [train step97320] D loss: 0.32526 G loss: 2.32415 (0.039 sec/batch, 1624.814 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:20,544] [train step97331] D loss: 0.32531 G loss: 2.26269 (0.039 sec/batch, 1630.162 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:20,929] [train step97340] D loss: 0.32528 G loss: 2.26845 (0.038 sec/batch, 1696.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:21,315] [train step97350] D loss: 0.32533 G loss: 2.34549 (0.037 sec/batch, 1717.701 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:21,705] [train step97360] D loss: 0.32523 G loss: 2.28864 (0.034 sec/batch, 1901.774 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:22,093] [train step97371] D loss: 0.32526 G loss: 2.26933 (0.039 sec/batch, 1636.971 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:22,499] [train step97380] D loss: 0.32539 G loss: 2.35249 (0.045 sec/batch, 1423.079 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:22,886] [train step97391] D loss: 0.32526 G loss: 2.33270 (0.036 sec/batch, 1786.069 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:23,269] [train step97401] D loss: 0.32528 G loss: 2.33491 (0.035 sec/batch, 1808.621 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:23,674] [train step97410] D loss: 0.32526 G loss: 2.30607 (0.038 sec/batch, 1691.635 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:24,057] [train step97421] D loss: 0.32541 G loss: 2.35303 (0.037 sec/batch, 1735.559 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:24,451] [train step97431] D loss: 0.32534 G loss: 2.35296 (0.035 sec/batch, 1823.722 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:24,849] [train step97440] D loss: 0.32525 G loss: 2.28656 (0.039 sec/batch, 1622.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:25,249] [train step97450] D loss: 0.32532 G loss: 2.34194 (0.044 sec/batch, 1466.668 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:25,654] [train step97460] D loss: 0.32533 G loss: 2.34469 (0.039 sec/batch, 1631.013 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:26,051] [train step97470] D loss: 0.32539 G loss: 2.24602 (0.039 sec/batch, 1655.129 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:26,441] [train step97480] D loss: 0.32549 G loss: 2.23993 (0.038 sec/batch, 1695.835 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:26,836] [train step97491] D loss: 0.32526 G loss: 2.32772 (0.037 sec/batch, 1739.394 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:27,225] [train step97500] D loss: 0.32526 G loss: 2.28925 (0.036 sec/batch, 1793.444 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:27,629] [train step97511] D loss: 0.32521 G loss: 2.28270 (0.048 sec/batch, 1334.146 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:28,011] [train step97521] D loss: 0.32532 G loss: 2.33519 (0.037 sec/batch, 1733.620 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:28,401] [train step97530] D loss: 0.32537 G loss: 2.25182 (0.040 sec/batch, 1591.181 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:28,808] [train step97541] D loss: 0.32559 G loss: 2.22929 (0.046 sec/batch, 1401.657 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:29,196] [train step97550] D loss: 0.32529 G loss: 2.31827 (0.038 sec/batch, 1699.604 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:29,582] [train step97560] D loss: 0.32524 G loss: 2.29659 (0.038 sec/batch, 1703.595 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:29,987] [train step97571] D loss: 0.32542 G loss: 2.35718 (0.039 sec/batch, 1659.037 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:30,373] [train step97581] D loss: 0.32523 G loss: 2.31265 (0.033 sec/batch, 1939.352 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:30,771] [train step97590] D loss: 0.32522 G loss: 2.30941 (0.038 sec/batch, 1668.223 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:31,164] [train step97600] D loss: 0.32541 G loss: 2.35090 (0.039 sec/batch, 1643.053 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:31,543] [train step97610] D loss: 0.32532 G loss: 2.26848 (0.029 sec/batch, 2230.531 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:31,944] [train step97620] D loss: 0.32553 G loss: 2.36729 (0.036 sec/batch, 1762.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:32,327] [train step97631] D loss: 0.32531 G loss: 2.34139 (0.040 sec/batch, 1594.376 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:32,708] [train step97641] D loss: 0.32544 G loss: 2.35065 (0.035 sec/batch, 1835.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:33,091] [train step97650] D loss: 0.32542 G loss: 2.24517 (0.037 sec/batch, 1736.502 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:33,476] [train step97660] D loss: 0.32532 G loss: 2.26445 (0.041 sec/batch, 1545.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:33,857] [train step97671] D loss: 0.32524 G loss: 2.31787 (0.034 sec/batch, 1894.567 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:34,254] [train step97680] D loss: 0.32536 G loss: 2.35153 (0.043 sec/batch, 1495.312 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:34,629] [train step97690] D loss: 0.32530 G loss: 2.34253 (0.034 sec/batch, 1883.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:35,029] [train step97701] D loss: 0.32524 G loss: 2.30436 (0.037 sec/batch, 1733.172 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:35,411] [train step97710] D loss: 0.32524 G loss: 2.31467 (0.038 sec/batch, 1662.829 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:35,788] [train step97720] D loss: 0.32526 G loss: 2.32929 (0.035 sec/batch, 1851.075 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:36,181] [train step97730] D loss: 0.32532 G loss: 2.33930 (0.038 sec/batch, 1698.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:36,561] [train step97740] D loss: 0.32523 G loss: 2.30979 (0.038 sec/batch, 1677.323 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:36,942] [train step97750] D loss: 0.32526 G loss: 2.28375 (0.037 sec/batch, 1731.071 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:37,326] [train step97760] D loss: 0.32534 G loss: 2.28598 (0.037 sec/batch, 1733.497 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:37,712] [train step97770] D loss: 0.32557 G loss: 2.37781 (0.034 sec/batch, 1866.884 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:38,108] [train step97781] D loss: 0.32588 G loss: 2.40501 (0.038 sec/batch, 1684.428 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:38,503] [train step97790] D loss: 0.32549 G loss: 2.35433 (0.040 sec/batch, 1603.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:38,882] [train step97800] D loss: 0.32528 G loss: 2.28220 (0.036 sec/batch, 1754.904 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:39,268] [train step97810] D loss: 0.32530 G loss: 2.29207 (0.034 sec/batch, 1897.178 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:39,662] [train step97821] D loss: 0.32524 G loss: 2.28810 (0.036 sec/batch, 1800.952 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:40,052] [train step97830] D loss: 0.32534 G loss: 2.33687 (0.033 sec/batch, 1923.979 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:40,448] [train step97840] D loss: 0.32529 G loss: 2.27741 (0.038 sec/batch, 1692.713 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:40,830] [train step97850] D loss: 0.32521 G loss: 2.29793 (0.042 sec/batch, 1519.865 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:41,224] [train step97860] D loss: 0.32526 G loss: 2.29303 (0.041 sec/batch, 1563.717 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:41,626] [train step97871] D loss: 0.32526 G loss: 2.27999 (0.040 sec/batch, 1595.560 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:42,023] [train step97880] D loss: 0.32571 G loss: 2.21706 (0.045 sec/batch, 1421.580 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:42,416] [train step97890] D loss: 0.32574 G loss: 2.39038 (0.038 sec/batch, 1702.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:42,803] [train step97901] D loss: 0.32536 G loss: 2.34069 (0.036 sec/batch, 1754.882 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:43,196] [train step97911] D loss: 0.32525 G loss: 2.30737 (0.039 sec/batch, 1625.296 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:43,579] [train step97920] D loss: 0.32524 G loss: 2.29710 (0.038 sec/batch, 1685.761 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:43,968] [train step97930] D loss: 0.32523 G loss: 2.29989 (0.047 sec/batch, 1366.640 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:44,362] [train step97940] D loss: 0.32523 G loss: 2.32341 (0.029 sec/batch, 2191.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:44,758] [train step97950] D loss: 0.32533 G loss: 2.26070 (0.034 sec/batch, 1901.384 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:45,160] [train step97960] D loss: 0.32527 G loss: 2.28362 (0.043 sec/batch, 1483.479 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:45,546] [train step97971] D loss: 0.32531 G loss: 2.26925 (0.037 sec/batch, 1717.624 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:45,933] [train step97980] D loss: 0.32530 G loss: 2.32261 (0.040 sec/batch, 1599.811 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:46,335] [train step97990] D loss: 0.32538 G loss: 2.33850 (0.041 sec/batch, 1570.744 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:46,720] [train step98001] D loss: 0.32526 G loss: 2.29256 (0.035 sec/batch, 1811.611 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:12:46,721] Saved checkpoint at 98000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:47,340] [train step98010] D loss: 0.32531 G loss: 2.32902 (0.028 sec/batch, 2262.777 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:47,747] [train step98020] D loss: 0.32534 G loss: 2.34067 (0.040 sec/batch, 1586.639 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:48,147] [train step98030] D loss: 0.32533 G loss: 2.26459 (0.037 sec/batch, 1711.339 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:48,555] [train step98040] D loss: 0.32525 G loss: 2.30762 (0.037 sec/batch, 1746.819 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:48,945] [train step98050] D loss: 0.32524 G loss: 2.31491 (0.036 sec/batch, 1774.229 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:49,355] [train step98060] D loss: 0.32523 G loss: 2.30111 (0.039 sec/batch, 1654.486 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:49,758] [train step98070] D loss: 0.32535 G loss: 2.26132 (0.042 sec/batch, 1517.673 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:50,158] [train step98080] D loss: 0.32524 G loss: 2.29800 (0.039 sec/batch, 1652.297 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:50,551] [train step98091] D loss: 0.32612 G loss: 2.43259 (0.036 sec/batch, 1780.619 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:50,943] [train step98100] D loss: 0.32625 G loss: 2.17689 (0.038 sec/batch, 1663.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:51,352] [train step98110] D loss: 0.32573 G loss: 2.21187 (0.051 sec/batch, 1254.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:51,743] [train step98120] D loss: 0.32535 G loss: 2.34197 (0.042 sec/batch, 1516.096 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:52,143] [train step98130] D loss: 0.32542 G loss: 2.26529 (0.040 sec/batch, 1580.324 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:52,554] [train step98141] D loss: 0.32549 G loss: 2.24465 (0.041 sec/batch, 1549.179 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:52,955] [train step98151] D loss: 0.32530 G loss: 2.33256 (0.041 sec/batch, 1555.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:53,353] [train step98160] D loss: 0.32539 G loss: 2.25692 (0.046 sec/batch, 1390.051 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:53,740] [train step98170] D loss: 0.32524 G loss: 2.28298 (0.036 sec/batch, 1771.793 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:54,131] [train step98180] D loss: 0.32527 G loss: 2.33159 (0.033 sec/batch, 1946.680 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:54,559] [train step98190] D loss: 0.32521 G loss: 2.29159 (0.041 sec/batch, 1557.077 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:54,956] [train step98201] D loss: 0.32535 G loss: 2.32656 (0.039 sec/batch, 1629.360 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:55,346] [train step98210] D loss: 0.32565 G loss: 2.38784 (0.043 sec/batch, 1476.380 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:55,755] [train step98220] D loss: 0.32559 G loss: 2.22182 (0.039 sec/batch, 1621.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:56,170] [train step98231] D loss: 0.32570 G loss: 2.22700 (0.039 sec/batch, 1660.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:56,570] [train step98240] D loss: 0.32685 G loss: 2.14836 (0.038 sec/batch, 1672.870 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:56,960] [train step98250] D loss: 0.32613 G loss: 2.42516 (0.039 sec/batch, 1638.840 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:57,346] [train step98260] D loss: 0.32529 G loss: 2.31608 (0.037 sec/batch, 1738.065 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:57,754] [train step98271] D loss: 0.32558 G loss: 2.22600 (0.038 sec/batch, 1700.756 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:58,148] [train step98280] D loss: 0.32600 G loss: 2.40367 (0.042 sec/batch, 1535.672 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:58,549] [train step98290] D loss: 0.32607 G loss: 2.42025 (0.041 sec/batch, 1568.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:58,931] [train step98301] D loss: 0.32557 G loss: 2.38484 (0.038 sec/batch, 1671.256 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:59,315] [train step98310] D loss: 0.32537 G loss: 2.35026 (0.037 sec/batch, 1745.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:12:59,714] [train step98320] D loss: 0.32598 G loss: 2.42184 (0.039 sec/batch, 1644.392 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:00,093] [train step98330] D loss: 0.32606 G loss: 2.41600 (0.032 sec/batch, 1991.361 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:00,488] [train step98340] D loss: 0.32581 G loss: 2.20265 (0.034 sec/batch, 1856.837 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:00,890] [train step98351] D loss: 0.32525 G loss: 2.29730 (0.040 sec/batch, 1596.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:01,276] [train step98361] D loss: 0.32530 G loss: 2.30388 (0.045 sec/batch, 1432.573 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:01,680] [train step98370] D loss: 0.32524 G loss: 2.29878 (0.041 sec/batch, 1569.587 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:02,065] [train step98381] D loss: 0.32521 G loss: 2.31298 (0.040 sec/batch, 1615.660 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:02,456] [train step98390] D loss: 0.32525 G loss: 2.31671 (0.043 sec/batch, 1492.776 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:02,853] [train step98400] D loss: 0.32531 G loss: 2.26216 (0.041 sec/batch, 1542.334 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:03,229] [train step98411] D loss: 0.32524 G loss: 2.29796 (0.037 sec/batch, 1728.363 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:03,598] [train step98421] D loss: 0.32529 G loss: 2.27945 (0.037 sec/batch, 1730.759 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:03,985] [train step98430] D loss: 0.32525 G loss: 2.32781 (0.033 sec/batch, 1921.968 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:04,365] [train step98440] D loss: 0.32569 G loss: 2.21877 (0.049 sec/batch, 1295.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:04,756] [train step98450] D loss: 0.32528 G loss: 2.28139 (0.038 sec/batch, 1680.032 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:05,138] [train step98460] D loss: 0.32523 G loss: 2.28985 (0.042 sec/batch, 1520.907 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:05,512] [train step98471] D loss: 0.32533 G loss: 2.32883 (0.037 sec/batch, 1744.889 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:05,894] [train step98481] D loss: 0.32532 G loss: 2.33710 (0.037 sec/batch, 1742.601 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:06,270] [train step98490] D loss: 0.32532 G loss: 2.31419 (0.039 sec/batch, 1635.186 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:06,651] [train step98500] D loss: 0.32573 G loss: 2.39589 (0.037 sec/batch, 1712.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:07,040] [train step98510] D loss: 0.32530 G loss: 2.31132 (0.040 sec/batch, 1615.543 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:07,422] [train step98520] D loss: 0.32526 G loss: 2.29134 (0.038 sec/batch, 1679.170 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:07,813] [train step98531] D loss: 0.32532 G loss: 2.26247 (0.038 sec/batch, 1699.346 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:08,181] [train step98541] D loss: 0.32597 G loss: 2.19684 (0.035 sec/batch, 1814.869 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:08,567] [train step98550] D loss: 0.32559 G loss: 2.38202 (0.038 sec/batch, 1691.998 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:08,965] [train step98560] D loss: 0.32524 G loss: 2.31471 (0.039 sec/batch, 1649.789 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:09,338] [train step98570] D loss: 0.32525 G loss: 2.28458 (0.032 sec/batch, 1989.354 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:09,720] [train step98580] D loss: 0.32553 G loss: 2.37442 (0.036 sec/batch, 1758.445 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:10,109] [train step98590] D loss: 0.32637 G loss: 2.44838 (0.037 sec/batch, 1718.625 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:10,499] [train step98600] D loss: 0.32606 G loss: 2.41137 (0.040 sec/batch, 1585.758 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:10,884] [train step98610] D loss: 0.32540 G loss: 2.24819 (0.037 sec/batch, 1712.911 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:11,267] [train step98621] D loss: 0.32525 G loss: 2.32266 (0.038 sec/batch, 1677.501 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:11,647] [train step98630] D loss: 0.32526 G loss: 2.32531 (0.041 sec/batch, 1567.003 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:12,066] [train step98640] D loss: 0.32528 G loss: 2.32327 (0.044 sec/batch, 1450.524 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:12,472] [train step98650] D loss: 0.32554 G loss: 2.36954 (0.045 sec/batch, 1409.909 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:12,867] [train step98661] D loss: 0.32543 G loss: 2.35969 (0.047 sec/batch, 1349.024 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:13,265] [train step98670] D loss: 0.32528 G loss: 2.28476 (0.042 sec/batch, 1537.642 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:13,656] [train step98681] D loss: 0.32534 G loss: 2.34407 (0.038 sec/batch, 1673.173 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:14,065] [train step98690] D loss: 0.32550 G loss: 2.36918 (0.040 sec/batch, 1616.137 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:14,458] [train step98700] D loss: 0.32587 G loss: 2.20659 (0.040 sec/batch, 1619.725 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:14,850] [train step98710] D loss: 0.32545 G loss: 2.25124 (0.039 sec/batch, 1655.466 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:15,260] [train step98720] D loss: 0.32527 G loss: 2.30233 (0.045 sec/batch, 1415.493 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:15,650] [train step98730] D loss: 0.32528 G loss: 2.29284 (0.038 sec/batch, 1684.333 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:16,056] [train step98741] D loss: 0.32536 G loss: 2.25848 (0.043 sec/batch, 1503.082 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:16,447] [train step98751] D loss: 0.32529 G loss: 2.27759 (0.037 sec/batch, 1712.114 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:16,845] [train step98760] D loss: 0.32528 G loss: 2.28307 (0.039 sec/batch, 1652.236 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:17,261] [train step98770] D loss: 0.32568 G loss: 2.21886 (0.039 sec/batch, 1641.958 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:17,660] [train step98781] D loss: 0.32541 G loss: 2.24451 (0.040 sec/batch, 1581.740 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:18,065] [train step98790] D loss: 0.32554 G loss: 2.37445 (0.045 sec/batch, 1420.767 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:18,462] [train step98800] D loss: 0.32520 G loss: 2.30205 (0.041 sec/batch, 1564.081 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:18,863] [train step98810] D loss: 0.32526 G loss: 2.30535 (0.039 sec/batch, 1635.087 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:19,263] [train step98820] D loss: 0.32524 G loss: 2.30254 (0.041 sec/batch, 1550.915 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:19,660] [train step98831] D loss: 0.32519 G loss: 2.29152 (0.042 sec/batch, 1514.377 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:20,058] [train step98840] D loss: 0.32526 G loss: 2.32475 (0.043 sec/batch, 1476.989 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:20,467] [train step98850] D loss: 0.32524 G loss: 2.30416 (0.040 sec/batch, 1590.418 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:20,862] [train step98860] D loss: 0.32526 G loss: 2.28263 (0.046 sec/batch, 1393.362 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:21,269] [train step98870] D loss: 0.32548 G loss: 2.24697 (0.040 sec/batch, 1593.675 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:21,662] [train step98880] D loss: 0.32529 G loss: 2.32265 (0.038 sec/batch, 1672.526 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:22,067] [train step98890] D loss: 0.32534 G loss: 2.25485 (0.042 sec/batch, 1515.240 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:22,470] [train step98901] D loss: 0.32524 G loss: 2.27857 (0.031 sec/batch, 2067.942 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:22,857] [train step98910] D loss: 0.32522 G loss: 2.30058 (0.040 sec/batch, 1615.602 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:23,253] [train step98921] D loss: 0.32533 G loss: 2.26062 (0.040 sec/batch, 1596.348 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:23,649] [train step98930] D loss: 0.32555 G loss: 2.23177 (0.044 sec/batch, 1446.327 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:24,037] [train step98940] D loss: 0.32586 G loss: 2.40724 (0.037 sec/batch, 1735.694 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:24,433] [train step98951] D loss: 0.32557 G loss: 2.36967 (0.041 sec/batch, 1579.004 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:24,821] [train step98960] D loss: 0.32544 G loss: 2.34935 (0.039 sec/batch, 1627.858 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:25,209] [train step98970] D loss: 0.32524 G loss: 2.28259 (0.039 sec/batch, 1643.496 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:25,609] [train step98980] D loss: 0.32525 G loss: 2.32339 (0.040 sec/batch, 1616.088 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:25,992] [train step98990] D loss: 0.32522 G loss: 2.30584 (0.036 sec/batch, 1787.246 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:26,389] [train step99000] D loss: 0.32527 G loss: 2.31937 (0.039 sec/batch, 1660.104 instances/sec)\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 05:13:26,389] Saved checkpoint at 99000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:26,990] [train step99010] D loss: 0.32522 G loss: 2.32073 (0.038 sec/batch, 1679.012 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:27,381] [train step99021] D loss: 0.32522 G loss: 2.30476 (0.038 sec/batch, 1680.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:27,774] [train step99030] D loss: 0.32545 G loss: 2.25198 (0.036 sec/batch, 1782.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:28,161] [train step99041] D loss: 0.32539 G loss: 2.25248 (0.038 sec/batch, 1696.864 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:28,552] [train step99051] D loss: 0.32527 G loss: 2.29427 (0.037 sec/batch, 1730.323 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:28,939] [train step99060] D loss: 0.32519 G loss: 2.31488 (0.041 sec/batch, 1579.943 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:29,328] [train step99071] D loss: 0.32526 G loss: 2.28863 (0.041 sec/batch, 1570.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:29,720] [train step99080] D loss: 0.32519 G loss: 2.30850 (0.035 sec/batch, 1823.821 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:30,112] [train step99090] D loss: 0.32524 G loss: 2.28938 (0.039 sec/batch, 1637.930 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:30,500] [train step99100] D loss: 0.32534 G loss: 2.25497 (0.037 sec/batch, 1749.951 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:30,891] [train step99111] D loss: 0.32524 G loss: 2.29912 (0.039 sec/batch, 1639.731 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:31,274] [train step99120] D loss: 0.32522 G loss: 2.29620 (0.037 sec/batch, 1733.799 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:31,682] [train step99131] D loss: 0.32520 G loss: 2.28417 (0.042 sec/batch, 1538.092 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:32,070] [train step99140] D loss: 0.32525 G loss: 2.32369 (0.037 sec/batch, 1739.507 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:32,454] [train step99150] D loss: 0.32525 G loss: 2.32724 (0.046 sec/batch, 1381.402 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:32,841] [train step99160] D loss: 0.32598 G loss: 2.41865 (0.040 sec/batch, 1604.084 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:33,221] [train step99170] D loss: 0.33014 G loss: 2.61392 (0.032 sec/batch, 2014.555 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:33,615] [train step99180] D loss: 0.32929 G loss: 2.05718 (0.042 sec/batch, 1531.222 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:33,993] [train step99190] D loss: 0.32677 G loss: 2.14531 (0.036 sec/batch, 1786.973 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:34,371] [train step99201] D loss: 0.32573 G loss: 2.21698 (0.035 sec/batch, 1836.184 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:34,762] [train step99210] D loss: 0.32527 G loss: 2.29880 (0.038 sec/batch, 1703.681 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:35,140] [train step99221] D loss: 0.32578 G loss: 2.21376 (0.036 sec/batch, 1761.110 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:35,524] [train step99231] D loss: 0.32590 G loss: 2.19689 (0.048 sec/batch, 1324.770 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:35,907] [train step99240] D loss: 0.32578 G loss: 2.39315 (0.048 sec/batch, 1341.527 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:36,288] [train step99251] D loss: 0.32555 G loss: 2.22932 (0.038 sec/batch, 1688.135 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:36,671] [train step99260] D loss: 0.32626 G loss: 2.17477 (0.038 sec/batch, 1690.314 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:37,058] [train step99270] D loss: 0.32614 G loss: 2.42868 (0.041 sec/batch, 1553.752 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:37,433] [train step99281] D loss: 0.32530 G loss: 2.27515 (0.032 sec/batch, 1981.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:37,819] [train step99290] D loss: 0.32523 G loss: 2.30013 (0.038 sec/batch, 1681.895 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:38,194] [train step99300] D loss: 0.32530 G loss: 2.29354 (0.035 sec/batch, 1838.939 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:38,582] [train step99311] D loss: 0.32528 G loss: 2.30373 (0.043 sec/batch, 1472.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:38,955] [train step99321] D loss: 0.32560 G loss: 2.37180 (0.037 sec/batch, 1744.367 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:39,329] [train step99330] D loss: 0.32535 G loss: 2.26037 (0.041 sec/batch, 1549.957 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:39,713] [train step99341] D loss: 0.32530 G loss: 2.27960 (0.037 sec/batch, 1739.710 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:40,101] [train step99351] D loss: 0.32528 G loss: 2.31825 (0.040 sec/batch, 1598.763 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:40,483] [train step99360] D loss: 0.32527 G loss: 2.30642 (0.034 sec/batch, 1903.136 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:40,874] [train step99371] D loss: 0.32526 G loss: 2.29417 (0.038 sec/batch, 1671.277 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:41,249] [train step99380] D loss: 0.32520 G loss: 2.30373 (0.039 sec/batch, 1650.702 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:41,638] [train step99390] D loss: 0.32528 G loss: 2.27596 (0.036 sec/batch, 1800.626 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:42,029] [train step99401] D loss: 0.32542 G loss: 2.25646 (0.034 sec/batch, 1910.042 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:42,410] [train step99410] D loss: 0.32536 G loss: 2.34110 (0.041 sec/batch, 1558.687 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:42,800] [train step99420] D loss: 0.32526 G loss: 2.27836 (0.038 sec/batch, 1675.700 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:43,181] [train step99431] D loss: 0.32554 G loss: 2.23686 (0.038 sec/batch, 1699.098 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:43,564] [train step99441] D loss: 0.32550 G loss: 2.23484 (0.038 sec/batch, 1706.259 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:43,953] [train step99450] D loss: 0.32530 G loss: 2.33468 (0.040 sec/batch, 1612.709 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:44,343] [train step99460] D loss: 0.32529 G loss: 2.29321 (0.037 sec/batch, 1706.671 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:44,717] [train step99471] D loss: 0.32529 G loss: 2.32957 (0.037 sec/batch, 1722.374 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:45,113] [train step99480] D loss: 0.32526 G loss: 2.27911 (0.035 sec/batch, 1819.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:45,501] [train step99491] D loss: 0.32532 G loss: 2.33278 (0.034 sec/batch, 1898.707 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:45,887] [train step99501] D loss: 0.32523 G loss: 2.29508 (0.035 sec/batch, 1836.725 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:46,272] [train step99510] D loss: 0.32526 G loss: 2.32904 (0.038 sec/batch, 1682.886 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:46,653] [train step99520] D loss: 0.32527 G loss: 2.28387 (0.038 sec/batch, 1697.938 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:47,048] [train step99531] D loss: 0.32555 G loss: 2.22999 (0.037 sec/batch, 1729.510 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:47,430] [train step99540] D loss: 0.32540 G loss: 2.36377 (0.037 sec/batch, 1728.318 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:47,829] [train step99551] D loss: 0.32528 G loss: 2.31662 (0.039 sec/batch, 1623.458 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:48,217] [train step99561] D loss: 0.32525 G loss: 2.29885 (0.035 sec/batch, 1841.412 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:48,604] [train step99570] D loss: 0.32550 G loss: 2.36946 (0.042 sec/batch, 1526.676 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:48,999] [train step99581] D loss: 0.32602 G loss: 2.41982 (0.035 sec/batch, 1814.010 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:49,380] [train step99591] D loss: 0.32532 G loss: 2.27448 (0.037 sec/batch, 1753.002 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:49,759] [train step99600] D loss: 0.32536 G loss: 2.34909 (0.034 sec/batch, 1910.355 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:50,158] [train step99610] D loss: 0.32520 G loss: 2.30957 (0.040 sec/batch, 1589.495 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:50,535] [train step99621] D loss: 0.32528 G loss: 2.30756 (0.037 sec/batch, 1742.918 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:50,927] [train step99630] D loss: 0.32524 G loss: 2.32748 (0.053 sec/batch, 1207.168 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:51,315] [train step99641] D loss: 0.32542 G loss: 2.24718 (0.040 sec/batch, 1615.825 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:51,699] [train step99651] D loss: 0.32534 G loss: 2.26536 (0.038 sec/batch, 1702.396 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:52,091] [train step99660] D loss: 0.32559 G loss: 2.38798 (0.037 sec/batch, 1725.485 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:52,475] [train step99671] D loss: 0.32579 G loss: 2.39650 (0.038 sec/batch, 1702.644 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:52,854] [train step99680] D loss: 0.32558 G loss: 2.37966 (0.036 sec/batch, 1760.972 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:53,275] [train step99690] D loss: 0.32536 G loss: 2.24982 (0.037 sec/batch, 1728.173 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:53,654] [train step99700] D loss: 0.32526 G loss: 2.27740 (0.034 sec/batch, 1870.278 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:54,041] [train step99710] D loss: 0.32526 G loss: 2.31974 (0.045 sec/batch, 1416.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:54,432] [train step99720] D loss: 0.32532 G loss: 2.26152 (0.038 sec/batch, 1687.987 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:54,821] [train step99731] D loss: 0.32539 G loss: 2.24668 (0.038 sec/batch, 1704.569 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:55,205] [train step99740] D loss: 0.32533 G loss: 2.35164 (0.031 sec/batch, 2037.399 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:55,590] [train step99750] D loss: 0.32557 G loss: 2.22624 (0.037 sec/batch, 1733.654 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:55,973] [train step99761] D loss: 0.32552 G loss: 2.23596 (0.037 sec/batch, 1732.926 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:56,360] [train step99770] D loss: 0.32526 G loss: 2.27713 (0.035 sec/batch, 1850.220 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:56,737] [train step99780] D loss: 0.32524 G loss: 2.27846 (0.039 sec/batch, 1661.141 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:57,126] [train step99790] D loss: 0.32546 G loss: 2.23836 (0.047 sec/batch, 1365.861 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:57,502] [train step99800] D loss: 0.32518 G loss: 2.30134 (0.039 sec/batch, 1657.326 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:57,890] [train step99810] D loss: 0.32529 G loss: 2.33879 (0.037 sec/batch, 1710.652 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:58,289] [train step99821] D loss: 0.32535 G loss: 2.34928 (0.036 sec/batch, 1802.185 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:58,673] [train step99830] D loss: 0.32524 G loss: 2.30678 (0.036 sec/batch, 1753.598 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:59,058] [train step99840] D loss: 0.32523 G loss: 2.32600 (0.043 sec/batch, 1491.805 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:59,446] [train step99851] D loss: 0.32524 G loss: 2.32242 (0.038 sec/batch, 1699.378 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:13:59,822] [train step99861] D loss: 0.32525 G loss: 2.33203 (0.037 sec/batch, 1740.195 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:00,212] [train step99870] D loss: 0.32526 G loss: 2.27798 (0.044 sec/batch, 1449.123 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:00,599] [train step99880] D loss: 0.32525 G loss: 2.33494 (0.039 sec/batch, 1647.288 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:00,975] [train step99891] D loss: 0.32569 G loss: 2.39234 (0.038 sec/batch, 1676.936 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:01,360] [train step99900] D loss: 0.32542 G loss: 2.24559 (0.036 sec/batch, 1753.838 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:01,739] [train step99910] D loss: 0.32522 G loss: 2.30502 (0.035 sec/batch, 1803.808 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:02,126] [train step99920] D loss: 0.32529 G loss: 2.32354 (0.037 sec/batch, 1728.129 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:02,514] [train step99930] D loss: 0.32522 G loss: 2.30129 (0.038 sec/batch, 1667.270 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:02,892] [train step99941] D loss: 0.32525 G loss: 2.31030 (0.047 sec/batch, 1355.741 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:03,289] [train step99951] D loss: 0.32536 G loss: 2.25173 (0.043 sec/batch, 1501.258 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:03,681] [train step99960] D loss: 0.32539 G loss: 2.36195 (0.043 sec/batch, 1501.099 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:04,058] [train step99970] D loss: 0.32550 G loss: 2.37853 (0.037 sec/batch, 1727.929 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:04,436] [train step99981] D loss: 0.32535 G loss: 2.33553 (0.034 sec/batch, 1862.480 instances/sec)\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 05:14:04,812] [train step99990] D loss: 0.32527 G loss: 2.28959 (0.038 sec/batch, 1697.658 instances/sec)\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4ABOAzXMHf3",
        "colab_type": "code",
        "outputId": "763f6034-2a68-4391-8892-6b5e3dd27923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 gdrive/My\\ Drive/Colab\\ Notebooks/DCGAN/evaler.py --dataset CIFAR10 --checkpoint train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190627-132728/model-1001"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[37m\u001b[01m[2019-06-28 06:18:36,148] Reading ./datasets/cifar10/data.hy ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:18:36,149] Reading Done: ./datasets/cifar10/data.hy\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:18:36,150] Reading ./datasets/cifar10/data.hy ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:18:36,150] Reading Done: ./datasets/cifar10/data.hy\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:18:36,150] self.train_dir = None\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:18:36,150] input_ops [input]: Using 10000 IDs from dataset\u001b[0m\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0628 06:18:36.198915 140368168249216 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:31: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0628 06:18:36.204230 140368168249216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0628 06:18:36.205036 140368168249216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "W0628 06:18:36.206522 140368168249216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0628 06:18:36.207567 140368168249216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0628 06:18:36.213509 140368168249216 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:40: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0628 06:18:36.214453 140368168249216 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:51: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0628 06:18:36.221712 140368168249216 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0628 06:18:36.222446 140368168249216 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:33: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0628 06:18:36.223521 140368168249216 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:91: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0628 06:18:36.227436 140368168249216 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:54: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "\u001b[93mGenerator\u001b[0m\n",
            "Generator Tensor(\"Generator/g_1_deconv/Relu:0\", shape=(64, 2, 2, 384), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_2_deconv/Relu:0\", shape=(64, 6, 6, 128), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_3_deconv/Relu:0\", shape=(64, 14, 14, 64), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_4_deconv/Tanh:0\", shape=(64, 32, 32, 3), dtype=float32)\n",
            "\u001b[93mDiscriminator\u001b[0m\n",
            "W0628 06:18:37.049772 140368168249216 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/ops.py:19: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "Discriminator Tensor(\"d_1_conv/dropout/mul_1:0\", shape=(64, 16, 16, 64), dtype=float32)\n",
            "Discriminator Tensor(\"d_2_conv/dropout/mul_1:0\", shape=(64, 8, 8, 128), dtype=float32)\n",
            "Discriminator Tensor(\"d_3_conv/dropout/mul_1:0\", shape=(64, 4, 4, 256), dtype=float32)\n",
            "Discriminator Tensor(\"Discriminator/d_4_fc/BiasAdd:0\", shape=(64, 1), dtype=float32)\n",
            "W0628 06:18:37.604176 140368168249216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0628 06:18:37.627924 140368168249216 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0628 06:18:37.641442 140368168249216 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:127: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "\u001b[93mSuccessfully loaded the model.\u001b[0m\n",
            "W0628 06:18:37.643887 140368168249216 deprecation.py:323] From gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py:50: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "2019-06-28 06:18:37.653613: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-06-28 06:18:37.653837: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a52840 executing computations on platform Host. Devices:\n",
            "2019-06-28 06:18:37.653868: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-28 06:18:37.655964: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-28 06:18:37.826640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:18:37.827151: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a52bc0 executing computations on platform CUDA. Devices:\n",
            "2019-06-28 06:18:37.827181: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-28 06:18:37.827387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:18:37.827734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-28 06:18:37.830580: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 06:18:37.837658: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-28 06:18:37.844233: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-28 06:18:37.853840: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-28 06:18:37.865030: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-28 06:18:37.873616: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-28 06:18:37.891711: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-28 06:18:37.891844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:18:37.892341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:18:37.892681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-28 06:18:37.892739: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 06:18:37.893791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-28 06:18:37.893818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-28 06:18:37.893829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-28 06:18:37.894126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:18:37.894534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:18:37.894971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0628 06:18:37.895570 140368168249216 deprecation_wrapper.py:119] From gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py:59: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:18:37,934] Checkpoint path : train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190627-132728/model-1001\u001b[0m\n",
            "\u001b[33m[2019-06-28 06:18:37,934] dataset: CIFAR10\u001b[0m\n",
            "W0628 06:18:37.934620 140368168249216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Traceback (most recent call last):\n",
            "  File \"gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py\", line 163, in <module>\n",
            "    main()\n",
            "  File \"gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py\", line 160, in main\n",
            "    evaler.eval_run()\n",
            "  File \"gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py\", line 74, in eval_run\n",
            "    self.saver.restore(self.session, self.checkpoint_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1276, in restore\n",
            "    if not checkpoint_management.checkpoint_exists(compat.as_text(save_path)):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_management.py\", line 372, in checkpoint_exists\n",
            "    if file_io.get_matching_files(pathname):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 363, in get_matching_files\n",
            "    return get_matching_files_v2(filename)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 384, in get_matching_files_v2\n",
            "    compat.as_bytes(pattern))\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190627-132728; No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCvC3DyVm66I",
        "colab_type": "code",
        "outputId": "bce8312a-d6a4-467b-d6ee-e71798063850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets  gdrive  sample_data  train_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Dp-tHFooON",
        "colab_type": "code",
        "outputId": "f909ae06-0c39-4bd3-aff3-c157c38a89f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 gdrive/My\\ Drive/Colab\\ Notebooks/DCGAN/evaler.py --dataset CIFAR10 --checkpoint gdrive/My\\ Drive/Colab\\ Notebooks/DCGAN/train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190627-123425/model-1000 --output_file output.hdf5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[37m\u001b[01m[2019-06-28 06:46:34,976] Reading ./datasets/cifar10/data.hy ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:34,977] Reading Done: ./datasets/cifar10/data.hy\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:34,978] Reading ./datasets/cifar10/data.hy ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:34,978] Reading Done: ./datasets/cifar10/data.hy\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:34,978] self.train_dir = None\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:34,979] input_ops [input]: Using 10000 IDs from dataset\u001b[0m\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0628 06:46:35.068838 140554602792832 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:31: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0628 06:46:35.079199 140554602792832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0628 06:46:35.080296 140554602792832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "W0628 06:46:35.082478 140554602792832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0628 06:46:35.083957 140554602792832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0628 06:46:35.091770 140554602792832 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:40: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0628 06:46:35.092964 140554602792832 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:51: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0628 06:46:35.103402 140554602792832 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0628 06:46:35.104429 140554602792832 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:33: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0628 06:46:35.105898 140554602792832 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:91: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0628 06:46:35.112179 140554602792832 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:54: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "\u001b[93mGenerator\u001b[0m\n",
            "Generator Tensor(\"Generator/g_1_deconv/Relu:0\", shape=(64, 2, 2, 384), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_2_deconv/Relu:0\", shape=(64, 6, 6, 128), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_3_deconv/Relu:0\", shape=(64, 14, 14, 64), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_4_deconv/Tanh:0\", shape=(64, 32, 32, 3), dtype=float32)\n",
            "\u001b[93mDiscriminator\u001b[0m\n",
            "W0628 06:46:36.405893 140554602792832 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/ops.py:19: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "Discriminator Tensor(\"d_1_conv/dropout/mul_1:0\", shape=(64, 16, 16, 64), dtype=float32)\n",
            "Discriminator Tensor(\"d_2_conv/dropout/mul_1:0\", shape=(64, 8, 8, 128), dtype=float32)\n",
            "Discriminator Tensor(\"d_3_conv/dropout/mul_1:0\", shape=(64, 4, 4, 256), dtype=float32)\n",
            "Discriminator Tensor(\"Discriminator/d_4_fc/BiasAdd:0\", shape=(64, 1), dtype=float32)\n",
            "W0628 06:46:37.268643 140554602792832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0628 06:46:37.305459 140554602792832 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0628 06:46:37.333613 140554602792832 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:127: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "\u001b[93mSuccessfully loaded the model.\u001b[0m\n",
            "W0628 06:46:37.337691 140554602792832 deprecation.py:323] From gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py:50: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "2019-06-28 06:46:37.352580: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-06-28 06:46:37.352896: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1890840 executing computations on platform Host. Devices:\n",
            "2019-06-28 06:46:37.352934: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-28 06:46:37.371622: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-28 06:46:37.574413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:46:37.575139: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1890bc0 executing computations on platform CUDA. Devices:\n",
            "2019-06-28 06:46:37.575174: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-28 06:46:37.575394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:46:37.575733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-28 06:46:37.576094: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 06:46:37.577288: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-28 06:46:37.578711: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-28 06:46:37.579155: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-28 06:46:37.581152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-28 06:46:37.584082: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-28 06:46:37.590508: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-28 06:46:37.590633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:46:37.591778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:46:37.592287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-28 06:46:37.595979: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 06:46:37.597595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-28 06:46:37.597631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-28 06:46:37.597648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-28 06:46:37.600823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:46:37.601487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 06:46:37.602049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12926 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0628 06:46:37.603467 140554602792832 deprecation_wrapper.py:119] From gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py:59: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:37,671] Checkpoint path : gdrive/My Drive/Colab Notebooks/DCGAN/train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190627-123425/model-1000\u001b[0m\n",
            "\u001b[33m[2019-06-28 06:46:37,672] dataset: CIFAR10\u001b[0m\n",
            "W0628 06:46:37.673072 140554602792832 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:38,743] Loaded from checkpoint!\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 06:46:38,743] Start 1-epoch Inference and Evaluation\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:38,743] # of examples = 10000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:38,743] max_steps = 1\u001b[0m\n",
            "W0628 06:46:38.743851 140554602792832 deprecation.py:323] From gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py:86: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "2019-06-28 06:46:38.813030: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-28 06:46:38.948676: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "\u001b[36m\u001b[01m[2019-06-28 06:46:42,614]  [val   step    0] (3.770 sec/batch, 16.978 instances/sec) \u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 06:46:42,649] Dumping prediction result into output.hdf5 ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 06:46:42,651] Dumping prediction done.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLwgdb5Fores",
        "colab_type": "code",
        "outputId": "43c351c5-43cb-47a1-d56f-d1965eede241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18aL-NQ5wOIS",
        "colab_type": "code",
        "outputId": "6e34a2f5-95c5-4983-f857-6a5b4592ebc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!python3 gdrive/My\\ Drive/Colab\\ Notebooks/DCGAN/visualize_training.py --train_dir gdrive/My\\ Drive/Colab\\ Notebooks/DCGAN/train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190627-123425/ --output_file output.hdf5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/Colab Notebooks/DCGAN/train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190627-123425/generated_0.hy\n",
            "Traceback (most recent call last):\n",
            "  File \"gdrive/My Drive/Colab Notebooks/DCGAN/visualize_training.py\", line 26, in <module>\n",
            "    I[args.h*i:args.h*(i+1), args.w*j:args.w*(j+1), :] = f[f.keys()[0]][i*args.n+j,:,:,:]\n",
            "TypeError: 'KeysView' object does not support indexing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JduthOEGwSlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "from PIL import Image\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORIk8zue_JyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hdf = h5py.File('output.hdf5', 'r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y6NTHVABPu-",
        "colab_type": "code",
        "outputId": "7023595b-391d-4aa2-ffaa-0024a040a692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets  gdrive  output.hdf5  sample_data  train_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3sn3fhe_jDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = hdf.get('image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgukVeeV_wy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_array = np.array(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZeX7SPS_8QY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(64):\n",
        "    image = data_array[i,:,:,:]\n",
        "    image = Image.fromarray(image, 'RGB')\n",
        "    image.save('images{0}.png'.format(i))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfM4JS9EAZ1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2rdwyjAC-nN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tRY2iQTEBrB",
        "colab_type": "code",
        "outputId": "1508d4e3-f3fb-4728-8e60-d99f1b17e048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets      images20.png  images33.png  images46.png\timages59.png\n",
            "gdrive\t      images21.png  images34.png  images47.png\timages5.png\n",
            "images0.png   images22.png  images35.png  images48.png\timages60.png\n",
            "images10.png  images23.png  images36.png  images49.png\timages61.png\n",
            "images11.png  images24.png  images37.png  images4.png\timages62.png\n",
            "images12.png  images25.png  images38.png  images50.png\timages63.png\n",
            "images13.png  images26.png  images39.png  images51.png\timages6.png\n",
            "images14.png  images27.png  images3.png   images52.png\timages7.png\n",
            "images15.png  images28.png  images40.png  images53.png\timages8.png\n",
            "images16.png  images29.png  images41.png  images54.png\timages9.png\n",
            "images17.png  images2.png   images42.png  images55.png\toutput.hdf5\n",
            "images18.png  images30.png  images43.png  images56.png\tsample_data\n",
            "images19.png  images31.png  images44.png  images57.png\ttrain_dir\n",
            "images1.png   images32.png  images45.png  images58.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0bPq63uEW1G",
        "colab_type": "code",
        "outputId": "d6b66eab-9135-42bc-8ec1-5681e1d1d98b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "im = cv2.imread('./images63.png')\n",
        "im2 = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(im2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcDDC2LLE9mg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0NVkMFnGiUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hdf1 = h5py.File('output1.hdf5', 'r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mMnWOxCG_F7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = hdf1.get('image')\n",
        "data_array = np.array(data)\n",
        "for i in range(64):\n",
        "    image = data_array[i,:,:,:]\n",
        "    image = Image.fromarray(image, 'RGB')\n",
        "    image.save('images3{0}.png'.format(i))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2cStzhBHuTQ",
        "colab_type": "code",
        "outputId": "1837d536-5cd9-4fda-d6b4-7d316035cd02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets      images30.png   images332.png  images355.png  images4.png\n",
            "gdrive\t      images310.png  images333.png  images356.png  images50.png\n",
            "images0.png   images311.png  images334.png  images357.png  images51.png\n",
            "images10.png  images312.png  images335.png  images358.png  images52.png\n",
            "images11.png  images313.png  images336.png  images359.png  images53.png\n",
            "images12.png  images314.png  images337.png  images35.png   images54.png\n",
            "images13.png  images315.png  images338.png  images360.png  images55.png\n",
            "images14.png  images316.png  images339.png  images361.png  images56.png\n",
            "images15.png  images317.png  images33.png   images362.png  images57.png\n",
            "images16.png  images318.png  images340.png  images363.png  images58.png\n",
            "images17.png  images319.png  images341.png  images36.png   images59.png\n",
            "images18.png  images31.png   images342.png  images37.png   images5.png\n",
            "images19.png  images320.png  images343.png  images38.png   images60.png\n",
            "images1.png   images321.png  images344.png  images39.png   images61.png\n",
            "images20.png  images322.png  images345.png  images3.png    images62.png\n",
            "images21.png  images323.png  images346.png  images40.png   images63.png\n",
            "images22.png  images324.png  images347.png  images41.png   images6.png\n",
            "images23.png  images325.png  images348.png  images42.png   images7.png\n",
            "images24.png  images326.png  images349.png  images43.png   images8.png\n",
            "images25.png  images327.png  images34.png   images44.png   images9.png\n",
            "images26.png  images328.png  images350.png  images45.png   output1.hdf5\n",
            "images27.png  images329.png  images351.png  images46.png   output.hdf5\n",
            "images28.png  images32.png   images352.png  images47.png   sample_data\n",
            "images29.png  images330.png  images353.png  images48.png   train_dir\n",
            "images2.png   images331.png  images354.png  images49.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bTQ2t7SREHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e1c429d-8d16-46d1-d4b4-848cbabc8563"
      },
      "source": [
        "!python3 /content/gdrive/My\\ Drive/Colab\\ Notebooks/DCGAN/evaler.py --dataset CIFAR10 --checkpoint_path /content/train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190628-063113/model-99000 --output_file outputlast.hdf5"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[37m\u001b[01m[2019-06-28 08:47:29,968] Reading ./datasets/cifar10/data.hy ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:29,969] Reading Done: ./datasets/cifar10/data.hy\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:29,970] Reading ./datasets/cifar10/data.hy ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:29,970] Reading Done: ./datasets/cifar10/data.hy\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:29,970] self.train_dir = None\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:29,970] input_ops [input]: Using 10000 IDs from dataset\u001b[0m\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0628 08:47:30.019012 140563378403200 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:31: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0628 08:47:30.023948 140563378403200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0628 08:47:30.024717 140563378403200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "W0628 08:47:30.026150 140563378403200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0628 08:47:30.027096 140563378403200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0628 08:47:30.032812 140563378403200 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:40: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0628 08:47:30.033775 140563378403200 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/preprocessing.py:51: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0628 08:47:30.041038 140563378403200 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0628 08:47:30.041735 140563378403200 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:33: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0628 08:47:30.042772 140563378403200 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:91: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0628 08:47:30.046535 140563378403200 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:54: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "\u001b[93mGenerator\u001b[0m\n",
            "Generator Tensor(\"Generator/g_1_deconv/Relu:0\", shape=(64, 2, 2, 384), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_2_deconv/Relu:0\", shape=(64, 6, 6, 128), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_3_deconv/Relu:0\", shape=(64, 14, 14, 64), dtype=float32)\n",
            "Generator Tensor(\"Generator/g_4_deconv/Tanh:0\", shape=(64, 32, 32, 3), dtype=float32)\n",
            "\u001b[93mDiscriminator\u001b[0m\n",
            "W0628 08:47:30.861669 140563378403200 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/ops.py:19: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "Discriminator Tensor(\"d_1_conv/dropout/mul_1:0\", shape=(64, 16, 16, 64), dtype=float32)\n",
            "Discriminator Tensor(\"d_2_conv/dropout/mul_1:0\", shape=(64, 8, 8, 128), dtype=float32)\n",
            "Discriminator Tensor(\"d_3_conv/dropout/mul_1:0\", shape=(64, 4, 4, 256), dtype=float32)\n",
            "Discriminator Tensor(\"Discriminator/d_4_fc/BiasAdd:0\", shape=(64, 1), dtype=float32)\n",
            "W0628 08:47:31.421624 140563378403200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0628 08:47:31.445663 140563378403200 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:120: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0628 08:47:31.459466 140563378403200 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/model.py:127: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "\u001b[93mSuccessfully loaded the model.\u001b[0m\n",
            "W0628 08:47:31.461846 140563378403200 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py:50: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "2019-06-28 08:47:31.470873: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-06-28 08:47:31.471093: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13abc00 executing computations on platform Host. Devices:\n",
            "2019-06-28 08:47:31.471121: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-28 08:47:31.473132: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-28 08:47:31.644139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 08:47:31.644676: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13aca00 executing computations on platform CUDA. Devices:\n",
            "2019-06-28 08:47:31.644725: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-28 08:47:31.644936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 08:47:31.645320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-28 08:47:31.645664: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 08:47:31.646844: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-28 08:47:31.648022: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-28 08:47:31.648387: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-28 08:47:31.649800: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-28 08:47:31.650755: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-28 08:47:31.653780: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-28 08:47:31.653890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 08:47:31.654320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 08:47:31.654658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-28 08:47:31.654714: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 08:47:31.655746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-28 08:47:31.655777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-28 08:47:31.655788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-28 08:47:31.656095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 08:47:31.656474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 08:47:31.656829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0628 08:47:31.657422 140563378403200 deprecation_wrapper.py:119] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py:59: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:31,696] Checkpoint path : /content/train_dir/default-CIFAR10_lr_0.0001_update_G5_D1-20190628-063113/model-99000\u001b[0m\n",
            "\u001b[33m[2019-06-28 08:47:31,696] dataset: CIFAR10\u001b[0m\n",
            "W0628 08:47:31.697032 140563378403200 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:32,316] Loaded from checkpoint!\u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 08:47:32,317] Start 1-epoch Inference and Evaluation\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:32,317] # of examples = 10000\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:32,317] max_steps = 1\u001b[0m\n",
            "W0628 08:47:32.317575 140563378403200 deprecation.py:323] From /content/gdrive/My Drive/Colab Notebooks/DCGAN/evaler.py:86: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "2019-06-28 08:47:32.360161: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-28 08:47:32.446466: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "\u001b[36m\u001b[01m[2019-06-28 08:47:34,374]  [val   step    0] (2.044 sec/batch, 31.304 instances/sec) \u001b[0m\n",
            "\u001b[36m\u001b[01m[2019-06-28 08:47:34,406] Dumping prediction result into outputlast.hdf5 ...\u001b[0m\n",
            "\u001b[37m\u001b[01m[2019-06-28 08:47:34,408] Dumping prediction done.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnzac3y5RlL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "from PIL import Image\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbPhOpS0R__V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hdfl = h5py.File('outputlast.hdf5', 'r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H0C1JuXSURV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = hdfl.get('image')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHkYLvkPSx1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_array = np.array(data)\n",
        "for i in range(64):\n",
        "    image = data_array[i,:,:,:]\n",
        "    image = Image.fromarray(image, 'RGB')\n",
        "    image.save('images99000-{0}.png'.format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ch0mw2vTGqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "55a06e13-ad38-4d1a-c283-a6b20d55cf92"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "im = cv2.imread('./images99000-0.png')\n",
        "im2 = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(im2)\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwy8VyvBLhTL8UqEMv1So/wPMhRNTX7Bx4AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4BAH8q0WUuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}